{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/Compression-Framework-for-EdgeAI\n!git clone https://github.com/ha405/Compression-Framework-for-EdgeAI","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"id":"zIhJAxWiRCz6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the below command only once per session. If you reset session run again.","metadata":{"id":"IJTF9qgyRC0C"}},{"cell_type":"code","source":"# !pip install -r /kaggle/working/Compression-Framework-for-EdgeAI/requirements.txt\n!pip install logbar\n!pip install tokenicer\n!pip install device_smi\n!pip install random_word\n!pip install datasets","metadata":{"trusted":true,"id":"LNFwIdPMRC0K"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\nlibrary_path = \"/kaggle/working/Compression-Framework-for-EdgeAI/KLAWQ\"\nif library_path not in sys.path:\n     sys.path.insert(0, library_path)\n     print(f\"Added '{library_path}' to sys.path\")\nfrom quant import GPTQModel, QuantizeConfig","metadata":{"trusted":true,"id":"tmbBZfI0RC0M","execution":{"iopub.status.busy":"2025-07-02T20:09:44.991636Z","iopub.execute_input":"2025-07-02T20:09:44.992110Z","iopub.status.idle":"2025-07-02T20:09:58.313750Z","shell.execute_reply.started":"2025-07-02T20:09:44.992087Z","shell.execute_reply":"2025-07-02T20:09:58.313138Z"}},"outputs":[{"name":"stdout","text":"Added '/kaggle/working/Compression-Framework-for-EdgeAI/KLAWQ' to sys.path\n\n\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n","output_type":"stream"},{"name":"stderr","text":"2025-07-02 20:09:53.363981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751486993.539829     248 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751486993.595094     248 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"li8RDMhbRC0P"}},{"cell_type":"code","source":"import os\n# os.environ[\"HF_HOME\"]           = os.path.abspath(\"/media/cvgl/5a7a01f8-7cab-46f8-a6d7-002c21d2008b1/haseeb/Compression-Framework-for-EdgeAI/hf_cache\")\n# os.environ[\"TRANSFORMERS_CACHE\"]= os.path.abspath(\"/media/cvgl/5a7a01f8-7cab-46f8-a6d7-002c21d2008b1/haseeb/Compression-Framework-for-EdgeAI/hf_cache/models\")\n# os.environ[\"HF_DATASETS_CACHE\"] = os.path.abspath(\"/media/cvgl/5a7a01f8-7cab-46f8-a6d7-002c21d2008b1/haseeb/Compression-Framework-for-EdgeAI/hf_cache/datasets\")\nimport gc\nimport torch\nimport shutil\nimport math\nimport pandas as pd\nfrom transformers import AutoTokenizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datasets import load_dataset, DatasetDict\nfrom tqdm import tqdm\nfrom typing import Dict, List\nfrom torch.utils.data import Dataset","metadata":{"trusted":true,"id":"jX2yPNIYRC0Q","execution":{"iopub.status.busy":"2025-07-02T20:09:58.314875Z","iopub.execute_input":"2025-07-02T20:09:58.315432Z","iopub.status.idle":"2025-07-02T20:09:58.320290Z","shell.execute_reply.started":"2025-07-02T20:09:58.315411Z","shell.execute_reply":"2025-07-02T20:09:58.319560Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## WikiText-2","metadata":{"id":"dKBfjngiRC0v"}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\n# Load only the 'test' and first 1000 'train' examples\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split={\n    \"test\": \"test\",\n    \"calibration\": \"train[:600]\",\n})\n\n# Print the sizes of loaded splits\nprint({k: len(v) for k, v in dataset.items()})\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"V0rLysHGRC0y","outputId":"edde6dd9-00f2-4596-bf36-49fbba5e73ba","execution":{"iopub.status.busy":"2025-07-02T20:09:58.321081Z","iopub.execute_input":"2025-07-02T20:09:58.321305Z","iopub.status.idle":"2025-07-02T20:10:00.958369Z","shell.execute_reply.started":"2025-07-02T20:09:58.321288Z","shell.execute_reply":"2025-07-02T20:10:00.957562Z"}},"outputs":[{"name":"stdout","text":"{'test': 4358, 'calibration': 600}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"To Clear GPU Cache","metadata":{"id":"wIoXqTHQRC01"}},{"cell_type":"code","source":"def clear_gpu_cache():\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    print(\"✅ GPU VRAM and cache cleared.\")\nclear_gpu_cache()","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"9BOJ1si4RC04","outputId":"9ec363ee-a0d4-4858-d8d7-1014f94d61cb","execution":{"iopub.status.busy":"2025-07-02T20:10:18.122694Z","iopub.execute_input":"2025-07-02T20:10:18.122994Z","iopub.status.idle":"2025-07-02T20:10:18.472517Z","shell.execute_reply.started":"2025-07-02T20:10:18.122972Z","shell.execute_reply":"2025-07-02T20:10:18.471635Z"}},"outputs":[{"name":"stdout","text":"✅ GPU VRAM and cache cleared.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Quantization Functions","metadata":{"id":"MEOEVZGrRC05"}},{"cell_type":"code","source":"class CalibDataset(Dataset):\n    def __init__(self, tokenized: Dict[str, torch.Tensor]):\n        self.input_ids      = tokenized[\"input_ids\"]\n        self.attention_mask = tokenized[\"attention_mask\"]\n\n    def __len__(self):\n        return self.input_ids.size(0)\n\n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        # Return torch.Tensors directly\n        return {\n            \"input_ids\":      self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n        }\n\ndef clear_quant_path(path=None):\n    if path and os.path.exists(path):\n        shutil.rmtree(path)\n    torch.cuda.empty_cache(); gc.collect()\n\ndef clear_memory():\n    torch.cuda.empty_cache(); gc.collect()\n\ndef quantize_and_eval(\n    model_id: str,\n    calib_tokenized: Dict[str, torch.Tensor],\n    eval_texts: List[str],\n    beta: float,\n    tau: float,\n    quant_path: str,\n    tokenizer: AutoTokenizer,\n    max_len: int,\n    batch_size: int = 8\n):\n    print(f\"\\n→ [Quantize] β={beta}, τ={tau}\")\n    clear_quant_path(quant_path)\n\n    # # 1) Load raw model\n    quant_cfg = QuantizeConfig(bits=4, group_size=-1, beta=beta, tau=tau)\n    model = GPTQModel.load(\n        model_id,\n        quantize_config=quant_cfg,\n        trust_remote_code=True,\n        torch_dtype=\"auto\",\n        device_map=\"auto\"\n    )\n\n    # 2) Prepare a Dataset of Tensors (no Python lists!)\n    calib_ds = CalibDataset(calib_tokenized)\n\n    # 3) Quantize—GPTQ will now skip tensor-to-list conversions\n    model.quantize(\n        calibration_dataset=calib_ds,\n        batch_size=batch_size,\n        calibration_dataset_concat_size=len(calib_ds),\n        calibration_enable_gpu_cache=False,\n        buffered_fwd=False,\n        auto_gc=True,\n        tokenizer=tokenizer,    # ← Key to use the fast path\n    )\n\n    os.makedirs(os.path.dirname(quant_path), exist_ok=True)\n    model.save(quant_path)\n    print(f\"   Quantization complete — saved to {quant_path}\")\n    del model\n    clear_memory()\n\n    # 4) Reload quantized model onto GPU(s)\n    model = GPTQModel.from_pretrained(\n        quant_path,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        torch_dtype=\"auto\",\n        quantize_config=quant_cfg\n    )\n    model.eval()\n    device = next(model.parameters()).device\n\n    # 3) Build a single long list of token IDs (bypass HF max-length)\n    all_ids = []\n    for text in eval_texts:\n        if not text.strip():\n            continue\n        all_ids += tokenizer.encode(text, add_special_tokens=False)\n\n    # 4) Truncate to multiple of max_len and slice into blocks\n    total = (len(all_ids) // max_len) * max_len\n    if total == 0:\n        raise ValueError(f\"Need ≥{max_len} tokens, got {len(all_ids)}\")\n    ids_tensor = torch.tensor(all_ids[:total], dtype=torch.long, device=device)\n    blocks = ids_tensor.view(-1, max_len)\n    n_blocks = blocks.size(0)\n\n    # 5) MagR-style block-wise PPL evaluation\n    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n    total_nll, total_count = 0.0, 0\n\n    for i in tqdm(range(0, n_blocks, batch_size), desc=\"Evaluating PPL\"):\n        batch = blocks[i : i + batch_size]         # [B, L]\n        attn = torch.ones_like(batch, device=device)\n        labels = batch.clone()\n        labels[:, :-1] = batch[:, 1:]              # shift tokens\n        labels[:, -1]  = -100                       # ignore last\n\n        with torch.no_grad():\n            out = model(input_ids=batch, attention_mask=attn, labels=labels)\n            logits = out.logits[:, :-1, :].reshape(-1, out.logits.size(-1))\n            labs   = labels[:, :-1].reshape(-1)\n            nlls   = loss_fct(logits, labs) * (labs != -100).float()\n\n        total_nll   += nlls.sum().item()\n        total_count += (labs != -100).sum().item()\n\n    avg_nll = total_nll / total_count\n    ppl     = math.exp(avg_nll)\n    print(f\"     Eval complete: loss={avg_nll:.4f}, ppl={ppl:.2f}\")\n\n    del model\n    clear_quant_path(quant_path)\n    return avg_nll, ppl\n","metadata":{"trusted":true,"id":"9c6mKTcGRC0-","execution":{"iopub.status.busy":"2025-07-02T20:10:20.408012Z","iopub.execute_input":"2025-07-02T20:10:20.408530Z","iopub.status.idle":"2025-07-02T20:10:20.422426Z","shell.execute_reply.started":"2025-07-02T20:10:20.408488Z","shell.execute_reply":"2025-07-02T20:10:20.421756Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from huggingface_hub import login\nfrom getpass import getpass\n\nHF_TOKEN = getpass(\"Enter your Hugging Face token:\")\nlogin(token=HF_TOKEN)\n\nprint(\"Hugging Face login successful!\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"DNMWpCwJRC0_","outputId":"d3a302e1-b14e-4c56-8513-eed3b0758683"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Max len is for llama. Adjust Batch Size if Out of memory error. reduce to 2","metadata":{"id":"DySniR1HjLbI"}},{"cell_type":"code","source":"calib_texts = dataset[\"calibration\"][\"text\"]\neval_texts  = [t for t in dataset[\"test\"][\"text\"] if t.strip()]\n\n# --- init tokenizer & pre-tokenize calibration set ---\nmodel_id = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\nmax_len = tokenizer.model_max_length\nmax_len = 4096\ncalib_tokenized = tokenizer(\n    calib_texts,\n    truncation=True,\n    padding=\"longest\",\n    max_length=max_len,\n    return_tensors=\"pt\"\n)\n\nbase_quant_path = \"/kaggle/working/llama2-7b-quant\"\nbeta_values     = [0.2]\ntau_values      = []\nresults = []\ntotal_iters = len(beta_values) + len(tau_values)\niter_count = 0\n\n\nfor beta in beta_values:\n    iter_count += 1\n    print(f\"[Iter {iter_count}/{total_iters}] β={beta}, τ=0.5\")\n    qp = f\"{base_quant_path}-b{beta}-t0.5\"\n    loss, ppl = quantize_and_eval(\n        model_id=model_id,\n        calib_tokenized=calib_tokenized,\n        eval_texts=eval_texts,\n        beta=beta,\n        tau=1.0,\n        quant_path=qp,\n        tokenizer=tokenizer,\n        max_len=max_len,\n        batch_size=1\n    )\n    results.append({\"beta\": beta, \"tau\": 0.5, \"loss\": loss, \"ppl\": ppl})\n\n# … select best_beta …\n\nfor tau in tau_values:\n    iter_count += 1\n    print(f\"[Iter {iter_count}/{total_iters}] β={best_beta}, τ={tau}\")\n    qp = f\"{base_quant_path}-b{best_beta}-t{tau}\"\n    loss, ppl = quantize_and_eval(\n        model_id=model_id,\n        calib_tokenized=calib_tokenized,\n        eval_texts=eval_texts,\n        beta=best_beta,\n        tau=tau,\n        quant_path=qp,\n        tokenizer=tokenizer,      # ← and here too\n        max_len=max_len,\n        batch_size=1\n    )\n    results.append({\"beta\": best_beta, \"tau\": tau, \"loss\": loss, \"ppl\": ppl})\n\ndf = pd.DataFrame(results)\nprint(df.to_markdown(index=False))\n","metadata":{"trusted":true,"id":"zKLm0EvMRC1B","execution":{"iopub.status.busy":"2025-07-02T20:10:28.109139Z","iopub.execute_input":"2025-07-02T20:10:28.109946Z","iopub.status.idle":"2025-07-02T21:56:19.537584Z","shell.execute_reply.started":"2025-07-02T20:10:28.109914Z","shell.execute_reply":"2025-07-02T21:56:19.536755Z"}},"outputs":[{"name":"stdout","text":"[Iter 1/1] β=0.2, τ=0.5\n\n→ [Quantize] β=0.2, τ=1.0\n\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4 bpw, based on [bits: 4, group_size: -1]      \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1cd88554864b70b8ef0f23ca5e7089"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                                         \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d9d886a77a4257aeecc65afe796fe0"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n\u001b[32mINFO\u001b[0m  Packing Kernel: selected: `TorchQuantLinear`                                                 \n\u001b[32mINFO\u001b[0m  Hooked Modules: Using tree based config for accurate targeting of modules                    \n\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_avoided_time_07_02_2025_20h_10m_31s.log`\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[92m0.0000000003\u001b[0m | 360000      | 0.01000     | 1.876     | 33.419       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.485     | 33.419       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[92m0.0000000003\u001b[0m | 360000      | 0.01000     | 1.469     | 33.419       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.452     | 12.200       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.608     | 36.170       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.595     | 36.170       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 5.324     | 69.372       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[92m0.0000000022\u001b[0m | 360000      | 0.01000     | 1.478     | 35.023       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.561     | 35.023       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[92m0.0000000020\u001b[0m | 360000      | 0.01000     | 1.463     | 35.023       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.506     | 12.201       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.611     | 35.636       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.682     | 35.636       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[92m0.0001922218\u001b[0m | 360000      | 0.01000     | 5.212     | 66.512       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[92m0.0000000045\u001b[0m | 360000      | 0.01000     | 1.538     | 35.117       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.0000000007\u001b[0m | 360000      | 0.01000     | 1.542     | 35.117       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[92m0.0000000067\u001b[0m | 360000      | 0.01000     | 1.604     | 35.117       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.483     | 12.210       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[92m0.0000000007\u001b[0m | 360000      | 0.01000     | 1.679     | 35.774       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[92m0.0000000006\u001b[0m | 360000      | 0.01000     | 1.719     | 35.774       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 5.306     | 70.013       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[92m0.0000000270\u001b[0m | 360000      | 0.01000     | 1.694     | 35.432       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.0000000056\u001b[0m | 360000      | 0.01000     | 1.627     | 35.432       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[92m0.0000000496\u001b[0m | 360000      | 0.01000     | 1.655     | 35.432       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.587     | 12.234       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[92m0.0000000025\u001b[0m | 360000      | 0.01000     | 1.707     | 36.129       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[92m0.0000000021\u001b[0m | 360000      | 0.01000     | 1.702     | 36.129       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 5.231     | 69.883       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[92m0.0000000381\u001b[0m | 360000      | 0.01000     | 1.574     | 35.180       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.0000000072\u001b[0m | 360000      | 0.01000     | 1.609     | 35.180       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[92m0.0000000793\u001b[0m | 360000      | 0.01000     | 1.639     | 35.180       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.574     | 12.236       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[92m0.0000000065\u001b[0m | 360000      | 0.01000     | 1.748     | 36.018       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[92m0.0000000052\u001b[0m | 360000      | 0.01000     | 1.713     | 36.018       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 5.497     | 70.185       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[92m0.0000000378\u001b[0m | 360000      | 0.01000     | 1.514     | 35.191       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.0000000087\u001b[0m | 360000      | 0.01000     | 1.518     | 35.191       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[92m0.0000000522\u001b[0m | 360000      | 0.01000     | 1.523     | 35.191       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.496     | 12.212       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[92m0.0000000085\u001b[0m | 360000      | 0.01000     | 1.649     | 35.678       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[92m0.0000000067\u001b[0m | 360000      | 0.01000     | 1.620     | 35.678       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 5.274     | 69.959       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[92m0.0000000830\u001b[0m | 360000      | 0.01000     | 1.498     | 35.165       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.0000000216\u001b[0m | 360000      | 0.01000     | 1.509     | 35.165       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[92m0.0000001366\u001b[0m | 360000      | 0.01000     | 1.491     | 35.165       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.519     | 12.202       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[92m0.0000000129\u001b[0m | 360000      | 0.01000     | 1.729     | 35.854       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[92m0.0000000098\u001b[0m | 360000      | 0.01000     | 1.681     | 35.854       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 5.577     | 70.089       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[92m0.0000001018\u001b[0m | 360000      | 0.01000     | 1.624     | 35.565       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.0000000279\u001b[0m | 360000      | 0.01000     | 1.724     | 35.565       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[92m0.0000001425\u001b[0m | 360000      | 0.01000     | 1.661     | 35.565       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.628     | 12.227       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[92m0.0000000166\u001b[0m | 360000      | 0.01000     | 1.739     | 36.068       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[92m0.0000000127\u001b[0m | 360000      | 0.01000     | 1.732     | 36.068       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 5.276     | 69.876       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[92m0.0000001152\u001b[0m | 360000      | 0.01000     | 1.483     | 35.160       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.0000000305\u001b[0m | 360000      | 0.01000     | 1.503     | 35.160       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[92m0.0000001538\u001b[0m | 360000      | 0.01000     | 1.452     | 35.160       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 360000      | 0.01000     | 1.477     | 12.167       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[92m0.0000000186\u001b[0m | 360000      | 0.01000     | 1.632     | 35.557       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[92m0.0000000150\u001b[0m | 360000      | 0.01000     | 1.619     | 35.557       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.0000000008\u001b[0m | 360000      | 0.01000     | 5.273     | 70.036       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[92m0.0000001463\u001b[0m | 360000      | 0.01000     | 1.470     | 35.191       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.0000000390\u001b[0m | 360000      | 0.01000     | 1.472     | 35.191       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[92m0.0000001813\u001b[0m | 360000      | 0.01000     | 1.499     | 35.191       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.456     | 12.150       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[92m0.0000000208\u001b[0m | 360000      | 0.01000     | 1.704     | 35.522       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[92m0.0000000174\u001b[0m | 360000      | 0.01000     | 1.658     | 35.522       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 5.307     | 70.112       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[92m0.0000002153\u001b[0m | 360000      | 0.01000     | 1.540     | 35.083       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.0000000607\u001b[0m | 360000      | 0.01000     | 1.538     | 35.083       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[92m0.0000002516\u001b[0m | 360000      | 0.01000     | 1.477     | 35.083       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.477     | 12.179       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[92m0.0000000209\u001b[0m | 360000      | 0.01000     | 1.625     | 35.440       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[92m0.0000000179\u001b[0m | 360000      | 0.01000     | 1.596     | 35.440       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 5.192     | 70.092       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[92m0.0000002159\u001b[0m | 360000      | 0.01000     | 1.479     | 35.157       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.0000000758\u001b[0m | 360000      | 0.01000     | 1.505     | 35.157       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[92m0.0000002729\u001b[0m | 360000      | 0.01000     | 1.465     | 35.157       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.481     | 12.148       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[92m0.0000000252\u001b[0m | 360000      | 0.01000     | 1.617     | 35.596       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[92m0.0000000222\u001b[0m | 360000      | 0.01000     | 1.624     | 35.596       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.0000000004\u001b[0m | 360000      | 0.01000     | 5.273     | 70.189       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[92m0.0000002296\u001b[0m | 360000      | 0.01000     | 1.508     | 35.149       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[92m0.0000000772\u001b[0m | 360000      | 0.01000     | 1.461     | 35.149       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[92m0.0000002472\u001b[0m | 360000      | 0.01000     | 1.550     | 35.149       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.477     | 12.207       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[92m0.0000000273\u001b[0m | 360000      | 0.01000     | 1.602     | 35.435       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[92m0.0000000245\u001b[0m | 360000      | 0.01000     | 1.607     | 35.435       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 5.270     | 70.061       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[92m0.0000002066\u001b[0m | 360000      | 0.01000     | 1.480     | 35.094       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[92m0.0000000742\u001b[0m | 360000      | 0.01000     | 1.488     | 35.094       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[92m0.0000002708\u001b[0m | 360000      | 0.01000     | 1.473     | 35.094       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 360000      | 0.01000     | 1.554     | 12.144       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[92m0.0000000302\u001b[0m | 360000      | 0.01000     | 1.633     | 35.505       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[92m0.0000000280\u001b[0m | 360000      | 0.01000     | 1.643     | 35.505       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[92m0.0000000004\u001b[0m | 360000      | 0.01000     | 5.374     | 70.102       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[92m0.0000002500\u001b[0m | 360000      | 0.01000     | 1.515     | 35.135       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[92m0.0000000789\u001b[0m | 360000      | 0.01000     | 1.519     | 35.135       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[92m0.0000002798\u001b[0m | 360000      | 0.01000     | 1.528     | 35.135       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.0000000002\u001b[0m | 360000      | 0.01000     | 1.535     | 12.180       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[92m0.0000000348\u001b[0m | 360000      | 0.01000     | 1.631     | 35.535       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[92m0.0000000322\u001b[0m | 360000      | 0.01000     | 1.594     | 35.535       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[92m0.0000000003\u001b[0m | 360000      | 0.01000     | 5.216     | 70.116       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[92m0.0000001730\u001b[0m | 360000      | 0.01000     | 1.492     | 35.132       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[92m0.0000000667\u001b[0m | 360000      | 0.01000     | 1.476     | 35.132       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[92m0.0000001838\u001b[0m | 360000      | 0.01000     | 1.511     | 35.132       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[92m0.0000000004\u001b[0m | 360000      | 0.01000     | 1.471     | 12.218       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[92m0.0000000412\u001b[0m | 360000      | 0.01000     | 1.687     | 35.632       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[92m0.0000000387\u001b[0m | 360000      | 0.01000     | 1.612     | 35.632       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[92m0.0000000006\u001b[0m | 360000      | 0.01000     | 5.204     | 70.115       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[92m0.0000001827\u001b[0m | 360000      | 0.01000     | 1.557     | 35.202       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[92m0.0000000746\u001b[0m | 360000      | 0.01000     | 1.518     | 35.202       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[92m0.0000002094\u001b[0m | 360000      | 0.01000     | 1.480     | 35.202       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.o_proj     | \u001b[92m0.0000000004\u001b[0m | 360000      | 0.01000     | 1.455     | 12.190       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.gate_proj        | \u001b[92m0.0000000515\u001b[0m | 360000      | 0.01000     | 1.613     | 35.410       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.up_proj          | \u001b[92m0.0000000475\u001b[0m | 360000      | 0.01000     | 1.594     | 35.410       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.down_proj        | \u001b[92m0.0000000143\u001b[0m | 360000      | 0.01000     | 5.268     | 70.240       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[92m0.0000002567\u001b[0m | 360000      | 0.01000     | 1.593     | 35.317       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[92m0.0000000930\u001b[0m | 360000      | 0.01000     | 1.583     | 35.317       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[92m0.0000003577\u001b[0m | 360000      | 0.01000     | 1.618     | 35.317       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.o_proj     | \u001b[92m0.0000000003\u001b[0m | 360000      | 0.01000     | 1.572     | 12.255       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.gate_proj        | \u001b[92m0.0000000628\u001b[0m | 360000      | 0.01000     | 1.641     | 35.881       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.up_proj          | \u001b[92m0.0000000564\u001b[0m | 360000      | 0.01000     | 1.639     | 35.881       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.down_proj        | \u001b[92m0.0000000011\u001b[0m | 360000      | 0.01000     | 5.279     | 70.036       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[92m0.0000002802\u001b[0m | 360000      | 0.01000     | 1.539     | 35.077       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[92m0.0000001161\u001b[0m | 360000      | 0.01000     | 1.442     | 35.077       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[92m0.0000004117\u001b[0m | 360000      | 0.01000     | 1.468     | 35.077       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.o_proj     | \u001b[92m0.0000000004\u001b[0m | 360000      | 0.01000     | 1.472     | 12.218       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.gate_proj        | \u001b[92m0.0000000777\u001b[0m | 360000      | 0.01000     | 1.589     | 35.619       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.up_proj          | \u001b[92m0.0000000682\u001b[0m | 360000      | 0.01000     | 1.568     | 35.619       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.down_proj        | \u001b[92m0.0000001651\u001b[0m | 360000      | 0.01000     | 5.139     | 70.386       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[92m0.0000002263\u001b[0m | 360000      | 0.01000     | 1.468     | 35.120       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[92m0.0000000954\u001b[0m | 360000      | 0.01000     | 1.443     | 35.120       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[92m0.0000003461\u001b[0m | 360000      | 0.01000     | 1.445     | 35.120       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.o_proj     | \u001b[92m0.0000000002\u001b[0m | 360000      | 0.01000     | 1.436     | 12.179       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.gate_proj        | \u001b[92m0.0000000852\u001b[0m | 360000      | 0.01000     | 1.576     | 35.391       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.up_proj          | \u001b[92m0.0000000743\u001b[0m | 360000      | 0.01000     | 1.624     | 35.391       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.down_proj        | \u001b[92m0.0000000258\u001b[0m | 360000      | 0.01000     | 5.112     | 70.515       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[92m0.0000002597\u001b[0m | 360000      | 0.01000     | 1.430     | 35.172       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[92m0.0000001002\u001b[0m | 360000      | 0.01000     | 1.423     | 35.172       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[92m0.0000003968\u001b[0m | 360000      | 0.01000     | 1.460     | 35.172       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.o_proj     | \u001b[92m0.0000000003\u001b[0m | 360000      | 0.01000     | 1.421     | 12.180       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.gate_proj        | \u001b[92m0.0000000904\u001b[0m | 360000      | 0.01000     | 1.612     | 35.433       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.up_proj          | \u001b[92m0.0000000784\u001b[0m | 360000      | 0.01000     | 1.630     | 35.433       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.down_proj        | \u001b[92m0.0000000031\u001b[0m | 360000      | 0.01000     | 5.082     | 70.160       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[92m0.0000002449\u001b[0m | 360000      | 0.01000     | 1.466     | 35.176       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[92m0.0000001140\u001b[0m | 360000      | 0.01000     | 1.529     | 35.176       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[92m0.0000003781\u001b[0m | 360000      | 0.01000     | 1.453     | 35.176       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.o_proj     | \u001b[92m0.0000000002\u001b[0m | 360000      | 0.01000     | 1.433     | 12.176       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.gate_proj        | \u001b[92m0.0000001068\u001b[0m | 360000      | 0.01000     | 1.571     | 35.473       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.up_proj          | \u001b[92m0.0000000913\u001b[0m | 360000      | 0.01000     | 1.586     | 35.473       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.down_proj        | \u001b[92m0.0000000033\u001b[0m | 360000      | 0.01000     | 5.142     | 70.145       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.k_proj     | \u001b[92m0.0000002820\u001b[0m | 360000      | 0.01000     | 1.452     | 35.218       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.v_proj     | \u001b[92m0.0000001299\u001b[0m | 360000      | 0.01000     | 1.533     | 35.218       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.q_proj     | \u001b[92m0.0000004096\u001b[0m | 360000      | 0.01000     | 1.459     | 35.218       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.o_proj     | \u001b[92m0.0000000007\u001b[0m | 360000      | 0.01000     | 1.423     | 12.182       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.gate_proj        | \u001b[92m0.0000001179\u001b[0m | 360000      | 0.01000     | 1.594     | 35.437       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.up_proj          | \u001b[92m0.0000000994\u001b[0m | 360000      | 0.01000     | 1.568     | 35.437       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.down_proj        | \u001b[92m0.0000000036\u001b[0m | 360000      | 0.01000     | 5.158     | 70.024       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.k_proj     | \u001b[92m0.0000003055\u001b[0m | 360000      | 0.01000     | 1.450     | 35.209       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.v_proj     | \u001b[92m0.0000001568\u001b[0m | 360000      | 0.01000     | 1.452     | 35.209       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.q_proj     | \u001b[92m0.0000004448\u001b[0m | 360000      | 0.01000     | 1.490     | 35.209       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.o_proj     | \u001b[92m0.0000000008\u001b[0m | 360000      | 0.01000     | 1.443     | 12.166       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.gate_proj        | \u001b[92m0.0000001405\u001b[0m | 360000      | 0.01000     | 1.583     | 35.447       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.up_proj          | \u001b[92m0.0000001205\u001b[0m | 360000      | 0.01000     | 1.563     | 35.447       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.down_proj        | \u001b[92m0.0000000051\u001b[0m | 360000      | 0.01000     | 5.180     | 70.169       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.k_proj     | \u001b[92m0.0000003376\u001b[0m | 360000      | 0.01000     | 1.475     | 35.128       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.v_proj     | \u001b[92m0.0000001670\u001b[0m | 360000      | 0.01000     | 1.436     | 35.128       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.q_proj     | \u001b[92m0.0000004475\u001b[0m | 360000      | 0.01000     | 1.429     | 35.128       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.o_proj     | \u001b[92m0.0000000006\u001b[0m | 360000      | 0.01000     | 1.436     | 12.149       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.gate_proj        | \u001b[92m0.0000001622\u001b[0m | 360000      | 0.01000     | 1.599     | 35.460       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.up_proj          | \u001b[92m0.0000001380\u001b[0m | 360000      | 0.01000     | 1.580     | 35.460       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.down_proj        | \u001b[92m0.0000000050\u001b[0m | 360000      | 0.01000     | 5.130     | 70.031       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.k_proj     | \u001b[92m0.0000004128\u001b[0m | 360000      | 0.01000     | 1.441     | 35.092       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.v_proj     | \u001b[92m0.0000002222\u001b[0m | 360000      | 0.01000     | 1.420     | 35.092       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.q_proj     | \u001b[92m0.0000006136\u001b[0m | 360000      | 0.01000     | 1.422     | 35.092       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.o_proj     | \u001b[92m0.0000000004\u001b[0m | 360000      | 0.01000     | 1.440     | 12.164       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 25        | mlp.gate_proj        | \u001b[92m0.0000001956\u001b[0m | 360000      | 0.01000     | 1.578     | 35.455       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 25        | mlp.up_proj          | \u001b[92m0.0000001669\u001b[0m | 360000      | 0.01000     | 1.559     | 35.455       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 25        | mlp.down_proj        | \u001b[92m0.0000000113\u001b[0m | 360000      | 0.01000     | 5.127     | 70.088       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.k_proj     | \u001b[92m0.0000004516\u001b[0m | 360000      | 0.01000     | 1.439     | 35.185       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.v_proj     | \u001b[92m0.0000002413\u001b[0m | 360000      | 0.01000     | 1.415     | 35.185       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.q_proj     | \u001b[92m0.0000007358\u001b[0m | 360000      | 0.01000     | 1.462     | 35.185       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.o_proj     | \u001b[92m0.0000000011\u001b[0m | 360000      | 0.01000     | 1.455     | 12.172       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 26        | mlp.gate_proj        | \u001b[92m0.0000002319\u001b[0m | 360000      | 0.01000     | 1.580     | 35.454       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 26        | mlp.up_proj          | \u001b[92m0.0000001997\u001b[0m | 360000      | 0.01000     | 1.560     | 35.454       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 26        | mlp.down_proj        | \u001b[92m0.0000000111\u001b[0m | 360000      | 0.01000     | 5.207     | 69.936       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.k_proj     | \u001b[92m0.0000003996\u001b[0m | 360000      | 0.01000     | 1.505     | 35.086       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.v_proj     | \u001b[92m0.0000002368\u001b[0m | 360000      | 0.01000     | 1.443     | 35.086       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.q_proj     | \u001b[92m0.0000006919\u001b[0m | 360000      | 0.01000     | 1.459     | 35.086       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.o_proj     | \u001b[92m0.0000000015\u001b[0m | 360000      | 0.01000     | 1.415     | 12.204       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 27        | mlp.gate_proj        | \u001b[92m0.0000002771\u001b[0m | 360000      | 0.01000     | 1.576     | 35.532       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 27        | mlp.up_proj          | \u001b[92m0.0000002418\u001b[0m | 360000      | 0.01000     | 1.579     | 35.532       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 27        | mlp.down_proj        | \u001b[92m0.0000000208\u001b[0m | 360000      | 0.01000     | 5.026     | 70.025       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 28        | self_attn.k_proj     | \u001b[92m0.0000004459\u001b[0m | 360000      | 0.01000     | 1.395     | 35.090       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 28        | self_attn.v_proj     | \u001b[92m0.0000002828\u001b[0m | 360000      | 0.01000     | 1.391     | 35.090       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 28        | self_attn.q_proj     | \u001b[92m0.0000008630\u001b[0m | 360000      | 0.01000     | 1.415     | 35.090       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 28        | self_attn.o_proj     | \u001b[92m0.0000000019\u001b[0m | 360000      | 0.01000     | 1.414     | 12.185       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 28        | mlp.gate_proj        | \u001b[92m0.0000003142\u001b[0m | 360000      | 0.01000     | 1.573     | 35.420       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 28        | mlp.up_proj          | \u001b[92m0.0000002816\u001b[0m | 360000      | 0.01000     | 1.573     | 35.420       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 28        | mlp.down_proj        | \u001b[92m0.0000000225\u001b[0m | 360000      | 0.01000     | 5.078     | 70.027       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 29        | self_attn.k_proj     | \u001b[92m0.0000003612\u001b[0m | 360000      | 0.01000     | 1.419     | 35.121       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 29        | self_attn.v_proj     | \u001b[92m0.0000002423\u001b[0m | 360000      | 0.01000     | 1.404     | 35.121       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 29        | self_attn.q_proj     | \u001b[92m0.0000006459\u001b[0m | 360000      | 0.01000     | 1.418     | 35.121       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 29        | self_attn.o_proj     | \u001b[92m0.0000000014\u001b[0m | 360000      | 0.01000     | 1.420     | 12.170       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 29        | mlp.gate_proj        | \u001b[92m0.0000003510\u001b[0m | 360000      | 0.01000     | 1.582     | 35.471       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 29        | mlp.up_proj          | \u001b[92m0.0000003183\u001b[0m | 360000      | 0.01000     | 1.595     | 35.471       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 29        | mlp.down_proj        | \u001b[92m0.0000004553\u001b[0m | 360000      | 0.01000     | 5.040     | 70.170       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 30        | self_attn.k_proj     | \u001b[92m0.0000004091\u001b[0m | 360000      | 0.01000     | 1.417     | 35.146       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 30        | self_attn.v_proj     | \u001b[92m0.0000002931\u001b[0m | 360000      | 0.01000     | 1.501     | 35.146       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 30        | self_attn.q_proj     | \u001b[92m0.0000010165\u001b[0m | 360000      | 0.01000     | 1.437     | 35.146       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 30        | self_attn.o_proj     | \u001b[92m0.0000000043\u001b[0m | 360000      | 0.01000     | 1.437     | 12.165       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 30        | mlp.gate_proj        | \u001b[92m0.0000003948\u001b[0m | 360000      | 0.01000     | 1.566     | 35.465       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 30        | mlp.up_proj          | \u001b[92m0.0000003479\u001b[0m | 360000      | 0.01000     | 1.564     | 35.465       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 30        | mlp.down_proj        | \u001b[92m0.0007652142\u001b[0m | 360000      | 0.01000     | 5.109     | 66.998       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 31        | self_attn.k_proj     | \u001b[92m0.0000002038\u001b[0m | 360000      | 0.01000     | 1.436     | 35.146       | 399.52MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 31        | self_attn.v_proj     | \u001b[92m0.0000001131\u001b[0m | 360000      | 0.01000     | 1.408     | 35.146       | 399.52MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 31        | self_attn.q_proj     | \u001b[92m0.0000002449\u001b[0m | 360000      | 0.01000     | 1.402     | 35.146       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 31        | self_attn.o_proj     | \u001b[92m0.0000000211\u001b[0m | 360000      | 0.01000     | 1.420     | 12.193       | 399.52MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 31        | mlp.gate_proj        | \u001b[92m0.0000002693\u001b[0m | 360000      | 0.01000     | 1.572     | 35.563       | 399.52MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 31        | mlp.up_proj          | \u001b[92m0.0000002281\u001b[0m | 360000      | 0.01000     | 1.568     | 35.563       | 399.52MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 31        | mlp.down_proj        | \u001b[92m0.0000051541\u001b[0m | 360000      | 0.01000     | 5.052     | 70.525       | 399.52MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.0000000003', 'samples': '360000', 'damp': '0.01000', 'time': '1.876', 'fwd_time': '33.419', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.485', 'fwd_time': '33.419', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.0000000003', 'samples': '360000', 'damp': '0.01000', 'time': '1.469', 'fwd_time': '33.419', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.452', 'fwd_time': '12.200', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.608', 'fwd_time': '36.170', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.595', 'fwd_time': '36.170', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '5.324', 'fwd_time': '69.372', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.0000000022', 'samples': '360000', 'damp': '0.01000', 'time': '1.478', 'fwd_time': '35.023', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.561', 'fwd_time': '35.023', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.0000000020', 'samples': '360000', 'damp': '0.01000', 'time': '1.463', 'fwd_time': '35.023', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.506', 'fwd_time': '12.201', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.611', 'fwd_time': '35.636', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.682', 'fwd_time': '35.636', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '0.0001922218', 'samples': '360000', 'damp': '0.01000', 'time': '5.212', 'fwd_time': '66.512', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.0000000045', 'samples': '360000', 'damp': '0.01000', 'time': '1.538', 'fwd_time': '35.117', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.0000000007', 'samples': '360000', 'damp': '0.01000', 'time': '1.542', 'fwd_time': '35.117', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '0.0000000067', 'samples': '360000', 'damp': '0.01000', 'time': '1.604', 'fwd_time': '35.117', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.483', 'fwd_time': '12.210', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '0.0000000007', 'samples': '360000', 'damp': '0.01000', 'time': '1.679', 'fwd_time': '35.774', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '0.0000000006', 'samples': '360000', 'damp': '0.01000', 'time': '1.719', 'fwd_time': '35.774', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '5.306', 'fwd_time': '70.013', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.0000000270', 'samples': '360000', 'damp': '0.01000', 'time': '1.694', 'fwd_time': '35.432', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.0000000056', 'samples': '360000', 'damp': '0.01000', 'time': '1.627', 'fwd_time': '35.432', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '0.0000000496', 'samples': '360000', 'damp': '0.01000', 'time': '1.655', 'fwd_time': '35.432', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.587', 'fwd_time': '12.234', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '0.0000000025', 'samples': '360000', 'damp': '0.01000', 'time': '1.707', 'fwd_time': '36.129', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '0.0000000021', 'samples': '360000', 'damp': '0.01000', 'time': '1.702', 'fwd_time': '36.129', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '5.231', 'fwd_time': '69.883', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.0000000381', 'samples': '360000', 'damp': '0.01000', 'time': '1.574', 'fwd_time': '35.180', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.0000000072', 'samples': '360000', 'damp': '0.01000', 'time': '1.609', 'fwd_time': '35.180', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '0.0000000793', 'samples': '360000', 'damp': '0.01000', 'time': '1.639', 'fwd_time': '35.180', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.574', 'fwd_time': '12.236', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '0.0000000065', 'samples': '360000', 'damp': '0.01000', 'time': '1.748', 'fwd_time': '36.018', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '0.0000000052', 'samples': '360000', 'damp': '0.01000', 'time': '1.713', 'fwd_time': '36.018', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '5.497', 'fwd_time': '70.185', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.0000000378', 'samples': '360000', 'damp': '0.01000', 'time': '1.514', 'fwd_time': '35.191', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.0000000087', 'samples': '360000', 'damp': '0.01000', 'time': '1.518', 'fwd_time': '35.191', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '0.0000000522', 'samples': '360000', 'damp': '0.01000', 'time': '1.523', 'fwd_time': '35.191', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.496', 'fwd_time': '12.212', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '0.0000000085', 'samples': '360000', 'damp': '0.01000', 'time': '1.649', 'fwd_time': '35.678', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '0.0000000067', 'samples': '360000', 'damp': '0.01000', 'time': '1.620', 'fwd_time': '35.678', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '5.274', 'fwd_time': '69.959', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.0000000830', 'samples': '360000', 'damp': '0.01000', 'time': '1.498', 'fwd_time': '35.165', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.0000000216', 'samples': '360000', 'damp': '0.01000', 'time': '1.509', 'fwd_time': '35.165', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '0.0000001366', 'samples': '360000', 'damp': '0.01000', 'time': '1.491', 'fwd_time': '35.165', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.519', 'fwd_time': '12.202', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '0.0000000129', 'samples': '360000', 'damp': '0.01000', 'time': '1.729', 'fwd_time': '35.854', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '0.0000000098', 'samples': '360000', 'damp': '0.01000', 'time': '1.681', 'fwd_time': '35.854', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '5.577', 'fwd_time': '70.089', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.0000001018', 'samples': '360000', 'damp': '0.01000', 'time': '1.624', 'fwd_time': '35.565', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.0000000279', 'samples': '360000', 'damp': '0.01000', 'time': '1.724', 'fwd_time': '35.565', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '0.0000001425', 'samples': '360000', 'damp': '0.01000', 'time': '1.661', 'fwd_time': '35.565', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.628', 'fwd_time': '12.227', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '0.0000000166', 'samples': '360000', 'damp': '0.01000', 'time': '1.739', 'fwd_time': '36.068', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '0.0000000127', 'samples': '360000', 'damp': '0.01000', 'time': '1.732', 'fwd_time': '36.068', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '5.276', 'fwd_time': '69.876', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '0.0000001152', 'samples': '360000', 'damp': '0.01000', 'time': '1.483', 'fwd_time': '35.160', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.0000000305', 'samples': '360000', 'damp': '0.01000', 'time': '1.503', 'fwd_time': '35.160', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '0.0000001538', 'samples': '360000', 'damp': '0.01000', 'time': '1.452', 'fwd_time': '35.160', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.0000000000', 'samples': '360000', 'damp': '0.01000', 'time': '1.477', 'fwd_time': '12.167', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '0.0000000186', 'samples': '360000', 'damp': '0.01000', 'time': '1.632', 'fwd_time': '35.557', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '0.0000000150', 'samples': '360000', 'damp': '0.01000', 'time': '1.619', 'fwd_time': '35.557', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.0000000008', 'samples': '360000', 'damp': '0.01000', 'time': '5.273', 'fwd_time': '70.036', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.0000001463', 'samples': '360000', 'damp': '0.01000', 'time': '1.470', 'fwd_time': '35.191', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.0000000390', 'samples': '360000', 'damp': '0.01000', 'time': '1.472', 'fwd_time': '35.191', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '0.0000001813', 'samples': '360000', 'damp': '0.01000', 'time': '1.499', 'fwd_time': '35.191', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.456', 'fwd_time': '12.150', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '0.0000000208', 'samples': '360000', 'damp': '0.01000', 'time': '1.704', 'fwd_time': '35.522', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '0.0000000174', 'samples': '360000', 'damp': '0.01000', 'time': '1.658', 'fwd_time': '35.522', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '5.307', 'fwd_time': '70.112', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.0000002153', 'samples': '360000', 'damp': '0.01000', 'time': '1.540', 'fwd_time': '35.083', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.0000000607', 'samples': '360000', 'damp': '0.01000', 'time': '1.538', 'fwd_time': '35.083', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '0.0000002516', 'samples': '360000', 'damp': '0.01000', 'time': '1.477', 'fwd_time': '35.083', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.477', 'fwd_time': '12.179', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '0.0000000209', 'samples': '360000', 'damp': '0.01000', 'time': '1.625', 'fwd_time': '35.440', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '0.0000000179', 'samples': '360000', 'damp': '0.01000', 'time': '1.596', 'fwd_time': '35.440', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '5.192', 'fwd_time': '70.092', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '0.0000002159', 'samples': '360000', 'damp': '0.01000', 'time': '1.479', 'fwd_time': '35.157', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.0000000758', 'samples': '360000', 'damp': '0.01000', 'time': '1.505', 'fwd_time': '35.157', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '0.0000002729', 'samples': '360000', 'damp': '0.01000', 'time': '1.465', 'fwd_time': '35.157', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.481', 'fwd_time': '12.148', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '0.0000000252', 'samples': '360000', 'damp': '0.01000', 'time': '1.617', 'fwd_time': '35.596', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '0.0000000222', 'samples': '360000', 'damp': '0.01000', 'time': '1.624', 'fwd_time': '35.596', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.0000000004', 'samples': '360000', 'damp': '0.01000', 'time': '5.273', 'fwd_time': '70.189', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '0.0000002296', 'samples': '360000', 'damp': '0.01000', 'time': '1.508', 'fwd_time': '35.149', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.0000000772', 'samples': '360000', 'damp': '0.01000', 'time': '1.461', 'fwd_time': '35.149', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '0.0000002472', 'samples': '360000', 'damp': '0.01000', 'time': '1.550', 'fwd_time': '35.149', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.477', 'fwd_time': '12.207', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '0.0000000273', 'samples': '360000', 'damp': '0.01000', 'time': '1.602', 'fwd_time': '35.435', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '0.0000000245', 'samples': '360000', 'damp': '0.01000', 'time': '1.607', 'fwd_time': '35.435', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '5.270', 'fwd_time': '70.061', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '0.0000002066', 'samples': '360000', 'damp': '0.01000', 'time': '1.480', 'fwd_time': '35.094', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.0000000742', 'samples': '360000', 'damp': '0.01000', 'time': '1.488', 'fwd_time': '35.094', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '0.0000002708', 'samples': '360000', 'damp': '0.01000', 'time': '1.473', 'fwd_time': '35.094', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.0000000001', 'samples': '360000', 'damp': '0.01000', 'time': '1.554', 'fwd_time': '12.144', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '0.0000000302', 'samples': '360000', 'damp': '0.01000', 'time': '1.633', 'fwd_time': '35.505', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '0.0000000280', 'samples': '360000', 'damp': '0.01000', 'time': '1.643', 'fwd_time': '35.505', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.0000000004', 'samples': '360000', 'damp': '0.01000', 'time': '5.374', 'fwd_time': '70.102', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '0.0000002500', 'samples': '360000', 'damp': '0.01000', 'time': '1.515', 'fwd_time': '35.135', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.0000000789', 'samples': '360000', 'damp': '0.01000', 'time': '1.519', 'fwd_time': '35.135', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '0.0000002798', 'samples': '360000', 'damp': '0.01000', 'time': '1.528', 'fwd_time': '35.135', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.0000000002', 'samples': '360000', 'damp': '0.01000', 'time': '1.535', 'fwd_time': '12.180', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '0.0000000348', 'samples': '360000', 'damp': '0.01000', 'time': '1.631', 'fwd_time': '35.535', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '0.0000000322', 'samples': '360000', 'damp': '0.01000', 'time': '1.594', 'fwd_time': '35.535', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.0000000003', 'samples': '360000', 'damp': '0.01000', 'time': '5.216', 'fwd_time': '70.116', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '0.0000001730', 'samples': '360000', 'damp': '0.01000', 'time': '1.492', 'fwd_time': '35.132', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.0000000667', 'samples': '360000', 'damp': '0.01000', 'time': '1.476', 'fwd_time': '35.132', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '0.0000001838', 'samples': '360000', 'damp': '0.01000', 'time': '1.511', 'fwd_time': '35.132', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.0000000004', 'samples': '360000', 'damp': '0.01000', 'time': '1.471', 'fwd_time': '12.218', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '0.0000000412', 'samples': '360000', 'damp': '0.01000', 'time': '1.687', 'fwd_time': '35.632', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '0.0000000387', 'samples': '360000', 'damp': '0.01000', 'time': '1.612', 'fwd_time': '35.632', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.0000000006', 'samples': '360000', 'damp': '0.01000', 'time': '5.204', 'fwd_time': '70.115', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '0.0000001827', 'samples': '360000', 'damp': '0.01000', 'time': '1.557', 'fwd_time': '35.202', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.0000000746', 'samples': '360000', 'damp': '0.01000', 'time': '1.518', 'fwd_time': '35.202', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '0.0000002094', 'samples': '360000', 'damp': '0.01000', 'time': '1.480', 'fwd_time': '35.202', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.0000000004', 'samples': '360000', 'damp': '0.01000', 'time': '1.455', 'fwd_time': '12.190', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '0.0000000515', 'samples': '360000', 'damp': '0.01000', 'time': '1.613', 'fwd_time': '35.410', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '0.0000000475', 'samples': '360000', 'damp': '0.01000', 'time': '1.594', 'fwd_time': '35.410', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '0.0000000143', 'samples': '360000', 'damp': '0.01000', 'time': '5.268', 'fwd_time': '70.240', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '0.0000002567', 'samples': '360000', 'damp': '0.01000', 'time': '1.593', 'fwd_time': '35.317', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.0000000930', 'samples': '360000', 'damp': '0.01000', 'time': '1.583', 'fwd_time': '35.317', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '0.0000003577', 'samples': '360000', 'damp': '0.01000', 'time': '1.618', 'fwd_time': '35.317', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.0000000003', 'samples': '360000', 'damp': '0.01000', 'time': '1.572', 'fwd_time': '12.255', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '0.0000000628', 'samples': '360000', 'damp': '0.01000', 'time': '1.641', 'fwd_time': '35.881', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '0.0000000564', 'samples': '360000', 'damp': '0.01000', 'time': '1.639', 'fwd_time': '35.881', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '0.0000000011', 'samples': '360000', 'damp': '0.01000', 'time': '5.279', 'fwd_time': '70.036', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.0000002802', 'samples': '360000', 'damp': '0.01000', 'time': '1.539', 'fwd_time': '35.077', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.0000001161', 'samples': '360000', 'damp': '0.01000', 'time': '1.442', 'fwd_time': '35.077', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '0.0000004117', 'samples': '360000', 'damp': '0.01000', 'time': '1.468', 'fwd_time': '35.077', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.0000000004', 'samples': '360000', 'damp': '0.01000', 'time': '1.472', 'fwd_time': '12.218', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '0.0000000777', 'samples': '360000', 'damp': '0.01000', 'time': '1.589', 'fwd_time': '35.619', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '0.0000000682', 'samples': '360000', 'damp': '0.01000', 'time': '1.568', 'fwd_time': '35.619', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '0.0000001651', 'samples': '360000', 'damp': '0.01000', 'time': '5.139', 'fwd_time': '70.386', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '0.0000002263', 'samples': '360000', 'damp': '0.01000', 'time': '1.468', 'fwd_time': '35.120', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.0000000954', 'samples': '360000', 'damp': '0.01000', 'time': '1.443', 'fwd_time': '35.120', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '0.0000003461', 'samples': '360000', 'damp': '0.01000', 'time': '1.445', 'fwd_time': '35.120', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '0.0000000002', 'samples': '360000', 'damp': '0.01000', 'time': '1.436', 'fwd_time': '12.179', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '0.0000000852', 'samples': '360000', 'damp': '0.01000', 'time': '1.576', 'fwd_time': '35.391', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '0.0000000743', 'samples': '360000', 'damp': '0.01000', 'time': '1.624', 'fwd_time': '35.391', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '0.0000000258', 'samples': '360000', 'damp': '0.01000', 'time': '5.112', 'fwd_time': '70.515', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.0000002597', 'samples': '360000', 'damp': '0.01000', 'time': '1.430', 'fwd_time': '35.172', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.0000001002', 'samples': '360000', 'damp': '0.01000', 'time': '1.423', 'fwd_time': '35.172', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '0.0000003968', 'samples': '360000', 'damp': '0.01000', 'time': '1.460', 'fwd_time': '35.172', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '0.0000000003', 'samples': '360000', 'damp': '0.01000', 'time': '1.421', 'fwd_time': '12.180', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '0.0000000904', 'samples': '360000', 'damp': '0.01000', 'time': '1.612', 'fwd_time': '35.433', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '0.0000000784', 'samples': '360000', 'damp': '0.01000', 'time': '1.630', 'fwd_time': '35.433', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '0.0000000031', 'samples': '360000', 'damp': '0.01000', 'time': '5.082', 'fwd_time': '70.160', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.0000002449', 'samples': '360000', 'damp': '0.01000', 'time': '1.466', 'fwd_time': '35.176', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.0000001140', 'samples': '360000', 'damp': '0.01000', 'time': '1.529', 'fwd_time': '35.176', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '0.0000003781', 'samples': '360000', 'damp': '0.01000', 'time': '1.453', 'fwd_time': '35.176', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '0.0000000002', 'samples': '360000', 'damp': '0.01000', 'time': '1.433', 'fwd_time': '12.176', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '0.0000001068', 'samples': '360000', 'damp': '0.01000', 'time': '1.571', 'fwd_time': '35.473', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '0.0000000913', 'samples': '360000', 'damp': '0.01000', 'time': '1.586', 'fwd_time': '35.473', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '0.0000000033', 'samples': '360000', 'damp': '0.01000', 'time': '5.142', 'fwd_time': '70.145', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '0.0000002820', 'samples': '360000', 'damp': '0.01000', 'time': '1.452', 'fwd_time': '35.218', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '0.0000001299', 'samples': '360000', 'damp': '0.01000', 'time': '1.533', 'fwd_time': '35.218', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '0.0000004096', 'samples': '360000', 'damp': '0.01000', 'time': '1.459', 'fwd_time': '35.218', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.o_proj', 'loss': '0.0000000007', 'samples': '360000', 'damp': '0.01000', 'time': '1.423', 'fwd_time': '12.182', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.gate_proj', 'loss': '0.0000001179', 'samples': '360000', 'damp': '0.01000', 'time': '1.594', 'fwd_time': '35.437', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.up_proj', 'loss': '0.0000000994', 'samples': '360000', 'damp': '0.01000', 'time': '1.568', 'fwd_time': '35.437', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.down_proj', 'loss': '0.0000000036', 'samples': '360000', 'damp': '0.01000', 'time': '5.158', 'fwd_time': '70.024', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '0.0000003055', 'samples': '360000', 'damp': '0.01000', 'time': '1.450', 'fwd_time': '35.209', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '0.0000001568', 'samples': '360000', 'damp': '0.01000', 'time': '1.452', 'fwd_time': '35.209', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '0.0000004448', 'samples': '360000', 'damp': '0.01000', 'time': '1.490', 'fwd_time': '35.209', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.o_proj', 'loss': '0.0000000008', 'samples': '360000', 'damp': '0.01000', 'time': '1.443', 'fwd_time': '12.166', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.gate_proj', 'loss': '0.0000001405', 'samples': '360000', 'damp': '0.01000', 'time': '1.583', 'fwd_time': '35.447', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.up_proj', 'loss': '0.0000001205', 'samples': '360000', 'damp': '0.01000', 'time': '1.563', 'fwd_time': '35.447', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.down_proj', 'loss': '0.0000000051', 'samples': '360000', 'damp': '0.01000', 'time': '5.180', 'fwd_time': '70.169', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.k_proj', 'loss': '0.0000003376', 'samples': '360000', 'damp': '0.01000', 'time': '1.475', 'fwd_time': '35.128', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.v_proj', 'loss': '0.0000001670', 'samples': '360000', 'damp': '0.01000', 'time': '1.436', 'fwd_time': '35.128', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.q_proj', 'loss': '0.0000004475', 'samples': '360000', 'damp': '0.01000', 'time': '1.429', 'fwd_time': '35.128', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.o_proj', 'loss': '0.0000000006', 'samples': '360000', 'damp': '0.01000', 'time': '1.436', 'fwd_time': '12.149', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'mlp.gate_proj', 'loss': '0.0000001622', 'samples': '360000', 'damp': '0.01000', 'time': '1.599', 'fwd_time': '35.460', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'mlp.up_proj', 'loss': '0.0000001380', 'samples': '360000', 'damp': '0.01000', 'time': '1.580', 'fwd_time': '35.460', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'mlp.down_proj', 'loss': '0.0000000050', 'samples': '360000', 'damp': '0.01000', 'time': '5.130', 'fwd_time': '70.031', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.k_proj', 'loss': '0.0000004128', 'samples': '360000', 'damp': '0.01000', 'time': '1.441', 'fwd_time': '35.092', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.v_proj', 'loss': '0.0000002222', 'samples': '360000', 'damp': '0.01000', 'time': '1.420', 'fwd_time': '35.092', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.q_proj', 'loss': '0.0000006136', 'samples': '360000', 'damp': '0.01000', 'time': '1.422', 'fwd_time': '35.092', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.o_proj', 'loss': '0.0000000004', 'samples': '360000', 'damp': '0.01000', 'time': '1.440', 'fwd_time': '12.164', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'mlp.gate_proj', 'loss': '0.0000001956', 'samples': '360000', 'damp': '0.01000', 'time': '1.578', 'fwd_time': '35.455', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'mlp.up_proj', 'loss': '0.0000001669', 'samples': '360000', 'damp': '0.01000', 'time': '1.559', 'fwd_time': '35.455', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'mlp.down_proj', 'loss': '0.0000000113', 'samples': '360000', 'damp': '0.01000', 'time': '5.127', 'fwd_time': '70.088', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.k_proj', 'loss': '0.0000004516', 'samples': '360000', 'damp': '0.01000', 'time': '1.439', 'fwd_time': '35.185', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.v_proj', 'loss': '0.0000002413', 'samples': '360000', 'damp': '0.01000', 'time': '1.415', 'fwd_time': '35.185', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.q_proj', 'loss': '0.0000007358', 'samples': '360000', 'damp': '0.01000', 'time': '1.462', 'fwd_time': '35.185', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.o_proj', 'loss': '0.0000000011', 'samples': '360000', 'damp': '0.01000', 'time': '1.455', 'fwd_time': '12.172', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'mlp.gate_proj', 'loss': '0.0000002319', 'samples': '360000', 'damp': '0.01000', 'time': '1.580', 'fwd_time': '35.454', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'mlp.up_proj', 'loss': '0.0000001997', 'samples': '360000', 'damp': '0.01000', 'time': '1.560', 'fwd_time': '35.454', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'mlp.down_proj', 'loss': '0.0000000111', 'samples': '360000', 'damp': '0.01000', 'time': '5.207', 'fwd_time': '69.936', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.k_proj', 'loss': '0.0000003996', 'samples': '360000', 'damp': '0.01000', 'time': '1.505', 'fwd_time': '35.086', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.v_proj', 'loss': '0.0000002368', 'samples': '360000', 'damp': '0.01000', 'time': '1.443', 'fwd_time': '35.086', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.q_proj', 'loss': '0.0000006919', 'samples': '360000', 'damp': '0.01000', 'time': '1.459', 'fwd_time': '35.086', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.o_proj', 'loss': '0.0000000015', 'samples': '360000', 'damp': '0.01000', 'time': '1.415', 'fwd_time': '12.204', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'mlp.gate_proj', 'loss': '0.0000002771', 'samples': '360000', 'damp': '0.01000', 'time': '1.576', 'fwd_time': '35.532', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'mlp.up_proj', 'loss': '0.0000002418', 'samples': '360000', 'damp': '0.01000', 'time': '1.579', 'fwd_time': '35.532', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'mlp.down_proj', 'loss': '0.0000000208', 'samples': '360000', 'damp': '0.01000', 'time': '5.026', 'fwd_time': '70.025', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.k_proj', 'loss': '0.0000004459', 'samples': '360000', 'damp': '0.01000', 'time': '1.395', 'fwd_time': '35.090', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.v_proj', 'loss': '0.0000002828', 'samples': '360000', 'damp': '0.01000', 'time': '1.391', 'fwd_time': '35.090', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.q_proj', 'loss': '0.0000008630', 'samples': '360000', 'damp': '0.01000', 'time': '1.415', 'fwd_time': '35.090', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.o_proj', 'loss': '0.0000000019', 'samples': '360000', 'damp': '0.01000', 'time': '1.414', 'fwd_time': '12.185', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'mlp.gate_proj', 'loss': '0.0000003142', 'samples': '360000', 'damp': '0.01000', 'time': '1.573', 'fwd_time': '35.420', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'mlp.up_proj', 'loss': '0.0000002816', 'samples': '360000', 'damp': '0.01000', 'time': '1.573', 'fwd_time': '35.420', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 28, 'module': 'mlp.down_proj', 'loss': '0.0000000225', 'samples': '360000', 'damp': '0.01000', 'time': '5.078', 'fwd_time': '70.027', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.k_proj', 'loss': '0.0000003612', 'samples': '360000', 'damp': '0.01000', 'time': '1.419', 'fwd_time': '35.121', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.v_proj', 'loss': '0.0000002423', 'samples': '360000', 'damp': '0.01000', 'time': '1.404', 'fwd_time': '35.121', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.q_proj', 'loss': '0.0000006459', 'samples': '360000', 'damp': '0.01000', 'time': '1.418', 'fwd_time': '35.121', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.o_proj', 'loss': '0.0000000014', 'samples': '360000', 'damp': '0.01000', 'time': '1.420', 'fwd_time': '12.170', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'mlp.gate_proj', 'loss': '0.0000003510', 'samples': '360000', 'damp': '0.01000', 'time': '1.582', 'fwd_time': '35.471', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'mlp.up_proj', 'loss': '0.0000003183', 'samples': '360000', 'damp': '0.01000', 'time': '1.595', 'fwd_time': '35.471', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 29, 'module': 'mlp.down_proj', 'loss': '0.0000004553', 'samples': '360000', 'damp': '0.01000', 'time': '5.040', 'fwd_time': '70.170', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'self_attn.k_proj', 'loss': '0.0000004091', 'samples': '360000', 'damp': '0.01000', 'time': '1.417', 'fwd_time': '35.146', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'self_attn.v_proj', 'loss': '0.0000002931', 'samples': '360000', 'damp': '0.01000', 'time': '1.501', 'fwd_time': '35.146', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'self_attn.q_proj', 'loss': '0.0000010165', 'samples': '360000', 'damp': '0.01000', 'time': '1.437', 'fwd_time': '35.146', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'self_attn.o_proj', 'loss': '0.0000000043', 'samples': '360000', 'damp': '0.01000', 'time': '1.437', 'fwd_time': '12.165', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'mlp.gate_proj', 'loss': '0.0000003948', 'samples': '360000', 'damp': '0.01000', 'time': '1.566', 'fwd_time': '35.465', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'mlp.up_proj', 'loss': '0.0000003479', 'samples': '360000', 'damp': '0.01000', 'time': '1.564', 'fwd_time': '35.465', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 30, 'module': 'mlp.down_proj', 'loss': '0.0007652142', 'samples': '360000', 'damp': '0.01000', 'time': '5.109', 'fwd_time': '66.998', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'self_attn.k_proj', 'loss': '0.0000002038', 'samples': '360000', 'damp': '0.01000', 'time': '1.436', 'fwd_time': '35.146', 'max_vram': '399.52MB, 424.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'self_attn.v_proj', 'loss': '0.0000001131', 'samples': '360000', 'damp': '0.01000', 'time': '1.408', 'fwd_time': '35.146', 'max_vram': '399.52MB, 232.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'self_attn.q_proj', 'loss': '0.0000002449', 'samples': '360000', 'damp': '0.01000', 'time': '1.402', 'fwd_time': '35.146', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'self_attn.o_proj', 'loss': '0.0000000211', 'samples': '360000', 'damp': '0.01000', 'time': '1.420', 'fwd_time': '12.193', 'max_vram': '399.52MB, 40.19MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'mlp.gate_proj', 'loss': '0.0000002693', 'samples': '360000', 'damp': '0.01000', 'time': '1.572', 'fwd_time': '35.563', 'max_vram': '399.52MB, 394.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'mlp.up_proj', 'loss': '0.0000002281', 'samples': '360000', 'damp': '0.01000', 'time': '1.568', 'fwd_time': '35.563', 'max_vram': '399.52MB, 94.29MB'}\n\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 31, 'module': 'mlp.down_proj', 'loss': '0.0000051541', 'samples': '360000', 'damp': '0.01000', 'time': '5.052', 'fwd_time': '70.525', 'max_vram': '399.52MB, 94.19MB'}\n\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n\u001b[32mINFO\u001b[0m  Packing Kernel: selected: `TorchQuantLinear`                                                 \n\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n{\n  \"bits\": 4,\n  \"group_size\": -1,\n  \"desc_act\": true,\n  \"sym\": true,\n  \"lm_head\": false,\n  \"quant_method\": \"gptq\",\n  \"checkpoint_format\": \"gptq\",\n  \"pack_dtype\": \"int32\",\n  \"meta\": {\n    \"quantizer\": [\n      \"gptqmodel:3.0.0-dev\"\n    ],\n    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n    \"damp_percent\": 0.01,\n    \"damp_auto_increment\": 0.0025,\n    \"static_groups\": false,\n    \"true_sequential\": true,\n    \"mse\": 0.0\n  },\n  \"beta\": 0.2,\n  \"tau\": 1.0,\n  \"gamma\": 0.0\n}\nFiles in directory:\nquant_log.csv\ngeneration_config.json\nquantize_config.json\nconfig.json\nContent of saved `generation_config.json`:\n{\n    \"bos_token_id\": 1,\n    \"do_sample\": true,\n    \"eos_token_id\": 2,\n    \"max_length\": 4096,\n    \"pad_token_id\": 0,\n    \"temperature\": 0.6,\n    \"top_p\": 0.9,\n    \"transformers_version\": \"4.51.3\"\n}\nContent of saved `config.json`:\n{\n    \"architectures\": [\n        \"LlamaForCausalLM\"\n    ],\n    \"attention_bias\": false,\n    \"attention_dropout\": 0.0,\n    \"bos_token_id\": 1,\n    \"eos_token_id\": 2,\n    \"head_dim\": 128,\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 4096,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 11008,\n    \"max_position_embeddings\": 4096,\n    \"mlp_bias\": false,\n    \"model_type\": \"llama\",\n    \"num_attention_heads\": 32,\n    \"num_hidden_layers\": 32,\n    \"num_key_value_heads\": 32,\n    \"pretraining_tp\": 1,\n    \"quantization_config\": {\n        \"beta\": 0.2,\n        \"bits\": 4,\n        \"checkpoint_format\": \"gptq\",\n        \"desc_act\": true,\n        \"gamma\": 0.0,\n        \"group_size\": -1,\n        \"lm_head\": false,\n        \"meta\": {\n            \"damp_auto_increment\": 0.0025,\n            \"damp_percent\": 0.01,\n            \"mse\": 0.0,\n            \"quantizer\": [\n                \"gptqmodel:3.0.0-dev\"\n            ],\n            \"static_groups\": false,\n            \"true_sequential\": true,\n            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n        },\n        \"pack_dtype\": \"int32\",\n        \"quant_method\": \"gptq\",\n        \"sym\": true,\n        \"tau\": 1.0\n    },\n    \"rms_norm_eps\": 1e-05,\n    \"rope_scaling\": null,\n    \"rope_theta\": 10000.0,\n    \"tie_word_embeddings\": false,\n    \"torch_dtype\": \"float16\",\n    \"transformers_version\": \"4.51.3\",\n    \"use_cache\": true,\n    \"vocab_size\": 32000\n}\n\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 25705.17MB, 25.10GB                                                \n\u001b[32mINFO\u001b[0m  Quantized model size: 3596.20MB, 3.51GB                                                      \n\u001b[32mINFO\u001b[0m  Size difference: 22108.98MB, 21.59GB - 86.01%                                                \n   Quantization complete — saved to /kaggle/working/llama2-7b-quant-b0.2-t0.5\n\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\nIf you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                                         \n\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4 bpw, based on [bits: 4, group_size: -1]      \n\u001b[32mINFO\u001b[0m   Kernel: selected: `TorchQuantLinear`                                                        \n\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2\n}\n\n\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.   \n\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n","output_type":"stream"},{"name":"stderr","text":"Evaluating PPL: 100%|██████████| 81/81 [05:01<00:00,  3.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"     Eval complete: loss=1.7466, ppl=5.74\n|   beta |   tau |   loss |     ppl |\n|-------:|------:|-------:|--------:|\n|    0.2 |   0.5 | 1.7466 | 5.73506 |\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Plotting","metadata":{"id":"MxEokE-TRC1H"}},{"cell_type":"code","source":"df2 = pd.DataFrame(results2)\n\n# Constants\nconst_tau = 0.5\ndf_beta = df[df['tau'] == const_tau].reset_index(drop=True)\n\nbest_beta = df_beta.loc[df_beta['ppl'].idxmin(), 'beta']\ndf_tau = df[df['beta'] == best_beta].reset_index(drop=True)\n\ndef plot_zoomed_bar(x, y, xlabel, ylabel, title, cmap):\n    colors = cmap(np.linspace(0, 1, len(x)))\n    fig, ax = plt.subplots(figsize=(8, 4))\n    bars = ax.bar(x, y, color=colors, edgecolor='black', linewidth=0.8)\n\n    ax.set_title(title, fontsize=14)\n    ax.set_xlabel(xlabel, fontsize=12)\n    ax.set_ylabel(ylabel, fontsize=12)\n    ax.grid(axis='y', linestyle='--', alpha=0.6)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    y_min, y_max = y.min(), y.max()\n    margin = (y_max - y_min) * 0.15\n    ax.set_ylim(y_min - margin, y_max + margin)\n\n    plt.tight_layout()\n\nplot_zoomed_bar(\n    x=df_beta['beta'].astype(str),\n    y=df_beta['ppl'],\n    xlabel='β (Beta values)',\n    ylabel='Perplexity',\n    title='Perplexity vs Beta @ τ = 0.5',\n    cmap=plt.cm.Set2\n)\n\nplot_zoomed_bar(\n    x=df_beta['beta'].astype(str),\n    y=df_beta['loss'],\n    xlabel='β (Beta values)',\n    ylabel='Avg NLL Loss',\n    title='Loss vs Beta @ τ = 0.5',\n    cmap=plt.cm.Pastel1\n)\n\nplot_zoomed_bar(\n    x=df_tau['tau'].astype(str),\n    y=df_tau['ppl'],\n    xlabel='τ (Tau values)',\n    ylabel='Perplexity',\n    title=f'Perplexity vs Tau @ β = {best_beta}',\n    cmap=plt.cm.Pastel2\n)\n\nplot_zoomed_bar(\n    x=df_tau['tau'].astype(str),\n    y=df_tau['loss'],\n    xlabel='τ (Tau values)',\n    ylabel='Avg NLL Loss',\n    title=f'Loss vs Tau @ β = {best_beta}',\n    cmap=plt.cm.Dark2\n)\n\nplt.show()","metadata":{"trusted":true,"id":"TqC7GYJJRC1K"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Debugging Only. If Model is saved in path","metadata":{"id":"NYpbaTyJjUYW"}},{"cell_type":"code","source":"def clear_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\nfrom torch.nn import CrossEntropyLoss\ndef load_quantized_model(quant_path: str, model_id: str, quant_cfg: QuantizeConfig):\n    \"\"\"Load the already‐quantized weights from disk.\"\"\"\n    return GPTQModel.from_pretrained(\n        quant_path,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        quantize_config=quant_cfg\n    )\n\n# ─── MagR-style block evaluation ────────────────────────────────────────────────────\ndef eval_magr(model, tokenizer, eval_texts, max_len, batch_size=8):\n    device = next(model.parameters()).device\n    # build flat token list\n    all_ids = []\n    for txt in eval_texts:\n        if txt.strip():\n            all_ids += tokenizer.encode(txt, add_special_tokens=False)\n    # slice into non-overlapping blocks\n    total = (len(all_ids) // max_len) * max_len\n    ids = torch.tensor(all_ids[:total], device=device).view(-1, max_len)\n    loss_fct = CrossEntropyLoss(reduction=\"none\")\n    nll = count = 0\n    for i in tqdm(range(0, ids.size(0), batch_size), desc=\"MagR PPL\"):\n        batch = ids[i : i + batch_size]\n        attn  = torch.ones_like(batch, device=device)\n        labels = batch.clone()\n        labels[:, :-1] = batch[:, 1:]\n        labels[:, -1]  = -100\n        with torch.no_grad():\n            out    = model(input_ids=batch, attention_mask=attn, labels=labels)\n            logits = out.logits[:, :-1, :].reshape(-1, out.logits.size(-1))\n            labs   = labels[:, :-1].reshape(-1)\n            nlls   = loss_fct(logits, labs) * (labs != -100).float()\n        nll   += nlls.sum().item()\n        count += (labs != -100).sum().item()\n    avg_nll = nll / count\n    return math.exp(avg_nll), avg_nll\n\n# ─── Simple pad-masked evaluation ───────────────────────────────────────────────────\ndef eval_padmask(model, tokenizer, eval_texts, max_len, batch_size=8):\n    device = next(model.parameters()).device\n    # tokenize with padding\n    enc = tokenizer(eval_texts,\n                    return_tensors=\"pt\",\n                    padding=\"longest\",\n                    truncation=True,\n                    max_length=max_len)\n    input_ids = enc.input_ids.to(device)\n    attn_mask = enc.attention_mask.to(device)\n    loss_fct = CrossEntropyLoss(ignore_index=-100, reduction=\"sum\")\n    total_nll = 0.0\n    total_tokens = 0\n    for i in tqdm(range(0, input_ids.size(0), batch_size), desc=\"Pad-mask PPL\"):\n        b_ids  = input_ids[i : i + batch_size]\n        b_mask = attn_mask[i : i + batch_size]\n        labels = b_ids.clone()\n        labels[b_mask == 0] = -100\n        with torch.no_grad():\n            out = model(input_ids=b_ids, attention_mask=b_mask, labels=labels)\n        # out.loss is mean over non-ignored tokens\n        num = (labels != -100).sum().item()\n        total_nll += out.loss.item() * num\n        total_tokens += num\n    avg_nll = total_nll / total_tokens\n    return math.exp(avg_nll), avg_nll\n\n# ─── Main: load and run ─────────────────────────────────────────────────────────────\nmodel_id   = \"meta-llama/Llama-2-7b-hf\"\nquant_path = \"/content/llama2-7b-quant-b0.0-t0.5\"  # your 4-bit checkpoint dir\ntokenizer  = AutoTokenizer.from_pretrained(model_id, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\nmax_len    = tokenizer.model_max_length  # 1024 for GPT-2\nbatch_size = 8\nclear_memory()\n\n# calibration config just to pass through loader (not used during eval)\nquant_cfg = QuantizeConfig(bits=4, group_size=-1, beta=0.0, tau=0.5)\n\n# load eval texts once\nfrom datasets import load_dataset\nds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\neval_texts = [t for t in ds[\"text\"] if t.strip()]\n\nclear_memory()\nmodel = load_quantized_model(quant_path, model_id, quant_cfg)\n\n# MagR-style\nmagr_ppl, magr_nll = eval_magr(model, tokenizer, eval_texts, max_len, batch_size)\nprint(f\"MagR-style → PPL: {magr_ppl:.2f}, avg NLL: {magr_nll:.4f}\")\n\n# Pad-mask style\npad_ppl, pad_nll = eval_padmask(model, tokenizer, eval_texts, max_len, batch_size)\nprint(f\"Pad-mask → PPL: {pad_ppl:.2f}, avg NLL: {pad_nll:.4f}\")\n\n# cleanup\ndel model\nclear_memory()","metadata":{"id":"Dv0LiD7ZXHSv","trusted":true},"outputs":[],"execution_count":null}]}