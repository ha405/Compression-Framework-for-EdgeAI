{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r '/kaggle/input/gptqmain/GPTQModel-main/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:23:47.875493Z",
     "iopub.status.busy": "2025-05-06T04:23:47.874704Z",
     "iopub.status.idle": "2025-05-06T04:23:48.548992Z",
     "shell.execute_reply": "2025-05-06T04:23:48.548286Z",
     "shell.execute_reply.started": "2025-05-06T04:23:47.875453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/klawq-gptq /kaggle/working/gptqvanilla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:24:19.867187Z",
     "iopub.status.busy": "2025-05-06T04:24:19.866594Z",
     "iopub.status.idle": "2025-05-06T04:24:19.872836Z",
     "shell.execute_reply": "2025-05-06T04:24:19.872092Z",
     "shell.execute_reply.started": "2025-05-06T04:24:19.867162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found gptq.py at: /kaggle/working/gptqvanilla2/gptqmodel/quantization/gptq.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "library_path = \"/kaggle/working/gptqvanilla2\" \n",
    "if library_path not in sys.path:\n",
    "     sys.path.insert(0, library_path)\n",
    "     print(f\"Added '{library_path}' to sys.path\")\n",
    "\n",
    "gptq_file_path = os.path.join(library_path, 'gptqmodel', 'quantization', 'gptq.py') \n",
    "if os.path.exists(gptq_file_path):\n",
    "    print(f\"Found gptq.py at: {gptq_file_path}\")\n",
    "from gptqmodel import GPTQModel, QuantizeConfig \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:24:31.719453Z",
     "iopub.status.busy": "2025-05-06T04:24:31.719157Z",
     "iopub.status.idle": "2025-05-06T04:24:31.723320Z",
     "shell.execute_reply": "2025-05-06T04:24:31.722657Z",
     "shell.execute_reply.started": "2025-05-06T04:24:31.719430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id   = \"gpt2\"                          \n",
    "quant_path = \"gpt2-text-calibrated\" \n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:24:34.499235Z",
     "iopub.status.busy": "2025-05-06T04:24:34.498622Z",
     "iopub.status.idle": "2025-05-06T04:24:34.502937Z",
     "shell.execute_reply": "2025-05-06T04:24:34.502366Z",
     "shell.execute_reply.started": "2025-05-06T04:24:34.499209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import islice\n",
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:26:02.342687Z",
     "iopub.status.busy": "2025-05-06T04:26:02.341954Z",
     "iopub.status.idle": "2025-05-06T04:26:04.806097Z",
     "shell.execute_reply": "2025-05-06T04:26:04.805329Z",
     "shell.execute_reply.started": "2025-05-06T04:26:02.342663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855828af06be4abd8f2f7fd049fe559d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25d3de41df7465bb81d16e757b3127e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1024 C4 examples for calibration.\n",
      "Using 1949 non-empty lines from Wikitext-2 for evaluation.\n"
     ]
    }
   ],
   "source": [
    "c4_stream = load_dataset(\n",
    "    \"allenai/c4\", \n",
    "    \"en\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ")\n",
    "num_calibration_samples = 1024\n",
    "calibration_dataset_c4 = []\n",
    "for sample in islice(c4_stream, num_calibration_samples):\n",
    "    text = sample.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        continue\n",
    "    calibration_dataset_c4.append({\"content\": text})\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(calibration_dataset_c4)} C4 examples for calibration.\")\n",
    "wt2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation[:3000]\")\n",
    "eval_texts = [ex[\"text\"] for ex in wt2 if ex[\"text\"].strip()]\n",
    "\n",
    "print(f\"Using {len(eval_texts)} non-empty lines from Wikitext-2 for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:25:02.839073Z",
     "iopub.status.busy": "2025-05-06T04:25:02.838518Z",
     "iopub.status.idle": "2025-05-06T04:25:03.159178Z",
     "shell.execute_reply": "2025-05-06T04:25:03.158350Z",
     "shell.execute_reply.started": "2025-05-06T04:25:02.839017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU VRAM and cache cleared.\n"
     ]
    }
   ],
   "source": [
    "def clear_gpu_cache():\n",
    "    gc.collect()  \n",
    "    torch.cuda.empty_cache()  \n",
    "    torch.cuda.ipc_collect()  \n",
    "    print(\"✅ GPU VRAM and cache cleared.\")\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:26:22.814471Z",
     "iopub.status.busy": "2025-05-06T04:26:22.813865Z",
     "iopub.status.idle": "2025-05-06T04:26:22.822420Z",
     "shell.execute_reply": "2025-05-06T04:26:22.821626Z",
     "shell.execute_reply.started": "2025-05-06T04:26:22.814446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clear_quant_path(path=None):\n",
    "    \"\"\"\n",
    "    Remove existing quantization directory (if given) and clear GPU memory.\n",
    "    \"\"\"\n",
    "    if path and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def quantize_and_eval(model_id, calib_tokenized, eval_texts, beta, tau, quant_path, batch_size=8):\n",
    "    \"\"\"\n",
    "    Quantize the model with given beta & tau using pre-tokenized calibration inputs,\n",
    "    then evaluate perplexity on eval_texts.\n",
    "    Returns (avg_loss, perplexity).\n",
    "    \"\"\"\n",
    "    print(f\"  -> [Quantize] beta={beta}, tau={tau}\")\n",
    "    clear_quant_path(quant_path)\n",
    "\n",
    "    # 1) Quantize\n",
    "    quant_cfg = QuantizeConfig(bits=8, group_size=128, beta=beta, tau=tau)\n",
    "    model = GPTQModel.load(model_id, quant_cfg, trust_remote_code=True)\n",
    "    model.quantize(calib_tokenized, batch_size=batch_size)\n",
    "    os.makedirs(os.path.dirname(quant_path), exist_ok=True)\n",
    "    model.save(quant_path)\n",
    "    print(f\"     Quantization complete and saved to {quant_path}\")\n",
    "\n",
    "    clear_gpu_cache()\n",
    "    # 2) Load quantized model\n",
    "    model = GPTQModel.from_pretrained(\n",
    "        quant_path,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantize_config=quant_cfg\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # 3) Tokenize evaluation texts\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    max_len = tokenizer.model_max_length\n",
    "    encodings = tokenizer(\n",
    "        eval_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    input_ids = encodings.input_ids.to(model.device)\n",
    "    attention_mask = encodings.attention_mask.to(model.device)\n",
    "\n",
    "    \n",
    "    # 4) Compute loss\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(eval_texts), batch_size):\n",
    "            b_ids  = input_ids[i:i+batch_size]\n",
    "            b_mask = attention_mask[i:i+batch_size]\n",
    "            out    = model(input_ids=b_ids, attention_mask=b_mask, labels=b_ids)\n",
    "            losses.append(out.loss.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f\"     Eval complete: loss={avg_loss:.4f}, ppl={perplexity:.2f}\")\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    clear_quant_path(quant_path)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T04:31:03.137512Z",
     "iopub.status.busy": "2025-05-06T04:31:03.137216Z",
     "iopub.status.idle": "2025-05-06T06:31:32.157006Z",
     "shell.execute_reply": "2025-05-06T06:31:32.156420Z",
     "shell.execute_reply.started": "2025-05-06T04:31:03.137491Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Setup] Loading C4 for calibration\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b811c34028cb436bb23acd191223f68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f112f39fe647c09f26558d639b2cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1024 C4 examples for calibration.\n",
      "[Setup] Pre-tokenizing calibration set\n",
      "[Setup] Tokenized 1024 calibration examples\n",
      "[Setup] Loading Wikitext-2 for evaluation\n",
      "Using 1949 non-empty lines from Wikitext-2 for evaluation.\n",
      "[Sweep] Starting parameter grid sweep\n",
      "[Iter 1/8] Sweeping beta=0.5, tau=0.7\n",
      "  -> [Quantize] beta=0.5, tau=0.7\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ae42c214804c2ca5d8e2194cb8be9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1da586a3bdd4fbcaf0cb023127c08e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affd2461d03e48329752ae48316756e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14e949631694b70b529327d2db60a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57db75cd1dab4a7889563150934526b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)57a5a8c2d55b2a941e5d9fe5852298268ddbe61b:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d106af4c1c47fd8e9f0cd8b7d5f3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "64-8bits.tflite:   0%|          | 0.00/125M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48048508c7144de088fef1dcc56cb527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dddd78a6c946c2afb07397e566e5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "64-fp16.tflite:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a047f4fcb45474c9031a567e163e3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31061513077445d7816d25008ed9a142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder_model_merged.onnx:   0%|          | 0.00/655M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd28a053807402d9b6ec4982584f8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder_model.onnx:   0%|          | 0.00/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4500e5e78e714441a76904b06962ce95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder_with_past_model.onnx:   0%|          | 0.00/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92212b0a8666460193cfadc6cdda2d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flax_model.msgpack:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2bfe5287744a189d41d7d9b223ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421e0edc1ad3496e9db59650fcf61758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e089392facbb4e8989524e634cc24a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8f8010d75e4fe1b2171bc5da1d760f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeecc43d44349329e3913d7a392bc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a46d9c291e4dc79a83fb45404d35a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634527cf7bab46b5b8fd65f50ee9189b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)6c66a3b3a3d09c8b3b3a1f6e5e9a265d94e0270e:   0%|          | 0.00/703M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa759bb176f949bc9d5e24955d4bab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.88 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_moider_time_05_06_2025_04h_31m_25s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.877     | 12.055       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.276     | 10.410       | 50.64MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.274     | 11.878       | 50.64MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.106     | 14.240       | 50.64MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 11.739       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.522       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.197       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.598       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.350       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.301     | 10.871       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.244       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.441       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000257\u001b[0m | 1048576     | 0.01000     | 0.285     | 12.257       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.967       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.353       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.100     | 14.462       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000140\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.179       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.898       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.337       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.086     | 14.465       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000100\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.183       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.870       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.317       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.454       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000171\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.225       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.890       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.312       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.470       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000235\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.197       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.910       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.311       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.454       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000180\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.208       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.886       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.285     | 12.314       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.457       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000146\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.196       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.874       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.316       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.454       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000099\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.226       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.911       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.326       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.465       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000036\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.185       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.277     | 10.886       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.29 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.288     | 12.319       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000030\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.462       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.877', 'fwd_time': '12.055', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '10.410', 'max_vram': '50.64MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '11.878', 'max_vram': '50.64MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.106', 'fwd_time': '14.240', 'max_vram': '50.64MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '11.739', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.522', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.197', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.598', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.350', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.301', 'fwd_time': '10.871', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.244', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.441', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000257', 'samples': '1048576', 'damp': '0.01000', 'time': '0.285', 'fwd_time': '12.257', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.967', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.353', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.100', 'fwd_time': '14.462', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000140', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.179', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.898', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.337', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.086', 'fwd_time': '14.465', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000100', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.183', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.870', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.317', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.454', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000171', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.225', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.890', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.312', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.470', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000235', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.197', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.910', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.311', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.454', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000180', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.208', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.886', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.285', 'fwd_time': '12.314', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.457', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000146', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.196', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.874', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.316', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.454', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000099', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.226', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.911', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.326', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.465', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000036', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.185', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '10.886', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.288', 'fwd_time': '12.319', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000030', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.462', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                                             \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.5,\n",
      "  \"tau\": 0.7\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.5,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.7\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.5-t0.7\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.003287792205810547s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Eval complete: loss=8.9692, ppl=7857.26\n",
      "[Iter 2/8] Sweeping beta=1.0, tau=0.7\n",
      "  -> [Quantize] beta=1.0, tau=0.7\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0401086b3d4afe9bc8917fad7f7301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_fathmur_time_05_06_2025_04h_46m_28s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.492       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.276     | 11.264       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.884       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.118     | 14.872       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.165       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.882       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.318       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.15 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.146     | 14.472       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.222       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.935       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000010\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.330       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.15 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.145     | 14.440       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000360\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.197       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.900       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.318       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.103     | 14.491       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000177\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.211       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.907       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.320       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.110     | 14.478       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000148\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.212       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.919       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000011\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.322       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.105     | 14.457       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000259\u001b[0m | 1048576     | 0.01000     | 0.281     | 12.224       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.908       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.328       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.451       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000318\u001b[0m | 1048576     | 0.01000     | 0.299     | 12.186       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.886       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.326       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.464       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000274\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.184       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.853       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.306       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 1.113     | 14.437       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000237\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.223       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.278     | 10.903       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.306       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 1.117     | 14.448       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000161\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.197       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.284     | 10.904       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.337       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000009\u001b[0m | 1048576     | 0.01000     | 1.107     | 14.463       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000062\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.194       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.281     | 10.869       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.318       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=1.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000033\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.460       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.492', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '11.264', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.884', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.118', 'fwd_time': '14.872', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.165', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.882', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.318', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.146', 'fwd_time': '14.472', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.222', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.935', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000010', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.330', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.145', 'fwd_time': '14.440', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000360', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.197', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.900', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.318', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '14.491', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000177', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.211', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.907', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.320', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.110', 'fwd_time': '14.478', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000148', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.212', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.919', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000011', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.322', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.105', 'fwd_time': '14.457', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000259', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '12.224', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.908', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.328', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.451', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000318', 'samples': '1048576', 'damp': '0.01000', 'time': '0.299', 'fwd_time': '12.186', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.886', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.326', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.464', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000274', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.184', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.853', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.306', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '1.113', 'fwd_time': '14.437', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000237', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.223', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '10.903', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.306', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '1.117', 'fwd_time': '14.448', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000161', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.197', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.284', 'fwd_time': '10.904', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.337', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000009', 'samples': '1048576', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '14.463', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000062', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.194', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '10.869', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.318', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000033', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.460', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 1.0,\n",
      "  \"tau\": 0.7\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 1.0,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.7\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b1.0-t0.7\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0020835399627685547s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9783, ppl=7928.76\n",
      "[Iter 3/8] Sweeping beta=2.0, tau=0.7\n",
      "  -> [Quantize] beta=2.0, tau=0.7\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfcf8edc05c467f8416941f7fd2bd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_hrimfaxi_time_05_06_2025_05h_01m_30s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.533       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 11.263       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.853       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.862       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.200       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.896       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.326       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.446       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000014\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.230       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.277     | 10.932       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.329       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.084     | 14.469       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000620\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.217       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.276     | 10.931       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000014\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.329       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.083     | 14.453       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.32 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000261\u001b[0m | 1048576     | 0.01000     | 0.315     | 12.198       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.908       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.328       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.124     | 14.455       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000273\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.194       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.280     | 10.893       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000024\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.319       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.079     | 14.473       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000495\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.191       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.902       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000019\u001b[0m | 1048576     | 0.01000     | 0.285     | 12.321       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.13 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.129     | 14.479       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000527\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.201       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.918       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000017\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.322       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 1.100     | 14.454       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000523\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.181       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.281     | 10.894       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.318       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000009\u001b[0m | 1048576     | 0.01000     | 1.100     | 14.445       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000487\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.185       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.876       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.314       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000011\u001b[0m | 1048576     | 0.01000     | 1.114     | 14.461       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000329\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.175       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.281     | 10.862       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.303       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000009\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.449       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000135\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.194       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.31 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.313     | 10.907       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000014\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.299       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=2.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000042\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.432       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.533', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '11.263', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.853', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.862', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.200', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.896', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.326', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.446', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000014', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.230', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '10.932', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.329', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.084', 'fwd_time': '14.469', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000620', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.217', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '10.931', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000014', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.329', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '14.453', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000261', 'samples': '1048576', 'damp': '0.01000', 'time': '0.315', 'fwd_time': '12.198', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.908', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.328', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.124', 'fwd_time': '14.455', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000273', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.194', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '10.893', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000024', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.319', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.079', 'fwd_time': '14.473', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000495', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.191', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.902', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000019', 'samples': '1048576', 'damp': '0.01000', 'time': '0.285', 'fwd_time': '12.321', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.129', 'fwd_time': '14.479', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000527', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.201', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.918', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000017', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.322', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '1.100', 'fwd_time': '14.454', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000523', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.181', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '10.894', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.318', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000009', 'samples': '1048576', 'damp': '0.01000', 'time': '1.100', 'fwd_time': '14.445', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000487', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.185', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.876', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.314', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000011', 'samples': '1048576', 'damp': '0.01000', 'time': '1.114', 'fwd_time': '14.461', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000329', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.175', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '10.862', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.303', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000009', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.449', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000135', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.194', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.313', 'fwd_time': '10.907', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000014', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.299', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000042', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.432', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 2.0,\n",
      "  \"tau\": 0.7\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 2.0,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.7\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b2.0-t0.7\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002353191375732422s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9804, ppl=7945.68\n",
      "[Iter 4/8] Sweeping beta=4.0, tau=0.7\n",
      "  -> [Quantize] beta=4.0, tau=0.7\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7fba631c8b40989591c031fbef9873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_infancies_time_05_06_2025_05h_16m_30s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.569       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 11.269       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.861       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.104     | 14.879       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.222       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.296     | 10.900       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.29 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.294     | 12.333       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.445       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000036\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.231       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.935       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000034\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.335       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 1.110     | 14.435       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000001345\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.218       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.923       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000038\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.336       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.458       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000478\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.202       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000010\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.899       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000043\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.325       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.13 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 1.132     | 14.449       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000632\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.204       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.276     | 10.908       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000067\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.330       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000009\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.456       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000001180\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.207       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.905       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000052\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.330       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000011\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.448       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000001097\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.196       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.890       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000048\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.322       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.469       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000001253\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.188       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000011\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.894       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000045\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.330       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000022\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.453       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000001245\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.200       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.278     | 10.890       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000045\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.314       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000023\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.454       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000844\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.213       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.31 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000010\u001b[0m | 1048576     | 0.01000     | 0.309     | 10.905       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000044\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.306       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000010\u001b[0m | 1048576     | 0.01000     | 1.104     | 14.447       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000366\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.186       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000017\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.910       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000036\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.312       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=4.0 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000060\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.444       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.569', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '11.269', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.861', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '14.879', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.222', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.296', 'fwd_time': '10.900', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.294', 'fwd_time': '12.333', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.445', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000036', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.231', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.935', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000034', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.335', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '1.110', 'fwd_time': '14.435', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000001345', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.218', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.923', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000038', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.336', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.458', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000478', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.202', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000010', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.899', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000043', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.325', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '1.132', 'fwd_time': '14.449', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000632', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.204', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '10.908', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000067', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.330', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000009', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.456', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000001180', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.207', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.905', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000052', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.330', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000011', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.448', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000001097', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.196', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.890', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000048', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.322', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.469', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000001253', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.188', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000011', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.894', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000045', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.330', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000022', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.453', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000001245', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.200', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '10.890', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000045', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.314', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000023', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.454', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000844', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.213', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000010', 'samples': '1048576', 'damp': '0.01000', 'time': '0.309', 'fwd_time': '10.905', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000044', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.306', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000010', 'samples': '1048576', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '14.447', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000366', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.186', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000017', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.910', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000036', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.312', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000060', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.444', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 4.0,\n",
      "  \"tau\": 0.7\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 4.0,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.7\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b4.0-t0.7\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0022292137145996094s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9733, ppl=7889.40\n",
      "[Sweep] Best beta selected: 0.5\n",
      "[Iter 5/8] Sweeping beta=0.5, tau=0.3\n",
      "  -> [Quantize] beta=0.5, tau=0.3\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f47c0617934170ad7b1a141ff3f500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_metates_time_05_06_2025_05h_31m_31s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.510       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 11.255       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.791       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.879       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.195       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.903       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.330       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.487       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.300     | 12.213       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.916       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.343       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.101     | 14.460       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000203\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.244       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.29 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.286     | 10.917       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.335       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.441       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000119\u001b[0m | 1048576     | 0.01000     | 0.282     | 12.267       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.895       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.325       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.086     | 14.462       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000078\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.215       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.927       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.30 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.297     | 12.324       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.460       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000129\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.230       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.940       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.324       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.124     | 14.438       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000187\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.205       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.924       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.29 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.290     | 12.330       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.106     | 14.469       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000133\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.204       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.885       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.331       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.101     | 14.444       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000105\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.190       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.862       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.306       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.119     | 14.458       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000070\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.214       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.908       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.314       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.087     | 14.446       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000025\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.202       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.907       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.334       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 1.120     | 14.458       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.510', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '11.255', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.791', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.879', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.195', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.903', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.330', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.487', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.300', 'fwd_time': '12.213', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.916', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.343', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.101', 'fwd_time': '14.460', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000203', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.244', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.286', 'fwd_time': '10.917', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.335', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.441', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000119', 'samples': '1048576', 'damp': '0.01000', 'time': '0.282', 'fwd_time': '12.267', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.895', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.325', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.086', 'fwd_time': '14.462', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000078', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.215', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.927', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.297', 'fwd_time': '12.324', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.460', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000129', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.230', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.940', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.324', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.124', 'fwd_time': '14.438', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000187', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.205', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.924', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.290', 'fwd_time': '12.330', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.106', 'fwd_time': '14.469', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000133', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.204', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.885', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.331', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.101', 'fwd_time': '14.444', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000105', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.190', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.862', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.306', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.119', 'fwd_time': '14.458', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000070', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.214', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.908', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.314', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '14.446', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000025', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.202', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.907', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.334', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '1.120', 'fwd_time': '14.458', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.5,\n",
      "  \"tau\": 0.3\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.5,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.3\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.5-t0.3\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0020813941955566406s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9831, ppl=7967.42\n",
      "[Iter 6/8] Sweeping beta=0.5, tau=0.5\n",
      "  -> [Quantize] beta=0.5, tau=0.5\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ade26a03e9438986e00d2577275ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_pycnia_time_05_06_2025_05h_46m_32s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.554       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 11.262       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.875       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.856       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.190       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.277     | 10.892       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.337       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.124     | 14.471       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.211       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.901       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.331       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.17 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.167     | 14.470       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000228\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.223       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.927       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.345       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.454       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000129\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.199       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.885       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.314       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.086     | 14.454       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000089\u001b[0m | 1048576     | 0.01000     | 0.266     | 12.251       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.948       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.314       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.084     | 14.461       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000150\u001b[0m | 1048576     | 0.01000     | 0.296     | 12.203       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.924       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.281     | 12.325       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.123     | 14.464       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000207\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.191       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.902       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.338       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.462       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.32 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000156\u001b[0m | 1048576     | 0.01000     | 0.321     | 12.186       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.277     | 10.893       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.313       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.443       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000127\u001b[0m | 1048576     | 0.01000     | 0.285     | 12.183       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.892       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.303       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.108     | 14.445       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000085\u001b[0m | 1048576     | 0.01000     | 0.283     | 12.189       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.872       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.316       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.083     | 14.460       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000032\u001b[0m | 1048576     | 0.01000     | 0.296     | 12.165       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.857       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.302       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000029\u001b[0m | 1048576     | 0.01000     | 1.120     | 14.440       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.554', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '11.262', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.875', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.856', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.190', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '10.892', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.337', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.124', 'fwd_time': '14.471', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.211', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.901', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.331', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.167', 'fwd_time': '14.470', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000228', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.223', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.927', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.345', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.454', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000129', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.199', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.885', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.314', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.086', 'fwd_time': '14.454', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000089', 'samples': '1048576', 'damp': '0.01000', 'time': '0.266', 'fwd_time': '12.251', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.948', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.314', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.084', 'fwd_time': '14.461', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000150', 'samples': '1048576', 'damp': '0.01000', 'time': '0.296', 'fwd_time': '12.203', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.924', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '12.325', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.123', 'fwd_time': '14.464', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000207', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.191', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.902', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.338', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.462', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000156', 'samples': '1048576', 'damp': '0.01000', 'time': '0.321', 'fwd_time': '12.186', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '10.893', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.313', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.443', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000127', 'samples': '1048576', 'damp': '0.01000', 'time': '0.285', 'fwd_time': '12.183', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.892', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.303', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.108', 'fwd_time': '14.445', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000085', 'samples': '1048576', 'damp': '0.01000', 'time': '0.283', 'fwd_time': '12.189', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.872', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.316', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '14.460', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000032', 'samples': '1048576', 'damp': '0.01000', 'time': '0.296', 'fwd_time': '12.165', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.857', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.302', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000029', 'samples': '1048576', 'damp': '0.01000', 'time': '1.120', 'fwd_time': '14.440', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.5,\n",
      "  \"tau\": 0.5\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.5,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.5\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.5-t0.5\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002365589141845703s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9665, ppl=7836.36\n",
      "[Iter 7/8] Sweeping beta=0.5, tau=0.7\n",
      "  -> [Quantize] beta=0.5, tau=0.7\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d3b3eb167a4d92b8b5b8f89046ab97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_beeherd_time_05_06_2025_06h_01m_32s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.558       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 11.263       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.873       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.882       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.212       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.910       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.353       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.125     | 14.433       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.201       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.890       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.331       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.440       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000257\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.233       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.921       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.324       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.087     | 14.450       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000140\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.217       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.947       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.357       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.450       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000100\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.201       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.899       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.267     | 12.338       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.096     | 14.457       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000171\u001b[0m | 1048576     | 0.01000     | 0.300     | 12.216       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.302     | 10.913       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.332       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.117     | 14.468       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000235\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.200       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.885       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.317       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.082     | 14.470       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000180\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.225       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.900       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.315       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.15 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.151     | 14.452       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000146\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.191       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.899       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.315       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 1.123     | 14.459       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000099\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.203       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.908       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.282     | 12.320       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.458       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000036\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.200       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.917       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.306       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000030\u001b[0m | 1048576     | 0.01000     | 1.118     | 14.456       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.558', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '11.263', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.873', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.882', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.212', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.910', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.353', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.125', 'fwd_time': '14.433', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.201', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.890', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.331', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.440', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000257', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.233', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.921', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.324', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '14.450', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000140', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.217', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.947', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.357', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.450', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000100', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.201', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.899', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '12.338', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.096', 'fwd_time': '14.457', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000171', 'samples': '1048576', 'damp': '0.01000', 'time': '0.300', 'fwd_time': '12.216', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.302', 'fwd_time': '10.913', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.332', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.117', 'fwd_time': '14.468', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000235', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.200', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.885', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.317', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.082', 'fwd_time': '14.470', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000180', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.225', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.900', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.315', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.151', 'fwd_time': '14.452', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000146', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.191', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.899', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.315', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '1.123', 'fwd_time': '14.459', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000099', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.203', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.908', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.282', 'fwd_time': '12.320', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.458', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000036', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.200', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.917', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.306', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000030', 'samples': '1048576', 'damp': '0.01000', 'time': '1.118', 'fwd_time': '14.456', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.5,\n",
      "  \"tau\": 0.7\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.5,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.7\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.5-t0.7\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0022284984588623047s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9692, ppl=7857.26\n",
      "[Iter 8/8] Sweeping beta=0.5, tau=1.0\n",
      "  -> [Quantize] beta=0.5, tau=1.0\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aa269520c8422996ea640544a145b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_outread_time_05_06_2025_06h_16m_32s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.508       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 11.241       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.896       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.849       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.194       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.898       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.319       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.480       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.207       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.931       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.322       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.114     | 14.453       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000293\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.221       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.938       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.348       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.081     | 14.482       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000157\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.191       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.887       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.281     | 12.325       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.490       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000115\u001b[0m | 1048576     | 0.01000     | 0.267     | 12.213       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.923       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.310       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.442       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000199\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.217       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.925       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.322       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.435       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000289\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.199       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.906       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.328       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.476       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000213\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.216       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.899       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.327       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.454       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000163\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.203       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.892       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.333       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.472       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000112\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.229       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.912       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.312       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000009\u001b[0m | 1048576     | 0.01000     | 1.098     | 14.433       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000038\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.212       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.912       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.322       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.5 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000031\u001b[0m | 1048576     | 0.01000     | 1.084     | 14.446       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.508', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '11.241', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.896', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.849', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.194', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.898', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.319', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.480', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.207', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.931', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.322', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.114', 'fwd_time': '14.453', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000293', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.221', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.938', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.348', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.081', 'fwd_time': '14.482', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000157', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.191', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.887', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '12.325', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.490', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000115', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '12.213', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.923', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.310', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.442', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000199', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.217', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.925', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.322', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.435', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000289', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.199', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.906', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.328', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.476', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000213', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.216', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.899', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.327', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.454', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000163', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.203', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.892', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.333', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.472', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000112', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.229', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.912', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.312', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000009', 'samples': '1048576', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '14.433', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000038', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.212', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.912', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.322', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000031', 'samples': '1048576', 'damp': '0.01000', 'time': '1.084', 'fwd_time': '14.446', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.5,\n",
      "  \"tau\": 1.0\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.5,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 1.0\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.5-t1.0\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002598285675048828s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9719, ppl=7878.93\n",
      "[Results] Summary of grid sweep:\n",
      "|   beta |   tau |    loss |     ppl |\n",
      "|-------:|------:|--------:|--------:|\n",
      "|    0.5 |   0.7 | 8.96919 | 7857.26 |\n",
      "|    1   |   0.7 | 8.97825 | 7928.76 |\n",
      "|    2   |   0.7 | 8.98038 | 7945.68 |\n",
      "|    4   |   0.7 | 8.97328 | 7889.4  |\n",
      "|    0.5 |   0.3 | 8.98312 | 7967.42 |\n",
      "|    0.5 |   0.5 | 8.96653 | 7836.36 |\n",
      "|    0.5 |   0.7 | 8.96919 | 7857.26 |\n",
      "|    0.5 |   1   | 8.97195 | 7878.93 |\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare calibration dataset from C4 streaming ---\n",
    "print(\"[Setup] Loading C4 for calibration\")\n",
    "c4_stream = load_dataset(\n",
    "    \"allenai/c4\", \n",
    "    \"en\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ")\n",
    "num_calibration_samples = 1024\n",
    "calibration_dataset_c4 = []\n",
    "for sample in islice(c4_stream, num_calibration_samples):\n",
    "    text = sample.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        continue\n",
    "    calibration_dataset_c4.append({\"content\": text})\n",
    "calibration_texts = [ex[\"content\"] for ex in calibration_dataset_c4]\n",
    "print(f\"Loaded {len(calibration_dataset_c4)} C4 examples for calibration.\")\n",
    "\n",
    "# Pre-tokenize calibration set once\n",
    "print(\"[Setup] Pre-tokenizing calibration set\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_len = tokenizer.model_max_length\n",
    "calib_tokenized = []\n",
    "for txt in calibration_texts:\n",
    "    enc = tokenizer(txt, truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    calib_tokenized.append({\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"]\n",
    "    })\n",
    "print(f\"[Setup] Tokenized {len(calib_tokenized)} calibration examples\")\n",
    "\n",
    "# --- Prepare evaluation texts from Wikitext-2 slice ---\n",
    "print(\"[Setup] Loading Wikitext-2 for evaluation\")\n",
    "wt2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation[:3000]\")\n",
    "eval_texts = [ex[\"text\"].strip() for ex in wt2 if ex[\"text\"].strip()]\n",
    "print(f\"Using {len(eval_texts)} non-empty lines from Wikitext-2 for evaluation.\")\n",
    "\n",
    "# Sweep settings\n",
    "model_id        = \"gpt2\"\n",
    "base_quant_path = \"/kaggle/working/gpt2-quant\"\n",
    "tau_fixed       = 0.7\n",
    "beta_values     = [0.5, 1.0, 2.0, 4.0]\n",
    "\n",
    "results = []\n",
    "print(\"[Sweep] Starting parameter grid sweep\")\n",
    "total_iters = len(beta_values) + len([0.3, 0.5, 0.7, 1.0])\n",
    "iter_count = 0\n",
    "\n",
    "# Beta sweep\n",
    "for beta in beta_values:\n",
    "    iter_count += 1\n",
    "    print(f\"[Iter {iter_count}/{total_iters}] Sweeping beta={beta}, tau={tau_fixed}\")\n",
    "    quant_path = f\"{base_quant_path}-b{beta}-t{tau_fixed}\"\n",
    "    loss, ppl = quantize_and_eval(model_id, calib_tokenized, eval_texts, beta, tau_fixed, quant_path)\n",
    "    results.append({\"beta\": beta, \"tau\": tau_fixed, \"loss\": loss, \"ppl\": ppl})\n",
    "\n",
    "# Select best beta\n",
    "best_beta = min([r for r in results if r[\"tau\"] == tau_fixed], key=lambda x: x[\"ppl\"])[\"beta\"]\n",
    "print(f\"[Sweep] Best beta selected: {best_beta}\")\n",
    "\n",
    "# Tau values sweep\n",
    "tau_values = [0.3, 0.5, 0.7, 1.0]\n",
    "for tau in tau_values:\n",
    "    iter_count += 1\n",
    "    print(f\"[Iter {iter_count}/{total_iters}] Sweeping beta={best_beta}, tau={tau}\")\n",
    "    quant_path = f\"{base_quant_path}-b{best_beta}-t{tau}\"\n",
    "    loss, ppl = quantize_and_eval(model_id, calib_tokenized, eval_texts, best_beta, tau, quant_path)\n",
    "    results.append({\"beta\": best_beta, \"tau\": tau, \"loss\": loss, \"ppl\": ppl})\n",
    "\n",
    "# Summary\n",
    "print(\"[Results] Summary of grid sweep:\")\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T06:41:11.178349Z",
     "iopub.status.busy": "2025-05-06T06:41:11.177669Z",
     "iopub.status.idle": "2025-05-06T06:41:11.759000Z",
     "shell.execute_reply": "2025-05-06T06:41:11.758348Z",
     "shell.execute_reply.started": "2025-05-06T06:41:11.178324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQOUlEQVR4nO3de3QU9f3/8dfs5h6SJQESEhIuiUJoVChyEy9AvaAFJGoVWrSIFWlFhWoVUKOCoAU1h2+tRYFUuago1lttfwpS0dKo4RLBWE0sF7kGCORCSEg2u/P7AzNhSQIhJqwZno9zcg555zMzn/fM7vLKzOzGME3TFAAAAFo9h78nAAAAgOZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAOAVig7O1tBQUH67rvv/D2VH6Vp06ZpwIAB/p4GcMYR7ICz2EsvvSTDMLR+/Xp/T8Wvbr31VhmGYX0FBAQoMTFRY8aM0X//+98mrbO8vFyPPfaY1qxZ07yT/d5DDz2kX/7yl+rSpYtV+8tf/qKXXnqpRbbXnL7++mtdffXVatOmjaKjo3XLLbfowIEDp1xuzZo1PsfpxK/Zs2dbY6dMmaJNmzbp3XffbclWgB+dAH9PAAB+DIKDg7Vo0SJJUnV1tbZs2aLnn39e77//vv773/8qPj7+tNZXXl6uGTNmSJKGDBnSrHP94osv9OGHHyorK8un/pe//EXt27fXrbfe2qzba067du3SZZddJpfLpSeeeEJlZWV6+umn9eWXX1pnIRvSs2dPLV26tE596dKlWrlypa666iqr1rFjR40aNUpPP/20rr322hbpBfgxItgBgKSAgADdfPPNPrWBAwdqxIgR+sc//qEJEyb4aWZ1vfjii+rcubMGDhzo76mctieeeEJHjhzRhg0b1LlzZ0lS//79deWVV+qll17SHXfc0eCysbGxdY6RJM2YMUPnnnuu+vXr51O/6aabdOONN2rr1q1KSkpq3kaAHykuxQI4pZycHF1zzTWKjIxUmzZtdPnll+uzzz7zGeN2u63/YENCQtSuXTtdcsklWrVqlTWmoKBA48ePV0JCgoKDgxUXF6dRo0Zp+/btDW776aeflmEY9d5LNn36dAUFBamoqEiS9O233+qGG25Qx44dFRISooSEBI0ZM0YlJSVN6rtjx46SjoW+4xUXF2vKlClKTExUcHCwzjnnHM2ZM0der1eStH37dnXo0EHSsdBRc6nwsccekyRt3rxZt956q5KSkhQSEqKOHTvqtttu08GDBxs1r7fffls/+9nPZBiGVevatau++uorffzxx9b2as4UHjp0SH/4wx90/vnnq02bNoqMjNQ111yjTZs2+ay35tL8icej5hJoc1xW/tvf/qYRI0ZYoU6SrrjiCnXv3l2vv/76aa8vOztb//vf/zR27Ng6P7viiiskSe+8807TJwy0MpyxA3BSX331lS699FJFRkbqgQceUGBgoF544QUNGTJEH3/8sXWD+mOPPaYnn3xSt99+u/r376/S0lKtX79eGzdu1JVXXilJuuGGG/TVV1/p7rvvVteuXbV//36tWrVKO3bsUNeuXevd/k033aQHHnhAr7/+uu6//36fn73++uu66qqrFBUVpaqqKg0bNkyVlZW6++671bFjR+3evVvvvfeeiouL5XK5TtlrYWGhJMnj8Wjr1q2aOnWq2rVrpxEjRlhjysvLNXjwYO3evVsTJ05U586dlZWVpenTp2vv3r2aN2+eOnTooPnz5+t3v/udrrvuOl1//fWSpAsuuECStGrVKm3dulXjx49Xx44d9dVXX2nBggX66quv9Nlnn/kEthPt3r1bO3bsUJ8+fXzq8+bN09133602bdrooYceknTsDJckbd26VW+//bZuvPFGdevWTfv27dMLL7ygwYMHN+kyc81+KC8vP+U4p9OpqKgoa+779+9X375964zr37+//vnPf572PF5++WVJqjfYuVwuJScn6z//+Y9+//vfn/a6gVbJBHDWevHFF01J5rp16xock5aWZgYFBZlbtmyxanv27DEjIiLMyy67zKr16tXLHD58eIPrKSoqMiWZTz311GnP86KLLjIvvPBCn1p2drYpyVyyZIlpmqaZk5NjSjJXrFhx2usfN26cKanOV6dOncwNGzb4jH388cfN8PBwMz8/36c+bdo00+l0mjt27DBN0zQPHDhgSjIfffTROtsrLy+vU3v11VdNSeYnn3xy0rl++OGHpiTz73//e52fpaammoMHD65TP3r0qOnxeHxq27ZtM4ODg82ZM2datZrHw7Zt23zGfvTRR6Yk86OPPrJqjz76aL377MSvLl26WMusW7fO55gd7/777zclmUePHj1p/8errq42Y2Njzf79+zc45qqrrjJ79uzZ6HUCrR1n7AA0yOPxaOXKlUpLS/O5RykuLk6/+tWvtHDhQpWWlioyMlJt27bVV199pW+//VbnnntunXWFhoYqKChIa9as0W9+8xvrLE5jjB49WlOmTNGWLVuUnJwsSXrttdcUHBysUaNGSZJ1Ru6DDz7Qz3/+c4WFhZ1WryEhIfr73/8uSfJ6vdq+fbsyMjL085//XJ988om6d+8uSVqxYoUuvfRSRUVFWWf4pGOX/f74xz/qk08+qffs0fFCQ0Otfx89elRlZWXW/XIbN27UpZde2uCyNZdrT2f/BQcHW//2eDwqLi5WmzZt1KNHD23cuLHR6zner3/9a11yySWnHHd8rxUVFXXmUyMkJMQaU9/P67N69Wrt27dPDz74YINjoqKilJOT06j1AXZAsAPQoAMHDqi8vFw9evSo87OePXvK6/Vq586dSk1N1cyZMzVq1Ch1795d5513nq6++mrdcsst1uXH4OBgzZkzR/fdd59iY2OtNyb8+te/tu5la8iNN96oe++9V6+99poefPBBmaapFStWWPf9SVK3bt107733KiMjQy+//LIuvfRSXXvttbr55psbdRnW6XRa92TV+PnPf65zzz1X06dP19/+9jdJx+7j27x5s3UP3Yn2799/ym0dOnRIM2bM0PLly+uMb+z9gKZpNmqcdCyo/t///Z/+8pe/aNu2bfJ4PNbP2rVr1+j1HC8pKem035BQE/IqKyvr/Ozo0aM+Yxrj5ZdfltPp1OjRoxscY5rmSS9tA3bDmycANIvLLrtMW7Zs0V//+ledd955WrRokfr06WN9hIh07LPF8vPz9eSTTyokJETp6enq2bPnKc+oxMfH69JLL7Vurv/ss8+0Y8eOOv+hP/PMM9q8ebMefPBBVVRU6J577lFqaqp27drVpJ4SEhLUo0cPffLJJ1bN6/Xqyiuv1KpVq+r9uuGGG0653ptuukkLFy7Ub3/7W7355ptauXKl3n//fWv9J1MTxGreMNIYTzzxhO69915ddtllWrZsmT744AOtWrVKqampPttrKAAdHwRrlJWVqaCg4JRfx38+XVxcnCRp7969dda3d+9eRUdHN/psXUVFhd566y1dccUV1r2E9SkqKlL79u0btU7ADjhjB6BBHTp0UFhYmPLy8ur87JtvvpHD4VBiYqJVi46O1vjx4zV+/HiVlZXpsssu02OPPabbb7/dGpOcnKz77rtP9913n7799lv17t1bzzzzjJYtW3bSuYwePVp33nmn8vLy9NprryksLEwjR46sM+7888/X+eefr4cfflhZWVm6+OKL9fzzz2vWrFlN2gfV1dUqKyvzmX9ZWVmds3snaigkFRUVafXq1ZoxY4YeeeQRq/7tt982aj4pKSmSpG3btjV6m2+88YaGDh2qzMxMn3pxcbFP6Km5vFtcXOwzrr53JD/99NPW5/SdTJcuXax32Xbq1EkdOnSo9wOxs7Oz1bt371Our8a7776rw4cPn/Ky97Zt29SrV69Grxdo7Qh2ABrkdDp11VVX6Z133tH27dutd67u27dPr7zyii655BLrUujBgwd9Luu1adNG55xzjnbu3Cnp2LsoHQ6HdS+VdCwkRURE1Htp7kQ33HCD7r77br366qtasWKFRowYofDwcOvnpaWlCgsL8/lokvPPP18Oh6NR669Pfn6+8vLydOGFF1q1m266SY899pg++OADDRs2zGd8zb1rAQEB1j1+J4Ykp9Mpqe6l1Hnz5jVqTp06dVJiYmK94Sg8PLzO9mq2eeL2VqxYod27d+ucc86xajX3L37yySdWyPJ4PFqwYEGddTblHjvp2HFcvHixdu7caf1SsHr1auXn5/u8c9XtdmvLli1yuVzWmb7jvfLKKwoLC9N1113X4LZLSkq0ZcsW/e53vzvlPAG7INgB0F//+lfrUuDxJk+erFmzZmnVqlW65JJLdOeddyogIEAvvPCCKisrNXfuXGvsT37yEw0ZMkQXXnihoqOjtX79er3xxhu66667JB0LSZdffrluuukm/eQnP1FAQIDeeust7du3T2PGjDnlHGNiYjR06FBlZGTo8OHDdS7D/utf/9Jdd92lG2+8Ud27d1d1dbWWLl0qp9PZqMuj1dXV1lnDmjdPPP/88/J6vXr00Uetcffff7/effddjRgxQrfeeqsuvPBCHTlyRF9++aXeeOMNbd++Xe3bt1doaKh+8pOf6LXXXlP37t0VHR2t8847T+edd54uu+wyzZ07V263W506ddLKlSvrPQPXkFGjRumtt96qc//YhRdeqPnz52vWrFk655xzFBMTo5/97GcaMWKEZs6cqfHjx2vQoEH68ssv9fLLL9e5Ry41NVUDBw7U9OnTdejQIUVHR2v58uWqrq6uM4em3GMnSQ8++KBWrFihoUOHavLkySorK9NTTz2l888/X+PHj7fG7d69Wz179tS4cePq/Jm0Q4cO6f/9v/+nG264QW3atGlwWx9++KFM07TeYAOcFfz4jlwAflbz8RYNfe3cudM0TdPcuHGjOWzYMLNNmzZmWFiYOXToUDMrK8tnXbNmzTL79+9vtm3b1gwNDTVTUlLM2bNnm1VVVaZpmmZhYaE5adIkMyUlxQwPDzddLpc5YMAA8/XXX2/0fBcuXGhKMiMiIsyKigqfn23dutW87bbbzOTkZDMkJMSMjo42hw4dan744YenXG99H3cSGRlpXn755fUuf/jwYXP69OnmOeecYwYFBZnt27c3Bw0aZD799NNWv6ZpmllZWeaFF15oBgUF+Xz0ya5du8zrrrvObNu2relyucwbb7zR3LNnT4Mfj3KijRs3mpLMf//73z71goICc/jw4WZERIQpyfrok6NHj5r33XefGRcXZ4aGhpoXX3yx+emnn5qDBw+u8/EoW7ZsMa+44gozODjYjI2NNR988EFz1apVdT7u5IfIzc01r7rqKjMsLMxs27atOXbsWLOgoMBnzLZt20xJ5rhx4+os//zzz5uSzHffffek2xk9erR5ySWXNMucgdbCMM3TeGsVAOBH4fLLL1d8fHy9fzsVx/7KSbdu3bR8+XLO2OGsQrADgFbo888/16WXXqpvv/1WXbp08fd0fnSmTZumf/3rX8rOzvb3VIAzimAHAABgE3yOHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACb4AOKm8jr9WrPnj2KiIjgD0wDAIAWY5qmDh8+rPj4eDkcJz8nR7Broj179vj8jUwAAICWtHPnTiUkJJx0DMGuiSIiIiQd28k1fysTAACguZWWlioxMdHKHidDsGuimsuvkZGRBDsAANDiGnPrF2+eAAAAsAmCHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACbINgBAADYBMEOAADAJgh2AAAANkGwAwAAsAmCHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACbINgBAADYBMEOAADAJgh2AAAANkGwAwAAsAmCHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACbINgBAADYBMEOAADAJgh2AAAANkGwAwAAsAmCHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACbCPD3BADgeBsKs/w9hWZxYftB/p4CgLMQZ+wAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADbh12Dn8XiUnp6ubt26KTQ0VMnJyXr88cdlmuZJl3vuuefUs2dPhYaGqkePHlqyZEmdMfPmzVOPHj0UGhqqxMRE/f73v9fRo0frrKdr164KCQnRgAEDlJ2d3az9AQAAnEl+/Ry7OXPmaP78+Vq8eLFSU1O1fv16jR8/Xi6XS/fcc0+9y8yfP1/Tp0/XwoUL1a9fP2VnZ2vChAmKiorSyJEjJUmvvPKKpk2bpr/+9a8aNGiQ8vPzdeutt8owDGVkZEiSXnvtNd177716/vnnNWDAAM2bN0/Dhg1TXl6eYmJiztg+AAAAaC6GearTYy1oxIgRio2NVWZmplW74YYbFBoaqmXLltW7zKBBg3TxxRfrqaeesmr33XefPv/8c61du1aSdNddd+nrr7/W6tWrGxwzYMAA9evXT3/+858lSV6vV4mJibr77rs1bdq0U869tLRULpdLJSUlioyMPP3mAdSLDygGAF+nkzn8esZu0KBBWrBggfLz89W9e3dt2rRJa9eutc6q1aeyslIhISE+tdDQUGVnZ8vtdiswMFCDBg3SsmXLlJ2drf79+2vr1q365z//qVtuuUWSVFVVpQ0bNmj69OnWOhwOh6644gp9+umnDW63srLS+r60tFSSVF1drerqamsdDodDXq9XXq/XZ90Oh0Mej8fnMnNDdafTKcMwrPUeX5eOXcJuTD0gIECmafrUDcOQ0+msM8eG6vRET2e6J6/H/L4uGQ5DptfU8b9+Nlh3HFtXQ/Wa9R5flyTTq0bVHU5Dpmn61K251FOXZOvjRE/0RE9nrqfj/30qfg1206ZNU2lpqVJSUuR0OuXxeDR79myNHTu2wWWGDRumRYsWKS0tTX369NGGDRu0aNEiud1uFRYWKi4uTr/61a9UWFioSy65RKZpqrq6Wr/97W/14IMPSpIKCwvl8XgUGxvrs+7Y2Fh988039W73ySef1IwZM+rUc3JyFB4eLknq0KGDkpOTtW3bNh04cMAak5CQoISEBOXn56ukpMSqJyUlKSYmRrm5uaqoqLDqKSkpatu2rXJycnweIBdccIGCgoK0fv16nzn07dtXVVVV2rx5s1VzOp3q16+fSkpKfHoKDQ1Vr169VFhYqK1bt1p1l8ulnj17as+ePdq1a5dVpyd6OtM9lVQd+6UpuG2gIuLDVFZQocpitzU+rEOwwjqEqHRnudxHal+I28SFKiQqSMXbyuSprH0RjOwcpqA2gSr69rBMb+2LedvkNnIEOHQor9Snp+gekfJWe1W8pcyqGQ5D7VIi5T5SrdId5bW9BjsUlRyhymK3yvbW7sfA8AApRrY+TvRET/R05npyuVxqLL9eil2+fLnuv/9+PfXUU0pNTdUXX3yhKVOmKCMjQ+PGjat3mYqKCk2aNElLly6VaZqKjY3VzTffrLlz56qgoECxsbFas2aNxowZo1mzZmnAgAH63//+p8mTJ2vChAlKT0/Xnj171KlTJ2VlZemiiy6y1v3AAw/o448/1ueff15nu/WdsUtMTNTBgwet06Jn028P9ERPLdVTzsHPv6+37jN2fWMutvVxoid6oqcz11NZWZmioqIadSnWr8EuMTFR06ZN06RJk6zarFmztGzZsgbPnNVwu93at2+f4uLitGDBAk2dOlXFxcVyOBy69NJLNXDgQJ/78JYtW6Y77rhDZWVlqq6uVlhYmN544w2lpaVZY8aNG6fi4mK98847p5w799gBLYN77ADA1+lkDr9+3El5ebkcDt8p1CTYUwkMDFRCQoKcTqeWL1+uESNGWOtqaL2SZJqmgoKCdOGFF/q8ucLr9Wr16tU+Z/AAAABaE7/eYzdy5EjNnj1bnTt3VmpqqnJycpSRkaHbbrvNGjN9+nTt3r3b+qy6/Px8ZWdna8CAASoqKlJGRoZyc3O1ePFin/VmZGTopz/9qXUpNj09XSNHjrQC3r333qtx48apb9++6t+/v+bNm6cjR45o/PjxZ3YnAAAANBO/Brtnn31W6enpuvPOO7V//37Fx8dr4sSJeuSRR6wxe/fu1Y4dO6zvPR6PnnnmGeXl5SkwMFBDhw5VVlaWunbtao15+OGHZRiGHn74Ye3evVsdOnSwQmSN0aNH68CBA3rkkUdUUFCg3r176/3336/zhgoAAIDWwq/32LVm3GMHtAzusQMAX63mHjsAAAA0H4IdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbCLA3xMAGvLexl3+nkKzGNEnwd9TAACcJThjBwAAYBMEOwAAAJvwa7DzeDxKT09Xt27dFBoaquTkZD3++OMyTfOkyz333HPq2bOnQkND1aNHDy1ZssTn50OGDJFhGHW+hg8fbo0pKyvTXXfdpYSEBIWGhuonP/mJnn/++RbpEwAA4Ezw6z12c+bM0fz587V48WKlpqZq/fr1Gj9+vFwul+655556l5k/f76mT5+uhQsXql+/fsrOztaECRMUFRWlkSNHSpLefPNNVVVVWcscPHhQvXr10o033mjV7r33Xv3rX//SsmXL1LVrV61cuVJ33nmn4uPjde2117Zs4wAAAC3Ar8EuKytLo0aNss6kde3aVa+++qqys7MbXGbp0qWaOHGiRo8eLUlKSkrSunXrNGfOHCvYRUdH+yyzfPlyhYWF+QS7rKwsjRs3TkOGDJEk3XHHHXrhhReUnZ1NsAMAAK2SXy/FDho0SKtXr1Z+fr4kadOmTVq7dq2uueaaBpeprKxUSEiITy00NFTZ2dlyu931LpOZmakxY8YoPDzcZ9vvvvuudu/eLdM09dFHHyk/P19XXXVVM3QGAABw5vn1jN20adNUWlqqlJQUOZ1OeTwezZ49W2PHjm1wmWHDhmnRokVKS0tTnz59tGHDBi1atEhut1uFhYWKi4vzGZ+dna3c3FxlZmb61J999lndcccdSkhIUEBAgBwOhxYuXKjLLrus3u1WVlaqsrLS+r60tFSSVF1drerqakmSw+GQw+GQ1+uV1+u1xtbUPR6Pz/2DDdWdTqcMw7DWe3xdOnZvYmPqAQEBMk3Tp24YhpxOZ505NlT3Z0+maUon3G9pOBx164ZkGE2pe6XjV//9vZinXT9uv9TUJVnbrOnZrsepuXvyekxrNxoOQ6bX9D18DdUd+v541F+vWe/xdUkyTzx8DdQdTkOmafrUrbnUU5dk6+NET/RET2eup+P/fSp+DXavv/66Xn75Zb3yyitKTU3VF198oSlTpig+Pl7jxo2rd5n09HQVFBRo4MCBMk1TsbGxGjdunObOnSuHo+4JyMzMTJ1//vnq37+/T/3ZZ5/VZ599pnfffVddunTRJ598okmTJik+Pl5XXHFFnfU8+eSTmjFjRp16Tk6OdSawQ4cOSk5O1rZt23TgwAFrTEJCghISEpSfn6+SkhKrnpSUpJiYGOXm5qqiosKqp6SkqG3btsrJyfF5gFxwwQUKCgrS+vXrfebQt29fVVVVafPmzVbN6XSqX79+Kikp0TfffGPVQ0ND1atXLxUWFmrr1q1W3eVyqWfPntqzZ4927ar9/Dh/9iSPW96iHVbNMBwy2idJ7gp5S/bU1p1BMqI7S5WH5T28v7YeFCbDFS+zvEhm+aHaekikjIgYmWWFMo+W1tbDomWER8ssLZBZVW7VHRExUkikzKJdMj219246XPFSUJjMQ9uPhb6aelRnyREg78Fj+3f9+j1WT3Y8Ts3dU0nVsWMS3DZQEfFhKiuoUGVx7dn4sA7BCusQotKd5XIfqX0hbhMXqpCoIBVvK5OnsvZ4RHYOU1CbQBV9e1imt/bFvG1yGzkCHDqUV/sYkKToHpHyVntVvKXMqhkOQ+1SIuU+Uq3SHbWPDWewQ1HJEaosdqtsb+1+DAwPkGJk6+NET/RET2euJ5fLpcYyzFO9BbUFJSYmatq0aZo0aZJVmzVrlpYtW+azY+rjdru1b98+xcXFacGCBZo6daqKi4t9wt2RI0cUHx+vmTNnavLkyVa9oqJCLpdLb731ls87ZW+//Xbt2rVL77//fp3t1XfGLjExUQcPHlRkZKSks+u3hzPR03sbd9nijN01veNP2mtrP07N3VPOwc+t3diaz9j1jbnY1seJnuiJns5cT2VlZYqKilJJSYmVORri1zN25eXldc6y1TR6KoGBgUpIOPaJ/suXL9eIESPqrGvFihWqrKzUzTff7FN3u91yu92nte3g4GAFBwfXqQcEBCggwHc31hyYE9U8GBpbP3G9TakbhlFvvaE5nm69JXsyDKM2JLVI3SHVLZ9+vZ798v0Ckur2Zrfj1FC9qT05nL472XAY9e32066fuF5rfP27oN66YRinVbfzcWpsnZ7oqaE6PTW+Xt+Yhvg12I0cOVKzZ89W586dlZqaqpycHGVkZOi2226zxkyfPl27d++2PqsuPz9f2dnZGjBggIqKipSRkaHc3FwtXry4zvozMzOVlpamdu3a+dQjIyM1ePBg3X///QoNDVWXLl308ccfa8mSJcrIyGjZpgEAAFqIX4Pds88+q/T0dN15553av3+/4uPjNXHiRD3yyCPWmL1792rHjtr7rDwej5555hnl5eUpMDBQQ4cOVVZWlrp27eqz7ry8PK1du1YrV66sd9vLly/X9OnTNXbsWB06dEhdunTR7Nmz9dvf/rZFegUAAGhpfr3HrjUrLS2Vy+Vq1PVuNM17G3edelArMKJPgr+n0KpsKMzy9xSaxYXtB/l7CgBs4nQyB38rFgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgE34Ndh6PR+np6erWrZtCQ0OVnJysxx9/XKZpnnS55557Tj179lRoaKh69OihJUuW+Px8yJAhMgyjztfw4cN9xn399de69tpr5XK5FB4ern79+mnHjh3N3icAAMCZEODPjc+ZM0fz58/X4sWLlZqaqvXr12v8+PFyuVy655576l1m/vz5mj59uhYuXKh+/fopOztbEyZMUFRUlEaOHClJevPNN1VVVWUtc/DgQfXq1Us33nijVduyZYsuueQS/eY3v9GMGTMUGRmpr776SiEhIS3bNAAAQAvxa7DLysrSqFGjrDNpXbt21auvvqrs7OwGl1m6dKkmTpyo0aNHS5KSkpK0bt06zZkzxwp20dHRPsssX75cYWFhPsHuoYce0s9//nPNnTvXqiUnJzdbbwAAAGeaXy/FDho0SKtXr1Z+fr4kadOmTVq7dq2uueaaBpeprKysc1YtNDRU2dnZcrvd9S6TmZmpMWPGKDw8XJLk9Xr1j3/8Q927d9ewYcMUExOjAQMG6O23326exgAAAPzAr2fspk2bptLSUqWkpMjpdMrj8Wj27NkaO3Zsg8sMGzZMixYtUlpamvr06aMNGzZo0aJFcrvdKiwsVFxcnM/47Oxs5ebmKjMz06rt379fZWVl+uMf/6hZs2Zpzpw5ev/993X99dfro48+0uDBg+tst7KyUpWVldb3paWlkqTq6mpVV1dLkhwOhxwOh7xer7xerzW2pu7xeHzuH2yo7nQ6ZRiGtd7j69KxexMbUw8ICJBpmj51wzDkdDrrzLGhuj97Mk1TOuF+S8PhqFs3JMNoSt0rHb/67+/FPO36cfulpi7J2mZNz3Y9Ts3dk9djWrvRcBgyvabv4Wuo7tD3x6P+es16j69Lknni4Wug7nAaMk3Tp27NpZ66JFsfJ3qiJ3o6cz0d/+9T8Wuwe/311/Xyyy/rlVdeUWpqqr744gtNmTJF8fHxGjduXL3LpKenq6CgQAMHDpRpmoqNjdW4ceM0d+5cORx1T0BmZmbq/PPPV//+/a1azQ4aNWqUfv/730uSevfuraysLD3//PP1Brsnn3xSM2bMqFPPycmxzgR26NBBycnJ2rZtmw4cOGCNSUhIUEJCgvLz81VSUmLVk5KSFBMTo9zcXFVUVFj1lJQUtW3bVjk5OT4PkAsuuEBBQUFav369zxz69u2rqqoqbd682ao5nU7169dPJSUl+uabb6x6aGioevXqpcLCQm3dutWqu1wu9ezZU3v27NGuXbusuj97ksctb1Htm1kMwyGjfZLkrpC3ZE9t3RkkI7qzVHlY3sP7a+tBYTJc8TLLi2SWH6qth0TKiIiRWVYo82hpbT0sWkZ4tMzSAplV5VbdEREjhUTKLNol01N776bDFS8Fhck8tP1Y6KupR3WWHAHyHjy2f9ev32P1ZMfj1Nw9lVQdOybBbQMVER+msoIKVRbXno0P6xCssA4hKt1ZLveR2hfiNnGhCokKUvG2Mnkqa49HZOcwBbUJVNG3h2V6a1/M2ya3kSPAoUN5tY8BSYruESlvtVfFW8qsmuEw1C4lUu4j1SrdUfvYcAY7FJUcocpit8r21u7HwPAAKUa2Pk70RE/0dOZ6crlcaizDPNVbUFtQYmKipk2bpkmTJlm1WbNmadmyZT47pj5ut1v79u1TXFycFixYoKlTp6q4uNgn3B05ckTx8fGaOXOmJk+ebNWrqqoUHh6uRx99VA8//LBVnzp1qtauXav//Oc/dbZX3xm7xMREHTx4UJGRkZLOrt8ezkRP723cZYszdtf0jj9pr639ODV3TzkHP7d2Y2s+Y9c35mJbHyd6oid6OnM9lZWVKSoqSiUlJVbmaIhfz9iVl5fXOctW0+ipBAYGKiEhQdKxN0eMGDGizrpWrFihyspK3XzzzT71oKAg9evXT3l5eT71/Px8denSpd7tBQcHKzg4uE49ICBAAQG+u7HmwJyo5sHQ2PqJ621K3TCMeusNzfF06y3Zk2EYtSGpReoOqW759Ov17JfvF5BUtze7HaeG6k3tyeH03cmGw6hvt592/cT1WuPr3wX11g3DOK26nY9TY+v0RE8N1emp8fX6xjTEr8Fu5MiRmj17tjp37qzU1FTl5OQoIyNDt912mzVm+vTp2r17t/VZdfn5+crOztaAAQNUVFSkjIwM5ebmavHixXXWn5mZqbS0NLVr167Oz+6//36NHj1al112mYYOHar3339ff//737VmzZoW6xcAAKAl+TXYPfvss0pPT9edd96p/fv3Kz4+XhMnTtQjjzxijdm7d6/PhwZ7PB4988wzysvLU2BgoIYOHaqsrCx17drVZ915eXlau3atVq5cWe+2r7vuOj3//PN68skndc8996hHjx7629/+pksuuaRFegUAAGhpfr3HrjUrLS2Vy+Vq1PVuNM17G3edelArMKJPgr+n0KpsKMzy9xSaxYXtB/l7CgBs4nQyB38rFgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCaaFOx27typXbt2Wd9nZ2drypQpWrBgQbNNDAAAAKenScHuV7/6lT766CNJUkFBga688kplZ2froYce0syZM5t1ggAAAGicJgW73Nxc9e/fX5L0+uuv67zzzlNWVpZefvllvfTSS805PwAAADRSk4Kd2+1WcHCwJOnDDz/UtddeK0lKSUnR3r17m292AAAAaLQmBbvU1FQ9//zz+ve//61Vq1bp6quvliTt2bNH7dq1a9YJAgAAoHGaFOzmzJmjF154QUOGDNEvf/lL9erVS5L07rvvWpdoAQAAcGYFNGWhIUOGqLCwUKWlpYqKirLqd9xxh8LCwpptcgAAAGi8Jp2xq6ioUGVlpRXqvvvuO82bN095eXmKiYlp1gkCAACgcZoU7EaNGqUlS5ZIkoqLizVgwAA988wzSktL0/z585t1ggAAAGicJgW7jRs36tJLL5UkvfHGG4qNjdV3332nJUuW6E9/+lOzThAAAACN06RgV15eroiICEnSypUrdf3118vhcGjgwIH67rvvmnWCAAAAaJwmBbtzzjlHb7/9tnbu3KkPPvhAV111lSRp//79ioyMbNYJAgAAoHGaFOweeeQR/eEPf1DXrl3Vv39/XXTRRZKOnb376U9/2qwTBAAAQOM06eNOfvGLX+iSSy7R3r17rc+wk6TLL79c1113XbNNDgAAAI3XpGAnSR07dlTHjh21a9cuSVJCQgIfTgwAAOBHTboU6/V6NXPmTLlcLnXp0kVdunRR27Zt9fjjj8vr9Tb3HAEAANAITTpj99BDDykzM1N//OMfdfHFF0uS1q5dq8cee0xHjx7V7Nmzm3WSAAAAOLUmBbvFixdr0aJFuvbaa63aBRdcoE6dOunOO+8k2AEAAPhBky7FHjp0SCkpKXXqKSkpOnTo0A+eFAAAAE5fk4Jdr1699Oc//7lO/c9//rMuuOCCHzwpAAAAnL4mXYqdO3euhg8frg8//ND6DLtPP/1UO3fu1D//+c9mnSAAAAAap0ln7AYPHqz8/Hxdd911Ki4uVnFxsa6//np99dVXWrp0aXPPEQAAAI3Q5M+xi4+Pr/MmiU2bNikzM1MLFiz4wRMDAADA6WnSGTsAAAD8+BDsAAAAbIJgBwAAYBOndY/d9ddff9KfFxcX/5C5AAAA4Ac4rWDncrlO+fNf//rXP2hCAAAAaJrTCnYvvvhiS80DAAAAPxD32AEAANiEX4Odx+NRenq6unXrptDQUCUnJ+vxxx+XaZonXe65555Tz549FRoaqh49emjJkiU+Px8yZIgMw6jzNXz48HrX99vf/laGYWjevHnN1RoAAMAZ1+QPKG4Oc+bM0fz587V48WKlpqZq/fr1Gj9+vFwul+655556l5k/f76mT5+uhQsXql+/fsrOztaECRMUFRWlkSNHSpLefPNNVVVVWcscPHhQvXr10o033lhnfW+99ZY+++wzxcfHt0yTAAAAZ4hfg11WVpZGjRplnUnr2rWrXn31VWVnZze4zNKlSzVx4kSNHj1akpSUlKR169Zpzpw5VrCLjo72WWb58uUKCwurE+x2796tu+++Wx988EGDZ/MAAABaC78Gu0GDBmnBggXKz89X9+7dtWnTJq1du1YZGRkNLlNZWamQkBCfWmhoqLKzs+V2uxUYGFhnmczMTI0ZM0bh4eFWzev16pZbbtH999+v1NTUU861srJSlZWV1velpaWSpOrqalVXV0uSHA6HHA6HvF6vvF6vNbam7vF4fC4zN1R3Op0yDMNa7/F16dgl7MbUAwICZJqmT90wDDmdzjpzbKjuz55M05ROuCxvOBx164ZkGE2pe6XjV//9JfvTrh+3X2rqkqxt1vRs1+PU3D15Paa1Gw2HIdNr+h6+huoOfX886q/XrPf4uiSZJx6+BuoOpyHTNH3q1lzqqUuy9XGiJ3qipzPX0/H/PhW/Brtp06aptLRUKSkpcjqd8ng8mj17tsaOHdvgMsOGDdOiRYuUlpamPn36aMOGDVq0aJHcbrcKCwsVFxfnMz47O1u5ubnKzMz0qc+ZM0cBAQENXvI90ZNPPqkZM2bUqefk5FiBsUOHDkpOTta2bdt04MABa0xCQoISEhKUn5+vkpISq56UlKSYmBjl5uaqoqLCqqekpKht27bKycnxeYBccMEFCgoK0vr1633m0LdvX1VVVWnz5s1Wzel0ql+/fiopKdE333xj1UNDQ9WrVy8VFhZq69atVt3lcqlnz57as2ePdu3aZdX92ZM8bnmLdlg1w3DIaJ8kuSvkLdlTW3cGyYjuLFUelvfw/tp6UJgMV7zM8iKZ5Ydq6yGRMiJiZJYVyjxaWlsPi5YRHi2ztEBmVblVd0TESCGRMot2yfTUXuJ3uOKloDCZh7YfC3019ajOkiNA3oPH9u/69Xusnux4nJq7p5KqY8ckuG2gIuLDVFZQocpitzU+rEOwwjqEqHRnudxHal+I28SFKiQqSMXbyuSprD0ekZ3DFNQmUEXfHpbprX0xb5vcRo4Ahw7l1T4GJCm6R6S81V4VbymzaobDULuUSLmPVKt0R+1jwxnsUFRyhCqL3SrbW7sfA8MDpBjZ+jjREz3R05nr6VQfN3c8wzzVOxVa0PLly3X//ffrqaeeUmpqqr744gtNmTJFGRkZGjduXL3LVFRUaNKkSVq6dKlM01RsbKxuvvlmzZ07VwUFBYqNjfUZP3HiRH366ac+B2DDhg0aPny4Nm7caN1b17VrV02ZMkVTpkypd7v1nbFLTEzUwYMHFRkZKens+u3hTPT03sZdtjhjd03v+JP22tqPU3P3lHPwc2s3tuYzdn1jLrb1caIneqKnM9dTWVmZoqKiVFJSYmWOhvg12CUmJmratGmaNGmSVZs1a5aWLVvmk3jr43a7tW/fPsXFxWnBggWaOnWqiouL5XDUvtH3yJEjio+P18yZMzV58mSrPm/ePN17770+Yz0ejxwOhxITE7V9+/ZTzr20tFQul6tROxlN897GXace1AqM6JPg7ym0KhsKs/w9hWZxYftB/p4CAJs4nczh10ux5eXlPuFKkpVgTyUwMFAJCcf+w1y+fLlGjBhRZ10rVqxQZWWlbr75Zp/6LbfcoiuuuMKnNmzYMN1yyy0aP358U1oBAADwO78Gu5EjR2r27Nnq3LmzUlNTlZOTo4yMDN12223WmOnTp2v37t3WZ9Xl5+crOztbAwYMUFFRkTIyMpSbm6vFixfXWX9mZqbS0tLUrl07n3q7du3q1AIDA9WxY0f16NGjBToFAABoeX4Nds8++6zS09N15513av/+/YqPj9fEiRP1yCOPWGP27t2rHTtqb6D3eDx65plnlJeXp8DAQA0dOlRZWVnq2rWrz7rz8vK0du1arVy58ky1AwAA4Fd+vceuNeMeu5bHPXZnJ+6xAwBfp5M5+FuxAAAANkGwAwAAsAmCHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACbINgBAADYBMEOAADAJgh2AAAANkGwAwAAsAmCHQAAgE0Q7AAAAGyCYAcAAGATBDsAAACbCPD3BAAAwNlp2/rd/p5Cs+jWt5O/p2DhjB0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADbh12Dn8XiUnp6ubt26KTQ0VMnJyXr88cdlmuZJl3vuuefUs2dPhYaGqkePHlqyZInPz4cMGSLDMOp8DR8+XJLkdrs1depUnX/++QoPD1d8fLx+/etfa8+ePS3WKwAAQEsL8OfG58yZo/nz52vx4sVKTU3V+vXrNX78eLlcLt1zzz31LjN//nxNnz5dCxcuVL9+/ZSdna0JEyYoKipKI0eOlCS9+eabqqqqspY5ePCgevXqpRtvvFGSVF5ero0bNyo9PV29evVSUVGRJk+erGuvvVbr169v+cYBAABagF+DXVZWlkaNGmWdSevatateffVVZWdnN7jM0qVLNXHiRI0ePVqSlJSUpHXr1mnOnDlWsIuOjvZZZvny5QoLC7OCncvl0qpVq3zG/PnPf1b//v21Y8cOde7cudl6BAAAOFP8GuwGDRqkBQsWKD8/X927d9emTZu0du1aZWRkNLhMZWWlQkJCfGqhoaHKzs6W2+1WYGBgnWUyMzM1ZswYhYeHN7jekpISGYahtm3bNrjdyspK6/vS0lJJUnV1taqrqyVJDodDDodDXq9XXq/XGltT93g8PpeZG6o7nU4ZhmGt9/i6dOwSdmPqAQEBMk3Tp24YhpxOZ505NlT3Z0+maUonXJY3HI66dUMyjKbUvdLxq//+kv1p14/bLzV1SdY2a3q263Fq7p68HtPajYbDkOk1fQ9fQ3WHvj8e9ddr1nt8XZLMEw9fA3WH05Bpmj51ay711CXZ+jjREz01R09e73F1w5Dj+9ds84QnlOP712zzhBcDh+GQ1/T6vMYfu/2qvrrj2GtBQ3Wv7xyN718MzBNeDOqrV1dXt+hxOv7fp+LXYDdt2jSVlpYqJSVFTqdTHo9Hs2fP1tixYxtcZtiwYVq0aJHS0tLUp08fbdiwQYsWLZLb7VZhYaHi4uJ8xmdnZys3N1eZmZkNrvPo0aOaOnWqfvnLXyoyMrLeMU8++aRmzJhRp56Tk2MFxg4dOig5OVnbtm3TgQMHrDEJCQlKSEhQfn6+SkpKrHpSUpJiYmKUm5uriooKq56SkqK2bdsqJyfH5wFywQUXKCgoqM7l4r59+6qqqkqbN2+2ak6nU/369VNJSYm++eYbqx4aGqpevXqpsLBQW7duteoul0s9e/bUnj17tGvXLqvuz57kcctbtMOqGYZDRvskyV0hb0nt/ZCGM0hGdGep8rC8h/fX1oPCZLjiZZYXySw/VFsPiZQRESOzrFDm0dLaeli0jPBomaUFMqvKrbojIkYKiZRZtEump/YSv8MVLwWFyTy03ecJ7ojqLDkC5D14bP+uX7/H6smOx6m5eyqpOnZMgtsGKiI+TGUFFaosdlvjwzoEK6xDiEp3lst9pPY/lzZxoQqJClLxtjJ5KmuPR2TnMAW1CVTRt4dlemtfzNsmt5EjwKFDebWPAUmK7hEpb7VXxVvKrJrhMNQuJVLuI9Uq3VH72HAGOxSVHKHKYrfK9tbux8DwAClGtj5O9ERPzdHTd0W1cw9yBqlT2y4qqyxV4ZHa1/LQwDB1jOyk4ooiFVfUvpZHBEeqfZtYHTpyQIcra5/HbUOjFRXWTvsP71WFu/b52j48RhEhLu0t2amq417LYyPiFRYUrp3F24+Fvu91cnVWgCPAZ46S1CUqSdXeau0uqf3/6VDOvhY9Ti6XS41lmKd6p0ILWr58ue6//3499dRTSk1N1RdffKEpU6YoIyND48aNq3eZiooKTZo0SUuXLpVpmoqNjdXNN9+suXPnqqCgQLGxsT7jJ06cqE8//dTnQXU8t9utG264Qbt27dKaNWsaDHb1nbFLTEzUwYMHrWV+rL8Rtdbf8t7buMsWZ+yu6R1/0l5b+3Fq7p5yDn5u7cbWfMaub8zFtj5O9ERPzdHT1nU7a4ut+Ixdlz7xLXqcysrKFBUVpZKSkgZzirVufwa7xMRETZs2TZMmTbJqs2bN0rJly3wSb33cbrf27dunuLg4LViwQFOnTlVxcbEcjto3+h45ckTx8fGaOXOmJk+eXO86brrpJm3dulX/+te/1K5du0bPvbS0VC6Xq1E7GU3z3sZdpx7UCozok+DvKbQqGwqz/D2FZnFh+0H+ngLwo7dt/W5/T6FZdOvbqUXXfzqZw6+XYsvLy32CmCQrwZ5KYGCgEhKO/Ye5fPlyjRgxos66VqxYocrKSt188811lq8Jdd9++60++uij0wp1AAAAP0Z+DXYjR47U7Nmz1blzZ6WmpionJ0cZGRm67bbbrDHTp0/X7t27rc+qy8/PV3Z2tgYMGKCioiJlZGQoNzdXixcvrrP+zMxMpaWl1Qltbrdbv/jFL7Rx40a999578ng8KigokHTsHbVBQUEt2DUAAEDL8Guwe/bZZ5Wenq4777xT+/fvV3x8vCZOnKhHHnnEGrN3717t2FF7g6LH49EzzzyjvLw8BQYGaujQocrKylLXrl191p2Xl6e1a9dq5cqVdba7e/duvfvuu5Kk3r17+/zso48+0pAhQ5qtRwAAgDPFr/fYtWbcY9fyuMfu7MQ9dsDZg3vsGud0Mgd/KxYAAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBN+DXYej0fp6enq1q2bQkNDlZycrMcff1ymaZ50ueeee049e/ZUaGioevTooSVLlvj8fMiQITIMo87X8OHDrTGmaeqRRx5RXFycQkNDdcUVV+jbb79tkT4BAADOhAB/bnzOnDmaP3++Fi9erNTUVK1fv17jx4+Xy+XSPffcU+8y8+fP1/Tp07Vw4UL169dP2dnZmjBhgqKiojRy5EhJ0ptvvqmqqiprmYMHD6pXr1668cYbrdrcuXP1pz/9SYsXL1a3bt2Unp6uYcOG6b///a9CQkJatnEAAIAW4Ndgl5WVpVGjRlln0rp27apXX31V2dnZDS6zdOlSTZw4UaNHj5YkJSUlad26dZozZ44V7KKjo32WWb58ucLCwqxgZ5qm5s2bp4cfflijRo2SJC1ZskSxsbF6++23NWbMmGbvFQAAoKX59VLsoEGDtHr1auXn50uSNm3apLVr1+qaa65pcJnKyso6Z9RCQ0OVnZ0tt9td7zKZmZkaM2aMwsPDJUnbtm1TQUGBrrjiCmuMy+XSgAED9Omnn/7QtgAAAPzCr2fspk2bptLSUqWkpMjpdMrj8Wj27NkaO3Zsg8sMGzZMixYtUlpamvr06aMNGzZo0aJFcrvdKiwsVFxcnM/47Oxs5ebmKjMz06oVFBRIkmJjY33GxsbGWj87UWVlpSorK63vS0tLJUnV1dWqrq6WJDkcDjkcDnm9Xnm9XmtsTd3j8fjcP9hQ3el0yjAMa73H16Vj9yY2ph4QECDTNH3qhmHI6XTWmWNDdX/2ZJqmdML9lobDUbduSIbRlLpXOn7139+Ledr14/ZLTV2Stc2anu16nJq7J6/HtHaj4TBkek3fw9dQ3aHvj0f99Zr1Hl+XJPPEw9dA3eE0ZJqmT92aSz11SbY+TvRET83Rk9d7XN0w5Pj+Nds84Qnl+P412zzhxcBhOOQ1vT6v8cfuq6+v7jj2WtBQ3es7R+P7FwPzhBeD+urV1dUtepyO//ep+DXYvf7663r55Zf1yiuvKDU1VV988YWmTJmi+Ph4jRs3rt5l0tPTVVBQoIEDB8o0TcXGxmrcuHGaO3euHI66JyAzMzN1/vnnq3///j9ork8++aRmzJhRp56Tk2OdCezQoYOSk5O1bds2HThwwBqTkJCghIQE5efnq6SkxKonJSUpJiZGubm5qqiosOopKSlq27atcnJyfB4gF1xwgYKCgrR+/XqfOfTt21dVVVXavHmzVXM6nerXr59KSkr0zTffWPXQ0FD16tVLhYWF2rp1q1V3uVzq2bOn9uzZo127dll1f/Ykj1veoh1WzTAcMtonSe4KeUv21NadQTKiO0uVh+U9vL+2HhQmwxUvs7xIZvmh2npIpIyIGJllhTKPltbWw6JlhEfLLC2QWVVu1R0RMVJIpMyiXTI9tfduOlzxUlCYzEPbfZ7gjqjOkiNA3oPH9u/69Xusnux4nJq7p5KqY8ckuG2gIuLDVFZQocri2rPxYR2CFdYhRKU7y+U+UvufS5u4UIVEBal4W5k8lbXHI7JzmILaBKro28MyvbUv5m2T28gR4NChvNrHgCRF94iUt9qr4i1lVs1wGGqXEin3kWqV7qh9bDiDHYpKjlBlsVtle2v3Y2B4gBQjWx8neqKn5ujpu6LauQc5g9SpbReVVZaq8Ejta3loYJg6RnZScUWRiitqX8sjgiPVvk2sDh05oMOVtc/jtqHRigprp/2H96rCXft8bR8eo4gQl/aW7FTVca/lsRHxCgsK187i7cdC3/c6uTorwBHgM0dJ6hKVpGpvtXaX1P7/dChnX4seJ5fLpcYyzFO9BbUFJSYmatq0aZo0aZJVmzVrlpYtW+azY+rjdru1b98+xcXFacGCBZo6daqKi4t9wt2RI0cUHx+vmTNnavLkyVZ969atSk5OVk5Ojnr37m3VBw8erN69e+v//u//6myvvjN2iYmJOnjwoCIjIyX9eH8jaq2/5b23cZctzthd0zv+pL229uPU3D3lHPzc2o2t+Yxd35iLbX2c6ImemqOnret21hZb8Rm7Ln3iW/Q4lZWVKSoqSiUlJVbmaIhfz9iVl5fXOctW0+ipBAYGKiEhQdKxN0eMGDGizrpWrFihyspK3XzzzT71bt26qWPHjlq9erUV7EpLS/X555/rd7/7Xb3bCw4OVnBwcJ16QECAAgJ8d2PNgTlRzQO8sfUT19uUumEY9dYbmuPp1luyJ8MwakNSi9QdUt3y6dfr2S/fLyCpbm92O04N1Zvak8Ppu5MNh1Hfbj/t+onrtcbXvwvqrRuGcVp1Ox+nxtbpiZ4aqhuGIYej7vqPBbP66o76XsrlaOC1+bTr9czl2HZPXa/pr6WOU31jGuLXYDdy5EjNnj1bnTt3VmpqqnJycpSRkaHbbrvNGjN9+nTt3r3b+qy6/Px8ZWdna8CAASoqKlJGRoZyc3O1ePHiOuvPzMxUWlqa2rVr51M3DENTpkzRrFmzdO6551ofdxIfH6+0tLQW7RkAAKCl+DXYPfvss0pPT9edd96p/fv3Kz4+XhMnTtQjjzxijdm7d6927Ki9ju3xePTMM88oLy9PgYGBGjp0qLKystS1a1efdefl5Wnt2rVauXJlvdt+4IEHdOTIEd1xxx0qLi7WJZdcovfff5/PsAMAAK2WX++xa81KS0vlcrkadb0bTfPexl2nHtQKjOiT4O8ptCobCrP8PYVmcWH7Qf6eAvCjt239bn9PoVl069upRdd/OpmDvxULAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwH+ngBOzsx/1d9TaBZG91/6ewoAANgeZ+wAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADYR4O8JtFamaUqSSktLW3Y7ZeUtuv4zxWjCfiovO9wCMznzWvoxYjdlh4/4ewrNojSI4w6cymFe509r/TXZ42QIdk10+PCxB2NiYqKfZ9Ja3O7vCQAA0KodPnxYLpfrpGMMszHxD3V4vV7t2bNHERERMgzD39Npst27d+snP/mJ/vvf/6pTp07+ns4ZVVpaqsTERO3cuVORkZH+ng7OEI47cPawy/PdNE0dPnxY8fHxcjhOfhcdZ+yayOFwKCEhwd/T+MFqTu9GRES06gf9DxEZGXnW9n4247gDZw87PN9PdaauBm+eAAAAsAmCHQAAgE0Q7M5ykZGRGjx4cKs/Rd0UwcHBevTRRxUcHOzvqeAM4rgDZ4+z8fnOmycAAABsgjN2AAAANkGwAwAAsAmCHQAAgE0Q7M4CN910kwICAmQYhtq0aaMXX3yxwbG33367DMOo89XafPLJJxo5cqTi4+NlGIbefvvtUy6zZs0a9enTR8HBwTrnnHP00ksvtfg80byefPJJ9evXTxEREYqJiVFaWpry8vJOudyKFSuUkpKikJAQnX/++frnP/95BmYLoDn98Y9/lGEYmjJlyknH2f35TrCzucmTJ2vFihX69a9/rXfeeUeJiYn6zW9+o6+++uqky23atMn6+vLLL8/QbJvPkSNH1KtXLz333HONGr9t2zYNHz5cQ4cO1RdffKEpU6bo9ttv1wcffNDCM0Vz+vjjjzVp0iR99tlnWrVqldxut6666iodOdLw35/NysrSL3/5S/3mN79RTk6O0tLSlJaWptzc3DM4cwA/xLp16/TCCy/oggsuOOm4s+H5zrtiba5NmzZKSkrS5s2bJUnV1dUKDg7WlVdeqffff7/O+Ntvv12ZmZmN+kPDrYVhGHrrrbeUlpbW4JipU6fqH//4h8+Te8yYMSouLq53P6F1OHDggGJiYvTxxx/rsssuq3fM6NGjdeTIEb333ntWbeDAgerdu7eef/75MzVVAE1UVlamPn366C9/+YtmzZql3r17a968efWOPRue75yxs7GysjIdOXJEw4cPt2oBAQHq2rWrNm3adNJlAwICFBAQoLi4OL3zzjstPVW/+/TTT3XFFVf41IYNG6ZPP/3UTzNCcygpKZEkRUdHNziGYw+0bpMmTdLw4cPrPI/rczY83/lbsTaWn58vSerWrZtPvV27dtq3b1+9y/Tv318Oh0OXX365CgoK9MQTTygtLU3Z2dnq169fi8/ZXwoKChQbG+tTi42NVWlpqSoqKhQaGuqnmaGpvF6vpkyZoosvvljnnXdeg+MaOvYFBQUtPUUAP9Dy5cu1ceNGrVu3rlHjz4bnO8EOPu644w7dcccd1vcTJkyQy+XSvffeq3//+99+nBlweiZNmqTc3FytXbvW31MB0AJ27typyZMna9WqVQoJCfH3dH40CHY21r17d0nH3hhwvIMHDyoiIqJR6wgLC1NsbKx27NjR7PP7MenYsWOds5j79u1TZGQkZ+taobvuukvvvfeePvnkEyUkJJx0bEPHvmPHji05RQA/0IYNG7R//3716dPHqnk8Hn3yySf685//rMrKSjmdTp9lzobnO/fY2VibNm0UHh6uf/zjH1aturpa27dvV69evRq1jqqqKh04cEDt27dvqWn+KFx00UVavXq1T23VqlW66KKL/DQjNIVpmrrrrrv01ltv6V//+led2xDqw7EHWqfLL79cX375pb744gvrq2/fvho7dqy++OKLOqFOOkue7yZs7Z577jElmbfffrv597//3UxJSTENwzC//PJL0zRNMykpyRw4cKA1fujQoeYTTzxhfvTRR+ayZcvMzp07m5LMd955x18tNMnhw4fNnJwcMycnx5RkZmRkmDk5OeZ3331nmqZpTps2zbzlllus8Vu3bjXDwsLM+++/3/z666/N5557znQ6neb777/vrxbQBL/73e9Ml8tlrlmzxty7d6/1VV5ebo255ZZbzGnTplnf/+c//zEDAgLMp59+2vz666/NRx991AwMDLSeIwBaj8GDB5uTJ0+2vj8bn+8Eu7PAL37xC9PpdJqSzPDwcHPRokXWz1wul5mcnGx9/9Of/tQa63A4zA4dOpivvPKKP6b9g3z00UempDpf48aNM03TNMeNG2cOHjy4zjK9e/c2g4KCzKSkJPPFF1884/PGD1PfMZfkcywHDx5sPQ5qvP7662b37t3NoKAgMzU11fzHP/5xZicOoFmcGOzOxuc7n2MHAABgE9xjBwAAYBMEOwAAAJsg2AEAANgEwQ4AAMAmCHYAAAA2QbADAACwCYIdAACATRDsAAAAbIJgBwAAYBMEOwBogltvvVWGYVhf7dq109VXX63Nmzef1jrS0tJabpIAzjoEOwBooquvvlp79+7V3r17tXr1agUEBGjEiBH+nhaAsxjBDgCaKDg4WB07dlTHjh3Vu3dvTZs2TTt37tSBAwckSTt37tRNN92ktm3bKjo6WqNGjdL27dslSY899pgWL16sd955xzrrt2bNGknS1KlT1b17d4WFhSkpKUnp6elyu91+6hJAaxLg7wkAgB2UlZVp2bJlOuecc9SuXTu53W4NGzZMF110kf79738rICBAs2bNsi7X/uEPf9DXX3+t0tJSvfjii5Kk6OhoSVJERIReeuklxcfH68svv9SECRMUERGhBx54wJ8tAmgFCHYA0ETvvfee2rRpI0k6cuSI4uLi9N5778nhcOiVV16R1+vVokWLZBiGJOnFF19U27ZttWbNGl111VUKDQ1VZWWlOnbs6LPehx9+2Pp3165d9Yc//EHLly8n2AE4JYIdADTR0KFDNX/+fElSUVGR/vKXv+iaa65Rdna2Nm3apP/973+KiIjwWebo0aPasmXLSdf72muv6U9/+pO2bNmisrIyVVdXKzIyssX6AGAfBDsAaKLw8HCdc8451veLFi2Sy+XSwoULVVZWpgsvvFAvv/xyneU6dOjQ4Do//fRTjR07VjNmzNCwYcPkcrm0fPlyPfPMMy3SAwB7IdgBQDMxDEMOh0MVFRXq06ePXnvtNcXExDR4ti0oKEgej8enlpWVpS5duuihhx6yat99912LzhuAffCuWABoosrKShUUFKigoEBff/217r77bpWVlWnkyJEaO3as2rdvr1GjRunf//63tm3bpjVr1uiee+7Rrl27JB27f27z5s3Ky8tTYWGh3G63zj33XO3YsUPLly/Xli1b9Kc//UlvvfWWnzsF0FoQ7ACgid5//33FxcUpLi5OAwYM0Lp167RixQoNGTJEYWFh+uSTT9S5c2ddf/316tmzp37zm9/o6NGj1hm8CRMmqEePHurbt686dOig//znP7r22mv1+9//XnfddZd69+6trKwspaen+7lTAK2FYZqm6e9JAAAA4IfjjB0AAIBNEOwAAABsgmAHAABgEwQ7AAAAmyDYAQAA2ATBDgAAwCYIdgAAADZBsAMAALAJgh0AAIBNEOwAAABsgmAHAABgEwQ7AAAAm/j/6eLMpE+c8r8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert your results list into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Prepare data slices\n",
    "tau_fixed = 0.7\n",
    "beta_df = df[df['tau'] == tau_fixed].reset_index(drop=True)\n",
    "best_beta = beta_df.loc[beta_df['ppl'].idxmin(), 'beta']\n",
    "tau_df = df[df['beta'] == best_beta].reset_index(drop=True)\n",
    "\n",
    "# Helper to plot bar chart with zoomed y-axis\n",
    "def plot_bar_zoom(x, y, xlabel, ylabel, title, colors, width):\n",
    "    plt.figure()\n",
    "    plt.bar(x, y, width=width, color=colors[:len(x)])\n",
    "    plt.xticks(x)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    # Zoom y-axis to emphasize differences\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min - 0.1*y_range, y_max + 0.1*y_range)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# 1) Perplexity vs Beta (bar, zoomed)\n",
    "plot_bar_zoom(\n",
    "    beta_df['beta'],\n",
    "    beta_df['ppl'],\n",
    "    xlabel='Beta',\n",
    "    ylabel='Perplexity',\n",
    "    title=f'Perplexity vs Beta (tau={tau_fixed})',\n",
    "    colors=plt.cm.tab10.colors,\n",
    "    width=0.4\n",
    ")\n",
    "\n",
    "# 2) Perplexity vs Tau (bar, zoomed)\n",
    "plot_bar_zoom(\n",
    "    tau_df['tau'],\n",
    "    tau_df['ppl'],\n",
    "    xlabel='Tau',\n",
    "    ylabel='Perplexity',\n",
    "    title=f'Perplexity vs Tau (beta={best_beta})',\n",
    "    colors=plt.cm.Set2.colors,\n",
    "    width=0.1\n",
    ")\n",
    "\n",
    "# 3) Loss vs Beta (bar, zoomed)\n",
    "plot_bar_zoom(\n",
    "    beta_df['beta'],\n",
    "    beta_df['loss'],\n",
    "    xlabel='Beta',\n",
    "    ylabel='Loss',\n",
    "    title=f'Loss vs Beta (tau={tau_fixed})',\n",
    "    colors=plt.cm.Pastel1.colors,\n",
    "    width=0.4\n",
    ")\n",
    "\n",
    "# 4) Loss vs Tau (bar, zoomed)\n",
    "plot_bar_zoom(\n",
    "    tau_df['tau'],\n",
    "    tau_df['loss'],\n",
    "    xlabel='Tau',\n",
    "    ylabel='Loss',\n",
    "    title=f'Loss vs Tau (beta={best_beta})',\n",
    "    colors=plt.cm.Accent.colors,\n",
    "    width=0.1\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:23:45.165755Z",
     "iopub.status.busy": "2025-05-06T07:23:45.165164Z",
     "iopub.status.idle": "2025-05-06T09:08:50.358695Z",
     "shell.execute_reply": "2025-05-06T09:08:50.357986Z",
     "shell.execute_reply.started": "2025-05-06T07:23:45.165732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sweep 2] Starting beta sweep at tau = 0.5\n",
      "[Sweep 2][Beta 1/4] beta=0.2, tau=0.5\n",
      "  -> [Quantize] beta=0.2, tau=0.5\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3253fe1b68c44159be6f637430ef4890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_lustrical_time_05_06_2025_07h_23m_46s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.827       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 11.632       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.911       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.918       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.093       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.902       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.391       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.102     | 14.645       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.304     | 12.187       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.883       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.293       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.105     | 14.469       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000191\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.227       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.936       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.313       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.119     | 14.469       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000115\u001b[0m | 1048576     | 0.01000     | 0.295     | 12.205       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.901       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.333       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.14 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.135     | 14.478       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000072\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.196       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.898       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.307       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.449       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000121\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.219       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.923       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.312       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.103     | 14.457       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000181\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.182       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.277     | 10.898       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.31 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.314     | 12.323       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.124     | 14.444       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000126\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.205       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.878       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.285     | 12.321       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.117     | 14.436       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000095\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.191       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.891       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.29 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.286     | 12.308       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.448       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000065\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.199       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.889       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.304       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.102     | 14.448       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000022\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.185       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.895       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.318       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.2 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000027\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.451       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.827', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '11.632', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.911', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.918', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.093', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.902', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.391', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.102', 'fwd_time': '14.645', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.304', 'fwd_time': '12.187', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.883', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.293', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.105', 'fwd_time': '14.469', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000191', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.227', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.936', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.313', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.119', 'fwd_time': '14.469', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000115', 'samples': '1048576', 'damp': '0.01000', 'time': '0.295', 'fwd_time': '12.205', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.901', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.333', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.135', 'fwd_time': '14.478', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000072', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.196', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.898', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.307', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.449', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000121', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.219', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.923', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.312', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '14.457', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000181', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.182', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '10.898', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.314', 'fwd_time': '12.323', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.124', 'fwd_time': '14.444', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000126', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.205', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.878', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.285', 'fwd_time': '12.321', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.117', 'fwd_time': '14.436', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000095', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.191', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.891', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.286', 'fwd_time': '12.308', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.448', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000065', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.199', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.889', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.304', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.102', 'fwd_time': '14.448', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000022', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.185', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.895', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.318', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000027', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.451', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.2,\n",
      "  \"tau\": 0.5\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.2,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.5\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.2-t0.5\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0024442672729492188s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9656, ppl=7829.30\n",
      "[Sweep 2][Beta 2/4] beta=0.3, tau=0.5\n",
      "  -> [Quantize] beta=0.3, tau=0.5\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1f14b9866e44619c63d8ee1737963d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_prediabetes_time_05_06_2025_07h_38m_48s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.510       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.277     | 11.267       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.826       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.086     | 14.870       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.201       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.267     | 10.906       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.319       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.13 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.132     | 14.449       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.198       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.279     | 10.905       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.321       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.14 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.143     | 14.451       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000203\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.204       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.876       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.307       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.444       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000119\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.194       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.300     | 10.870       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.311       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.437       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000077\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.230       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.923       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.310       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.105     | 14.439       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000130\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.197       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.909       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.307       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.426       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000188\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.201       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.926       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.312       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.440       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.33 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000135\u001b[0m | 1048576     | 0.01000     | 0.329     | 12.161       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.284     | 10.873       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.306       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.446       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000105\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.165       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.302     | 10.888       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.307       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.451       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000071\u001b[0m | 1048576     | 0.01000     | 0.267     | 12.199       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.892       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.290       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.429       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000025\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.166       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.900       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.315       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.3 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.446       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.510', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '11.267', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.826', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.086', 'fwd_time': '14.870', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.201', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '10.906', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.319', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.132', 'fwd_time': '14.449', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.198', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '10.905', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.321', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.143', 'fwd_time': '14.451', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000203', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.204', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.876', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.307', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.444', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000119', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.194', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.300', 'fwd_time': '10.870', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.311', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.437', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000077', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.230', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.923', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.310', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.105', 'fwd_time': '14.439', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000130', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.197', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.909', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.307', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.426', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000188', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.201', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.926', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.312', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.440', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000135', 'samples': '1048576', 'damp': '0.01000', 'time': '0.329', 'fwd_time': '12.161', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.284', 'fwd_time': '10.873', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.306', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.446', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000105', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.165', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.302', 'fwd_time': '10.888', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.307', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.451', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000071', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '12.199', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.892', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.290', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.429', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000025', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.166', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.900', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.315', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.446', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.3,\n",
      "  \"tau\": 0.5\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.3,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.5\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.3-t0.5\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002173185348510742s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9780, ppl=7926.92\n",
      "[Sweep 2][Beta 3/4] beta=0.4, tau=0.5\n",
      "  -> [Quantize] beta=0.4, tau=0.5\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc306e787c04bcd9c1dbdb068d99abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_euhemerise_time_05_06_2025_07h_53m_49s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.511       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 11.266       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.890       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.105     | 14.884       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.185       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.863       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.317       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.442       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.214       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.906       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.328       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.446       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000215\u001b[0m | 1048576     | 0.01000     | 0.266     | 12.228       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.920       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.317       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.081     | 14.435       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000124\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.219       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.917       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.333       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.100     | 14.448       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000083\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.215       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.926       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.331       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.439       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000140\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.214       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.29 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.295     | 10.909       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.329       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.441       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000199\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.208       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.895       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.317       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.440       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000146\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.199       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.900       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.310       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.20 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.202     | 14.465       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000116\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.208       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.904       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.311       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.082     | 14.457       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000078\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.187       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.893       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.267     | 12.315       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.440       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.174       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.869       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.290       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.464       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.511', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '11.266', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.890', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.105', 'fwd_time': '14.884', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.185', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.863', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.317', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.442', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.214', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.906', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.328', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.446', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000215', 'samples': '1048576', 'damp': '0.01000', 'time': '0.266', 'fwd_time': '12.228', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.920', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.317', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.081', 'fwd_time': '14.435', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000124', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.219', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.917', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.333', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.100', 'fwd_time': '14.448', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000083', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.215', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.926', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.331', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.439', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000140', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.214', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.295', 'fwd_time': '10.909', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.329', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.441', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000199', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.208', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.895', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.317', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.440', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000146', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.199', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.900', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.310', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.202', 'fwd_time': '14.465', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000116', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.208', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.904', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.311', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.082', 'fwd_time': '14.457', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000078', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.187', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.893', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '12.315', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.440', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.174', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.869', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.290', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.464', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 0.5\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.5\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t0.5\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002561330795288086s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9574, ppl=7765.01\n",
      "[Sweep 2][Beta 4/4] beta=0.6, tau=0.5\n",
      "  -> [Quantize] beta=0.6, tau=0.5\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d7fdc81cc445b9a44eddb6fe59c955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_podal_time_05_06_2025_08h_08m_49s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.542       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 11.258       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.799       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.16 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.161     | 14.830       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.267     | 12.164       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.866       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.303       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.456       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.215       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.919       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.307       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.454       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000241\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.216       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.941       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.31 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.314     | 12.323       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.444       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000134\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.182       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.911       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.313       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.106     | 14.439       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000095\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.177       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.884       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000007\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.311       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.433       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000161\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.183       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.889       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.301       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.432       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000217\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.196       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.893       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.309       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.087     | 14.441       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000167\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.190       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.869       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.283     | 12.316       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.103     | 14.431       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000139\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.192       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.878       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.309       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.433       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000092\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.184       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.874       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.307       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.428       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000035\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.196       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.869       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.306       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.6 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000029\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.441       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.542', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '11.258', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.799', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.161', 'fwd_time': '14.830', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '12.164', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.866', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.303', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.456', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.215', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.919', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.307', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.454', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000241', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.216', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.941', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.314', 'fwd_time': '12.323', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.444', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000134', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.182', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.911', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.313', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.106', 'fwd_time': '14.439', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000095', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.177', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.884', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000007', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.311', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.433', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000161', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.183', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.889', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.301', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.432', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000217', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.196', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.893', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.309', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '14.441', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000167', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.190', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.869', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.283', 'fwd_time': '12.316', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '14.431', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000139', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.192', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.878', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.309', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.433', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000092', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.184', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.874', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.307', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.428', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000035', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.196', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.869', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.306', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000029', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.441', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.6,\n",
      "  \"tau\": 0.5\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.6,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.5\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.6-t0.5\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0032346248626708984s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9745, ppl=7899.04\n",
      "[Sweep 2] Best beta at tau=0.5: 0.4\n",
      "[Sweep 2] Starting tau sweep at beta = 0.4\n",
      "[Sweep 2][Tau 1/3] beta=0.4, tau=0.2\n",
      "  -> [Quantize] beta=0.4, tau=0.2\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f420996446f42a3b0017828f2d661fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_semitraditonal_time_05_06_2025_08h_23m_49s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.539       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 11.249       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.880       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.823       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.184       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.860       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.292       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.098     | 14.443       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.264       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.943       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.332       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.459       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000185\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.304       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.904       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.30 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.297     | 12.309       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.444       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000113\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.302       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.906       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.306       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.15 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.148     | 14.450       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000070\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.221       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.876       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.307       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.448       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000115\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.224       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.885       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.309       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.112     | 14.453       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000175\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.204       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.874       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.306       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.427       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000120\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.199       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.895       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.29 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.288     | 12.317       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.439       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000091\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.184       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.892       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.311       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.117     | 14.457       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000061\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.178       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.863       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.296       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.447       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000021\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.169       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.889       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.281     | 12.314       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000027\u001b[0m | 1048576     | 0.01000     | 1.091     | 14.437       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.539', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '11.249', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.880', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.823', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.184', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.860', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.292', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '14.443', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.264', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.943', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.332', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.459', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000185', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.304', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.904', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.297', 'fwd_time': '12.309', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.444', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000113', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.302', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.906', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.306', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.148', 'fwd_time': '14.450', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000070', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.221', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.876', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.307', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.448', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000115', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.224', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.885', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.309', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.112', 'fwd_time': '14.453', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000175', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.204', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.874', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.306', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.427', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000120', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.199', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.895', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.288', 'fwd_time': '12.317', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.439', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000091', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.184', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.892', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.311', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.117', 'fwd_time': '14.457', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000061', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.178', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.863', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.296', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.447', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000021', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.169', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.889', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '12.314', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000027', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '14.437', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 0.2\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.2\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t0.2\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002678394317626953s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9733, ppl=7889.56\n",
      "[Sweep 2][Tau 2/3] beta=0.4, tau=0.3\n",
      "  -> [Quantize] beta=0.4, tau=0.3\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02d6b9847c44bb4b75f20f607b98c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_fumigation_time_05_06_2025_08h_38m_50s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.497       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.278     | 11.268       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.801       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.086     | 14.862       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.266     | 12.156       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.868       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.313       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.442       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.208       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.909       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.325       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.434       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000196\u001b[0m | 1048576     | 0.01000     | 0.284     | 12.252       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.297     | 10.902       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.326       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.098     | 14.448       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000117\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.254       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.29 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.287     | 10.859       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.304       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.436       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000075\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.199       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.894       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.317       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.15 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.149     | 14.441       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000123\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.216       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.907       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.316       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.101     | 14.439       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000182\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.189       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.876       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.312       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.457       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000127\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.203       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.875       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.307       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.458       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000099\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.176       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.877       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.304       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.460       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000067\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.175       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.858       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.297       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.437       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000024\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.180       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.873       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.301       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 1.077     | 14.444       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.497', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '11.268', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.801', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.086', 'fwd_time': '14.862', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.266', 'fwd_time': '12.156', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.868', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.313', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.442', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.208', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.909', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.325', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.434', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000196', 'samples': '1048576', 'damp': '0.01000', 'time': '0.284', 'fwd_time': '12.252', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.297', 'fwd_time': '10.902', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.326', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '14.448', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000117', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.254', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.287', 'fwd_time': '10.859', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.304', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.436', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000075', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.199', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.894', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.317', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.149', 'fwd_time': '14.441', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000123', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.216', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.907', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.316', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.101', 'fwd_time': '14.439', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000182', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.189', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.876', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.312', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.457', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000127', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.203', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.875', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.307', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.458', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000099', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.176', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.877', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.304', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.460', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000067', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.175', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.858', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.297', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.437', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000024', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.180', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.873', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.301', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '1.077', 'fwd_time': '14.444', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 0.3\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.3\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t0.3\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002206563949584961s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9681, ppl=7848.91\n",
      "[Sweep 2][Tau 3/3] beta=0.4, tau=0.5\n",
      "  -> [Quantize] beta=0.4, tau=0.5\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d2a16d8ae948a48bf29a9992c3b7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_bolognese_time_05_06_2025_08h_53m_51s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.526       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 11.234       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.765       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.877       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.162       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.304     | 10.873       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.300       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.098     | 14.436       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.209       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.909       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.310       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.448       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000215\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.215       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.912       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.311       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.437       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000124\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.189       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.279     | 10.915       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.309       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.097     | 14.446       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000083\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.187       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.882       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.308       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.443       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000140\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.196       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.895       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.314       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.093     | 14.438       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000199\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.183       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.896       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.333       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.455       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000146\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.180       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.864       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.302       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.089     | 14.431       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000116\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.222       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.888       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.313       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.096     | 14.435       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000078\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.198       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.276     | 10.886       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.291       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.090     | 14.435       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.182       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.903       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.309       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000028\u001b[0m | 1048576     | 0.01000     | 1.083     | 14.432       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.526', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '11.234', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.765', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.877', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.162', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.304', 'fwd_time': '10.873', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.300', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '14.436', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.209', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.909', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.310', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.448', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000215', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.215', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.912', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.311', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.437', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000124', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.189', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '10.915', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.309', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '14.446', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000083', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.187', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.882', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.308', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.443', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000140', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.196', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.895', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.314', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '14.438', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000199', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.183', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.896', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.333', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.455', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000146', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.180', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.864', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.302', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '14.431', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000116', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.222', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.888', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.313', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.096', 'fwd_time': '14.435', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000078', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.198', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '10.886', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.291', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '14.435', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.182', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.903', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.309', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000028', 'samples': '1048576', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '14.432', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 0.5\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.5\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t0.5\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0020263195037841797s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9574, ppl=7765.01\n",
      "\n",
      "[Results 2] Summary of sweep at tau=0.5 and subsequent tau sweep:\n",
      "|   beta |   tau |    loss |     ppl |\n",
      "|-------:|------:|--------:|--------:|\n",
      "|    0.2 |   0.5 | 8.96563 | 7829.3  |\n",
      "|    0.3 |   0.5 | 8.97802 | 7926.92 |\n",
      "|    0.4 |   0.5 | 8.95738 | 7765.01 |\n",
      "|    0.6 |   0.5 | 8.9745  | 7899.04 |\n",
      "|    0.4 |   0.2 | 8.9733  | 7889.56 |\n",
      "|    0.4 |   0.3 | 8.96813 | 7848.91 |\n",
      "|    0.4 |   0.5 | 8.95738 | 7765.01 |\n"
     ]
    }
   ],
   "source": [
    "# --- Additional sweep: tau fixed at 0.5, beta ∈ [0.2, 0.3, 0.4, 0.6] (excluding 0.5) ---\n",
    "tau_fixed_2 = 0.5\n",
    "beta_values_2 = [0.2, 0.3, 0.4, 0.6]\n",
    "\n",
    "results2 = []\n",
    "print(\"[Sweep 2] Starting beta sweep at tau = 0.5\")\n",
    "for idx, beta in enumerate(beta_values_2, 1):\n",
    "    print(f\"[Sweep 2][Beta {idx}/{len(beta_values_2)}] beta={beta}, tau={tau_fixed_2}\")\n",
    "    quant_path = f\"{base_quant_path}-b{beta}-t{tau_fixed_2}\"\n",
    "    loss, ppl = quantize_and_eval(\n",
    "        model_id,\n",
    "        calib_tokenized,\n",
    "        eval_texts,\n",
    "        beta,\n",
    "        tau_fixed_2,\n",
    "        quant_path\n",
    "    )\n",
    "    results2.append({\"beta\": beta, \"tau\": tau_fixed_2, \"loss\": loss, \"ppl\": ppl})\n",
    "\n",
    "# Select best beta at tau=0.5\n",
    "best_beta_2 = min(results2, key=lambda x: x[\"ppl\"])[\"beta\"]\n",
    "print(f\"[Sweep 2] Best beta at tau={tau_fixed_2}: {best_beta_2}\")\n",
    "\n",
    "# --- Now sweep tau ∈ [0.2, 0.3, 0.5] at the selected best beta ---\n",
    "tau_values_2 = [0.2, 0.3, 0.5]\n",
    "print(f\"[Sweep 2] Starting tau sweep at beta = {best_beta_2}\")\n",
    "for idx, tau in enumerate(tau_values_2, 1):\n",
    "    print(f\"[Sweep 2][Tau {idx}/{len(tau_values_2)}] beta={best_beta_2}, tau={tau}\")\n",
    "    quant_path = f\"{base_quant_path}-b{best_beta_2}-t{tau}\"\n",
    "    loss, ppl = quantize_and_eval(\n",
    "        model_id,\n",
    "        calib_tokenized,\n",
    "        eval_texts,\n",
    "        best_beta_2,\n",
    "        tau,\n",
    "        quant_path\n",
    "    )\n",
    "    results2.append({\"beta\": best_beta_2, \"tau\": tau, \"loss\": loss, \"ppl\": ppl})\n",
    "\n",
    "# Summary of this second grid sweep\n",
    "import pandas as pd\n",
    "df2 = pd.DataFrame(results2)\n",
    "print(\"\\n[Results 2] Summary of sweep at tau=0.5 and subsequent tau sweep:\")\n",
    "print(df2.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T09:35:51.118078Z",
     "iopub.status.busy": "2025-05-06T09:35:51.117378Z",
     "iopub.status.idle": "2025-05-06T09:35:51.669716Z",
     "shell.execute_reply": "2025-05-06T09:35:51.669093Z",
     "shell.execute_reply.started": "2025-05-06T09:35:51.118032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build DataFrame from your secondary sweep results\n",
    "df2 = pd.DataFrame(results2)\n",
    "\n",
    "# Constants\n",
    "const_tau = 0.5\n",
    "df_beta = df2[df2['tau'] == const_tau].reset_index(drop=True)\n",
    "\n",
    "# Determine best beta for subsequent tau sweep\n",
    "best_beta = df_beta.loc[df_beta['ppl'].idxmin(), 'beta']\n",
    "df_tau = df2[df2['beta'] == best_beta].reset_index(drop=True)\n",
    "\n",
    "# Helper to plot zoomed bar chart\n",
    "def plot_zoomed_bar(x, y, xlabel, ylabel, title, cmap):\n",
    "    colors = cmap(np.linspace(0, 1, len(x)))\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    bars = ax.bar(x, y, color=colors, edgecolor='black', linewidth=0.8)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Zoom y-axis to highlight differences\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    margin = (y_max - y_min) * 0.15\n",
    "    ax.set_ylim(y_min - margin, y_max + margin)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# 1) Perplexity vs Beta @ τ = 0.5\n",
    "plot_zoomed_bar(\n",
    "    x=df_beta['beta'].astype(str),\n",
    "    y=df_beta['ppl'],\n",
    "    xlabel='β (Beta values)',\n",
    "    ylabel='Perplexity',\n",
    "    title='Perplexity vs Beta @ τ = 0.5',\n",
    "    cmap=plt.cm.Set2\n",
    ")\n",
    "\n",
    "# 2) Loss vs Beta @ τ = 0.5\n",
    "plot_zoomed_bar(\n",
    "    x=df_beta['beta'].astype(str),\n",
    "    y=df_beta['loss'],\n",
    "    xlabel='β (Beta values)',\n",
    "    ylabel='Avg NLL Loss',\n",
    "    title='Loss vs Beta @ τ = 0.5',\n",
    "    cmap=plt.cm.Pastel1\n",
    ")\n",
    "\n",
    "# 3) Perplexity vs Tau @ β = best_beta\n",
    "plot_zoomed_bar(\n",
    "    x=df_tau['tau'].astype(str),\n",
    "    y=df_tau['ppl'],\n",
    "    xlabel='τ (Tau values)',\n",
    "    ylabel='Perplexity',\n",
    "    title=f'Perplexity vs Tau @ β = {best_beta}',\n",
    "    cmap=plt.cm.Pastel2\n",
    ")\n",
    "\n",
    "# 4) Loss vs Tau @ β = best_beta\n",
    "plot_zoomed_bar(\n",
    "    x=df_tau['tau'].astype(str),\n",
    "    y=df_tau['loss'],\n",
    "    xlabel='τ (Tau values)',\n",
    "    ylabel='Avg NLL Loss',\n",
    "    title=f'Loss vs Tau @ β = {best_beta}',\n",
    "    cmap=plt.cm.Dark2\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T09:40:27.968530Z",
     "iopub.status.busy": "2025-05-06T09:40:27.967518Z",
     "iopub.status.idle": "2025-05-06T10:40:33.096782Z",
     "shell.execute_reply": "2025-05-06T10:40:33.095966Z",
     "shell.execute_reply.started": "2025-05-06T09:40:27.968503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sweep 3] Starting tau sweep at beta = 0.4\n",
      "[Sweep 3][Tau 1/4] beta=0.4, tau=0.1\n",
      "  -> [Quantize] beta=0.4, tau=0.1\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230bca48c5d04f918c4f3def04d59632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_sayonara_time_05_06_2025_09h_40m_29s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.904       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 11.624       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.941       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.938       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.202       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.31 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.306     | 10.897       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.416       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.568       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.336       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.863       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.300       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.17 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.165     | 14.451       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000177\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.360       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.944       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.322       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.436       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000110\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.300       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.279     | 10.899       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.320       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.442       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000066\u001b[0m | 1048576     | 0.01000     | 0.267     | 12.315       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.918       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.29 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.287     | 12.340       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.084     | 14.450       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000108\u001b[0m | 1048576     | 0.01000     | 0.281     | 12.327       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.896       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.283     | 12.309       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.115     | 14.428       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000170\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.315       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.876       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.311       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.445       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000113\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.275       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.881       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.284     | 12.310       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.105     | 14.447       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000084\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.265       | 2568.04MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.872       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.310       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.103     | 14.441       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000057\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.242       | 2568.53MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.868       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.327       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.096     | 14.379       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000019\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.255       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.267     | 10.872       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.382       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000027\u001b[0m | 1048576     | 0.01000     | 1.118     | 14.466       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.904', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '11.624', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.941', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.938', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.202', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.306', 'fwd_time': '10.897', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.416', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.568', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.336', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.863', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.300', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.165', 'fwd_time': '14.451', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000177', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.360', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.944', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.322', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.436', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000110', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.300', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '10.899', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.320', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.442', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000066', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '12.315', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.918', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.287', 'fwd_time': '12.340', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.084', 'fwd_time': '14.450', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000108', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '12.327', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.896', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.283', 'fwd_time': '12.309', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.115', 'fwd_time': '14.428', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000170', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.315', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.876', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.311', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.445', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000113', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.275', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.881', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.284', 'fwd_time': '12.310', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.105', 'fwd_time': '14.447', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000084', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.265', 'max_vram': '2568.04MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.872', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.310', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '14.441', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000057', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.242', 'max_vram': '2568.53MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.868', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.327', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.096', 'fwd_time': '14.379', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000019', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.255', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '10.872', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.382', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000027', 'samples': '1048576', 'damp': '0.01000', 'time': '1.118', 'fwd_time': '14.466', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 0.1\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.1\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t0.1\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.002533435821533203s                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9813, ppl=7953.29\n",
      "[Sweep 3][Tau 2/4] beta=0.4, tau=1e-06\n",
      "  -> [Quantize] beta=0.4, tau=1e-06\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6a492384244637ae5f43ab611a8384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_kerystic_time_05_06_2025_09h_55m_32s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.571       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 11.241       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.30 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.300     | 12.896       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.694       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.228       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.872       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.359       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.115     | 14.363       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.249       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.912       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.354       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.340       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000168\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.236       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.31 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.309     | 10.902       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.358       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.092     | 14.337       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.29 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000107\u001b[0m | 1048576     | 0.01000     | 0.289     | 12.226       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.276     | 10.898       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.366       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.087     | 14.357       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000061\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.222       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.903       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.369       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.087     | 14.340       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.29 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000102\u001b[0m | 1048576     | 0.01000     | 0.289     | 12.215       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.281     | 10.879       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.356       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.112     | 14.363       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000163\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.238       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.889       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.350       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.121     | 14.342       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000106\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.240       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.282     | 10.884       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.348       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.310       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000077\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.224       | 2568.04MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.862       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.280     | 12.359       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.107     | 14.341       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000052\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.210       | 2568.53MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.852       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.336       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.083     | 14.306       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000017\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.249       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.279     | 10.874       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.355       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000026\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.332       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.571', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '11.241', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.300', 'fwd_time': '12.896', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.694', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.228', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.872', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.359', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.115', 'fwd_time': '14.363', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.249', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.912', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.354', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.340', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000168', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.236', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.309', 'fwd_time': '10.902', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.358', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '14.337', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000107', 'samples': '1048576', 'damp': '0.01000', 'time': '0.289', 'fwd_time': '12.226', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '10.898', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.366', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '14.357', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000061', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.222', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.903', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.369', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '14.340', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000102', 'samples': '1048576', 'damp': '0.01000', 'time': '0.289', 'fwd_time': '12.215', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '10.879', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.356', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.112', 'fwd_time': '14.363', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000163', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.238', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.889', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.350', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.121', 'fwd_time': '14.342', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000106', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.240', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.282', 'fwd_time': '10.884', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.348', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.310', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000077', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.224', 'max_vram': '2568.04MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.862', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.280', 'fwd_time': '12.359', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '14.341', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000052', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.210', 'max_vram': '2568.53MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.852', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.336', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '14.306', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000017', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.249', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '10.874', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.355', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000026', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.332', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 1e-06\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 1e-06\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t1e-06\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0019943714141845703s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9662, ppl=7834.07\n",
      "[Sweep 3][Tau 3/4] beta=0.4, tau=0.6\n",
      "  -> [Quantize] beta=0.4, tau=0.6\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2f45408d464b0cbdc4ddcc3d2bf231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_pulsojets_time_05_06_2025_10h_10m_33s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.516       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 11.226       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.802       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.101     | 14.822       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.172       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.855       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.306       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.096     | 14.451       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.226       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.917       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.31 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.306     | 12.316       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.121     | 14.431       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000227\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.194       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.912       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.335       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.099     | 14.457       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000129\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.177       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.277     | 10.857       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.309       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.441       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000088\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.214       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.908       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.299       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.102     | 14.461       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000149\u001b[0m | 1048576     | 0.01000     | 0.276     | 12.209       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.914       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.317       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.11 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.107     | 14.445       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000209\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.181       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.910       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.323       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.480       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000155\u001b[0m | 1048576     | 0.01000     | 0.278     | 12.165       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.281     | 10.845       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.287       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.436       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000124\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.169       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.267     | 10.859       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.293       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 1.094     | 14.445       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000083\u001b[0m | 1048576     | 0.01000     | 0.273     | 12.164       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.858       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.283     | 12.294       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.429       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000030\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.179       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.899       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.300       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000029\u001b[0m | 1048576     | 0.01000     | 1.102     | 14.447       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.516', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '11.226', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.802', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.101', 'fwd_time': '14.822', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.172', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.855', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.306', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.096', 'fwd_time': '14.451', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.226', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.917', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.306', 'fwd_time': '12.316', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.121', 'fwd_time': '14.431', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000227', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.194', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.912', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.335', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '14.457', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000129', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.177', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '10.857', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.309', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.441', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000088', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.214', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.908', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.299', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.102', 'fwd_time': '14.461', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000149', 'samples': '1048576', 'damp': '0.01000', 'time': '0.276', 'fwd_time': '12.209', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.914', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.317', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '14.445', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000209', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.181', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.910', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.323', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.480', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000155', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '12.165', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '10.845', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.287', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.436', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000124', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.169', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '10.859', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.293', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '14.445', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000083', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '12.164', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.858', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.283', 'fwd_time': '12.294', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.429', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000030', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.179', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.899', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.300', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000029', 'samples': '1048576', 'damp': '0.01000', 'time': '1.102', 'fwd_time': '14.447', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 0.6\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 0.6\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t0.6\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0021088123321533203s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9650, ppl=7824.48\n",
      "[Sweep 3][Tau 4/4] beta=0.4, tau=1.0\n",
      "  -> [Quantize] beta=0.4, tau=1.0\n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9114ed442504988acdc975546f977fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_spiracles_time_05_06_2025_10h_25m_33s.log`\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.517       | 1030.63MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 11.239       | 1031.14MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.284     | 12.825       | 1031.40MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.095     | 14.862       | 1032.39MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.169       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.267     | 10.866       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.311       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.104     | 14.453       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.225       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.915       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.30 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000006\u001b[0m | 1048576     | 0.01000     | 0.302     | 12.300       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.087     | 14.439       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.32 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000266\u001b[0m | 1048576     | 0.01000     | 0.315     | 12.217       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.918       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.331       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.123     | 14.432       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000147\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.200       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.283     | 10.936       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.275     | 12.331       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.443       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000104\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.183       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.871       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.313       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.084     | 14.449       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000178\u001b[0m | 1048576     | 0.01000     | 0.272     | 12.221       | 2568.66MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.909       | 2568.66MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.331       | 2568.52MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.083     | 14.449       | 2567.52MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000262\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.190       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.881       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.305       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.459       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000190\u001b[0m | 1048576     | 0.01000     | 0.279     | 12.183       | 2568.25MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.885       | 2568.25MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.32 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.319     | 12.307       | 2568.11MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.115     | 14.455       | 2567.11MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000144\u001b[0m | 1048576     | 0.01000     | 0.270     | 12.205       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.903       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.28 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.277     | 12.320       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000005\u001b[0m | 1048576     | 0.01000     | 1.088     | 14.450       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000099\u001b[0m | 1048576     | 0.01000     | 0.269     | 12.200       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.909       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.274     | 12.315       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.080     | 14.439       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_attn                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000033\u001b[0m | 1048576     | 0.01000     | 0.268     | 12.181       | 2568.17MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for attn.c_proj                                              \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000016\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.904       | 2568.17MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_fc                                                 \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.271     | 12.307       | 2568.03MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Combining H and A with beta=0.4 for mlp.c_proj                                               \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000030\u001b[0m | 1048576     | 0.01000     | 1.085     | 14.447       | 2567.04MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.517', 'max_vram': '1030.63MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '11.239', 'max_vram': '1031.14MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.284', 'fwd_time': '12.825', 'max_vram': '1031.40MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '14.862', 'max_vram': '1032.39MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.169', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '10.866', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.311', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '14.453', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.225', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.915', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000006', 'samples': '1048576', 'damp': '0.01000', 'time': '0.302', 'fwd_time': '12.300', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '14.439', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000266', 'samples': '1048576', 'damp': '0.01000', 'time': '0.315', 'fwd_time': '12.217', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.918', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.331', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.123', 'fwd_time': '14.432', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000147', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.200', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.283', 'fwd_time': '10.936', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '12.331', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.443', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000104', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.183', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.871', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.313', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.084', 'fwd_time': '14.449', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000178', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '12.221', 'max_vram': '2568.66MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.909', 'max_vram': '2568.66MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.331', 'max_vram': '2568.52MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '14.449', 'max_vram': '2567.52MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000262', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.190', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.881', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.305', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.459', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000190', 'samples': '1048576', 'damp': '0.01000', 'time': '0.279', 'fwd_time': '12.183', 'max_vram': '2568.25MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.885', 'max_vram': '2568.25MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.319', 'fwd_time': '12.307', 'max_vram': '2568.11MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.115', 'fwd_time': '14.455', 'max_vram': '2567.11MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000144', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '12.205', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.903', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.277', 'fwd_time': '12.320', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000005', 'samples': '1048576', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '14.450', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000099', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '12.200', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.909', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '12.315', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '14.439', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000033', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '12.181', 'max_vram': '2568.17MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000016', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.904', 'max_vram': '2568.17MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '12.307', 'max_vram': '2568.03MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000030', 'samples': '1048576', 'damp': '0.01000', 'time': '1.085', 'fwd_time': '14.447', 'max_vram': '2567.04MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0.4,\n",
      "  \"tau\": 1.0\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0.4,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 1.0\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "     Quantization complete and saved to /kaggle/working/gpt2-quant-b0.4-t1.0\n",
      "✅ GPU VRAM and cache cleared.\n",
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0024590492248535156s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "     Eval complete: loss=8.9808, ppl=7949.11\n",
      "\n",
      "[Results 3] Updated full sweep results:\n",
      "|   beta |   tau |    loss |     ppl |\n",
      "|-------:|------:|--------:|--------:|\n",
      "|    0.2 | 0.5   | 8.96563 | 7829.3  |\n",
      "|    0.3 | 0.5   | 8.97802 | 7926.92 |\n",
      "|    0.4 | 1e-06 | 8.96624 | 7834.07 |\n",
      "|    0.4 | 0.1   | 8.98134 | 7953.29 |\n",
      "|    0.4 | 0.2   | 8.9733  | 7889.56 |\n",
      "|    0.4 | 0.3   | 8.96813 | 7848.91 |\n",
      "|    0.4 | 0.5   | 8.95738 | 7765.01 |\n",
      "|    0.4 | 0.5   | 8.95738 | 7765.01 |\n",
      "|    0.4 | 0.6   | 8.96501 | 7824.48 |\n",
      "|    0.4 | 1     | 8.98082 | 7949.11 |\n",
      "|    0.6 | 0.5   | 8.9745  | 7899.04 |\n"
     ]
    }
   ],
   "source": [
    "# --- Further sweep: tau ∈ [0.1, 1e-6, 0.6, 1.0] at beta = 0.4 ---\n",
    "fixed_beta = 0.4\n",
    "extra_tau_values = [0.1, 1e-6, 0.6, 1.0]\n",
    "\n",
    "print(f\"[Sweep 3] Starting tau sweep at beta = {fixed_beta}\")\n",
    "for idx, tau in enumerate(extra_tau_values, 1):\n",
    "    print(f\"[Sweep 3][Tau {idx}/{len(extra_tau_values)}] beta={fixed_beta}, tau={tau}\")\n",
    "    quant_path = f\"{base_quant_path}-b{fixed_beta}-t{tau}\"\n",
    "    loss, ppl = quantize_and_eval(\n",
    "        model_id,\n",
    "        calib_tokenized,\n",
    "        eval_texts,\n",
    "        fixed_beta,\n",
    "        tau,\n",
    "        quant_path\n",
    "    )\n",
    "    results2.append({\"beta\": fixed_beta, \"tau\": tau, \"loss\": loss, \"ppl\": ppl})\n",
    "\n",
    "# Update and display updated DataFrame\n",
    "df2 = pd.DataFrame(results2)\n",
    "print(\"\\n[Results 3] Updated full sweep results:\")\n",
    "print(df2.sort_values(by=[\"beta\", \"tau\"]).to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T06:45:32.702445Z",
     "iopub.status.busy": "2025-05-06T06:45:32.701689Z",
     "iopub.status.idle": "2025-05-06T06:56:56.467768Z",
     "shell.execute_reply": "2025-05-06T06:56:56.467003Z",
     "shell.execute_reply.started": "2025-05-06T06:45:32.702422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9200750a9d194e0581b0eb3310a9a6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native = `None`): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using legacy based config for targeting of modules                           \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_almose_time_05_06_2025_06h_45m_36s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram             |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.275     | 11.209       | 50.64MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 11.066       | 50.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.679       | 50.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.103     | 11.478       | 50.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_attn     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.278     | 10.363       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.271     | 10.286       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_fc        | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.303       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.121     | 11.436       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_attn     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.470       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.266     | 10.295       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_fc        | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.308       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.080     | 11.241       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_attn     | \u001b[92m0.0000000169\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.569       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.302     | 10.284       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.277       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.13 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.131     | 11.245       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_attn     | \u001b[92m0.0000000107\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.455       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.280       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.259       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.c_proj      | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 1.125     | 11.270       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.28 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_attn     | \u001b[92m0.0000000061\u001b[0m | 1048576     | 0.01000     | 0.281     | 10.532       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | attn.c_proj     | \u001b[92m0.0000000000\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.261       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_fc        | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 0.274     | 10.279       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.102     | 11.241       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_attn     | \u001b[92m0.0000000102\u001b[0m | 1048576     | 0.01000     | 0.266     | 10.460       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.300     | 10.270       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.269       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.09 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.091     | 11.273       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_attn     | \u001b[92m0.0000000163\u001b[0m | 1048576     | 0.01000     | 0.273     | 10.483       | 1586.77MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.296       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.275     | 10.270       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.c_proj      | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 1.124     | 11.257       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_attn     | \u001b[92m0.0000000106\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.463       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | attn.c_proj     | \u001b[92m0.0000000001\u001b[0m | 1048576     | 0.01000     | 0.272     | 10.284       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.292       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.10 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.c_proj      | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 1.096     | 11.248       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.30 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_attn     | \u001b[92m0.0000000077\u001b[0m | 1048576     | 0.01000     | 0.303     | 10.492       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.29 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | attn.c_proj     | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.295     | 10.261       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.279       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.c_proj      | \u001b[92m0.0000000003\u001b[0m | 1048576     | 0.01000     | 1.081     | 11.246       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_attn     | \u001b[92m0.0000000052\u001b[0m | 1048576     | 0.01000     | 0.268     | 10.406       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | attn.c_proj     | \u001b[92m0.0000000004\u001b[0m | 1048576     | 0.01000     | 0.267     | 10.271       | 1587.27MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.276       | 1587.27MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.12 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.c_proj      | \u001b[92m0.0000000008\u001b[0m | 1048576     | 0.01000     | 1.115     | 11.241       | 1587.27MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_attn on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_attn                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_attn in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module          | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_attn     | \u001b[92m0.0000000017\u001b[0m | 1048576     | 0.01000     | 0.270     | 10.478       | 1587.27MB, 11.62MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_attn                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer attn.c_proj on cuda:0, compute on cuda:1                                          \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for attn.c_proj                                    \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for attn.c_proj in 0.27 seconds. Final damp: 0.01000                   \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | attn.c_proj     | \u001b[92m0.0000000015\u001b[0m | 1048576     | 0.01000     | 0.267     | 10.277       | 1586.77MB, 9.92MB      | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer attn.c_proj                                                 \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_fc on cuda:0, compute on cuda:1                                             \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_fc                                       \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_fc in 0.27 seconds. Final damp: 0.01000                      \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_fc        | \u001b[92m0.0000000002\u001b[0m | 1048576     | 0.01000     | 0.269     | 10.275       | 1586.77MB, 12.79MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_fc                                                    \n",
      "\u001b[32mINFO\u001b[0m  GPTQ layer mlp.c_proj on cuda:0, compute on cuda:1                                           \n",
      "\u001b[36mDEBUG\u001b[0m Applying activation order (desc_act=True) for mlp.c_proj                                     \n",
      "\u001b[32mINFO\u001b[0m  Finished quantization for mlp.c_proj in 1.08 seconds. Final damp: 0.01000                    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.c_proj      | \u001b[92m0.0000000027\u001b[0m | 1048576     | 0.01000     | 1.083     | 11.250       | 1586.77MB, 12.77MB     | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.c_proj                                                  \n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '11.209', 'max_vram': '50.64MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '11.066', 'max_vram': '50.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.679', 'max_vram': '50.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '11.478', 'max_vram': '50.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_attn', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.278', 'fwd_time': '10.363', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.271', 'fwd_time': '10.286', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_fc', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.303', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.121', 'fwd_time': '11.436', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_attn', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.470', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.266', 'fwd_time': '10.295', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_fc', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.308', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.080', 'fwd_time': '11.241', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_attn', 'loss': '0.0000000169', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.569', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.302', 'fwd_time': '10.284', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.277', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.131', 'fwd_time': '11.245', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_attn', 'loss': '0.0000000107', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.455', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.280', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.259', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '1.125', 'fwd_time': '11.270', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_attn', 'loss': '0.0000000061', 'samples': '1048576', 'damp': '0.01000', 'time': '0.281', 'fwd_time': '10.532', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'attn.c_proj', 'loss': '0.0000000000', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.261', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_fc', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '0.274', 'fwd_time': '10.279', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.102', 'fwd_time': '11.241', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_attn', 'loss': '0.0000000102', 'samples': '1048576', 'damp': '0.01000', 'time': '0.266', 'fwd_time': '10.460', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.300', 'fwd_time': '10.270', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.269', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.091', 'fwd_time': '11.273', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_attn', 'loss': '0.0000000163', 'samples': '1048576', 'damp': '0.01000', 'time': '0.273', 'fwd_time': '10.483', 'max_vram': '1586.77MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.296', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.275', 'fwd_time': '10.270', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '1.124', 'fwd_time': '11.257', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_attn', 'loss': '0.0000000106', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.463', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'attn.c_proj', 'loss': '0.0000000001', 'samples': '1048576', 'damp': '0.01000', 'time': '0.272', 'fwd_time': '10.284', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.292', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '1.096', 'fwd_time': '11.248', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_attn', 'loss': '0.0000000077', 'samples': '1048576', 'damp': '0.01000', 'time': '0.303', 'fwd_time': '10.492', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'attn.c_proj', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.295', 'fwd_time': '10.261', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.279', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.c_proj', 'loss': '0.0000000003', 'samples': '1048576', 'damp': '0.01000', 'time': '1.081', 'fwd_time': '11.246', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_attn', 'loss': '0.0000000052', 'samples': '1048576', 'damp': '0.01000', 'time': '0.268', 'fwd_time': '10.406', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'attn.c_proj', 'loss': '0.0000000004', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '10.271', 'max_vram': '1587.27MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.276', 'max_vram': '1587.27MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.c_proj', 'loss': '0.0000000008', 'samples': '1048576', 'damp': '0.01000', 'time': '1.115', 'fwd_time': '11.241', 'max_vram': '1587.27MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_attn', 'loss': '0.0000000017', 'samples': '1048576', 'damp': '0.01000', 'time': '0.270', 'fwd_time': '10.478', 'max_vram': '1587.27MB, 11.62MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'attn.c_proj', 'loss': '0.0000000015', 'samples': '1048576', 'damp': '0.01000', 'time': '0.267', 'fwd_time': '10.277', 'max_vram': '1586.77MB, 9.92MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_fc', 'loss': '0.0000000002', 'samples': '1048576', 'damp': '0.01000', 'time': '0.269', 'fwd_time': '10.275', 'max_vram': '1586.77MB, 12.79MB'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.c_proj', 'loss': '0.0000000027', 'samples': '1048576', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '11.250', 'max_vram': '1586.77MB, 12.77MB'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:3.0.0-dev\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0,\n",
      "    \"v2\": false,\n",
      "    \"v2_alpha\": 0.25\n",
      "  },\n",
      "  \"beta\": 0,\n",
      "  \"tau\": 1e-06\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "config.json\n",
      "quant_log.csv\n",
      "generation_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"quantization_config\": {\n",
      "        \"beta\": 0,\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:3.0.0-dev\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "            \"v2\": false,\n",
      "            \"v2_alpha\": 0.25\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true,\n",
      "        \"tau\": 1e-06\n",
      "    },\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 1520.31MB, 1.48GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 158.52MB, 0.15GB                                                       \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1361.78MB, 1.33GB - 89.57%                                                  \n",
      "Quantized GPT-2 saved to /kaggle/working/gpt2-calibrated-8bit-g128\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_id    = \"gpt2\"\n",
    "quant_path  = \"/kaggle/working/gpt2-calibrated-8bit-g128\"\n",
    "quant_cfg   = QuantizeConfig(bits=8, group_size=128, beta=0, tau=1e-6)\n",
    "model       = GPTQModel.load(model_id, quant_cfg, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_len = tokenizer.model_max_length\n",
    "\n",
    "calib_tokenized = []\n",
    "for txt in calibration_texts:\n",
    "    enc = tokenizer(\n",
    "        txt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "    )\n",
    "    calib_tokenized.append({\n",
    "        \"input_ids\":      enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "    })\n",
    "\n",
    "model.quantize(calib_tokenized, batch_size=8)\n",
    "os.makedirs(os.path.dirname(quant_path), exist_ok=True)\n",
    "model.save(quant_path)\n",
    "print(f\"Quantized GPT-2 saved to {quant_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized 8 bit, beta = 2, temp = 0.7, WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:04:12.757968Z",
     "iopub.status.busy": "2025-05-06T07:04:12.757680Z",
     "iopub.status.idle": "2025-05-06T07:06:26.395857Z",
     "shell.execute_reply": "2025-05-06T07:06:26.395088Z",
     "shell.execute_reply.started": "2025-05-06T07:04:12.757950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\n",
      "If you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\n",
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0020589828491210938s                                          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n",
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n",
      "[Setup] Loading Wikitext-2 for evaluation\n",
      "Using 1949 non-empty lines from Wikitext-2 for evaluation.\n",
      "     Eval complete: loss=8.9618, ppl=7799.30\n"
     ]
    }
   ],
   "source": [
    "import os, math, torch, pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "from itertools import islice\n",
    "\n",
    "# 1) Load your quantized GPT-2\n",
    "quant_path = \"/kaggle/working/gpt2-calibrated-8bit-g128\"\n",
    "quant_cfg  = QuantizeConfig(bits=8, group_size=128, beta=0, tau=1e-6)\n",
    "model = GPTQModel.from_pretrained(\n",
    "    quant_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantize_config=quant_cfg\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"[Setup] Loading Wikitext-2 for evaluation\")\n",
    "wt2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation[:3000]\")\n",
    "eval_texts = [ex[\"text\"].strip() for ex in wt2 if ex[\"text\"].strip()]\n",
    "print(f\"Using {len(eval_texts)} non-empty lines from Wikitext-2 for evaluation.\")\n",
    "\n",
    "# 3) Tokenize with padding\n",
    "tokenizer           = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length,\n",
    ")\n",
    "input_ids      = encodings.input_ids.to(model.device)\n",
    "attention_mask = encodings.attention_mask.to(model.device)\n",
    "\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "        for i in range(0, len(eval_texts), batch_size):\n",
    "            b_ids  = input_ids[i:i+batch_size]\n",
    "            b_mask = attention_mask[i:i+batch_size]\n",
    "            out    = model(input_ids=b_ids, attention_mask=b_mask, labels=b_ids)\n",
    "            losses.append(out.loss.item())\n",
    "\n",
    "avg_loss = sum(losses) / len(losses)\n",
    "perplexity = math.exp(avg_loss)\n",
    "print(f\"     Eval complete: loss={avg_loss:.4f}, ppl={perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:08:04.265626Z",
     "iopub.status.busy": "2025-05-06T07:08:04.265356Z",
     "iopub.status.idle": "2025-05-06T07:08:04.416863Z",
     "shell.execute_reply": "2025-05-06T07:08:04.416015Z",
     "shell.execute_reply.started": "2025-05-06T07:08:04.265607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3348529662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mllm_int8_skip_modules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4228\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4229\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4230\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 1) Load GPT-2 with 8-bit quantization using bitsandbytes\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "wt2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation[:3000]\")\n",
    "eval_texts = [ex[\"text\"].strip() for ex in wt2 if ex[\"text\"].strip()]\n",
    "print(f\"Using {len(eval_texts)} non-empty lines from Wikitext-2 for evaluation.\")\n",
    "\n",
    "# 3) Tokenize\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length,\n",
    ")\n",
    "input_ids = encodings.input_ids.to(model.device)\n",
    "attention_mask = encodings.attention_mask.to(model.device)\n",
    "\n",
    "# 4) Evaluate metrics\n",
    "batch_size = 8\n",
    "losses, top1_accs, top5_accs = [], [], []\n",
    "\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "        for i in range(0, len(eval_texts), batch_size):\n",
    "            b_ids  = input_ids[i:i+batch_size]\n",
    "            b_mask = attention_mask[i:i+batch_size]\n",
    "            out    = model(input_ids=b_ids, attention_mask=b_mask, labels=b_ids)\n",
    "            losses.append(out.loss.item())\n",
    "\n",
    "avg_loss = sum(losses) / len(losses)\n",
    "perplexity = math.exp(avg_loss)\n",
    "print(f\"     Eval complete: loss={avg_loss:.4f}, ppl={perplexity:.2f}\")\n",
    "\n",
    "# 6) Generate sample continuations\n",
    "print(\"\\n--- Generations on Wikitext-2 ---\")\n",
    "for txt in texts[:3]:\n",
    "    enc = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(**enc, max_new_tokens=50, do_sample=False)\n",
    "    cont = tokenizer.decode(gen[0, enc[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    print(\"Prompt:      \", txt[:80] + \"…\")\n",
    "    print(\"Continuation:\", cont.strip())\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Comparison: Custom GPTQ vs. HF 8-bit GPT-2\n",
    "\n",
    "Below is a comparison between our custom GPTQ-quantized GPT-2 model (β = 2.0, τ = 0.7) and the default 8-bit GPT-2 model from the Hugging Face library, evaluated on 128 lines from the Wikitext-2 validation set.\n",
    "\n",
    "| Metric             | GPTQ (β=2.0, τ=0.7) | HF 8-bit GPT-2 |\n",
    "|--------------------|--------------------:|----------------:|\n",
    "| Avg NLL Loss       |              8.4160 |          8.5026 |\n",
    "| Perplexity         |           4518.89   |        4927.66  |\n",
    "| Top-1 Token Acc.   |             35.18 % |         35.50 % |\n",
    "| Top-5 Token Acc.   |             56.16 % |         56.36 % |\n",
    "\n",
    "Our KL-Aware GPTQ model demonstrates better loss and perplexity, showing stronger probabilistic modeling of the text. While the Hugging Face model performs slightly better in top-1 and top-5 token accuracy, the difference is minimal. This suggests that our quantization settings preserve model confidence better, with only marginal trade-offs in exact token prediction. I believe this will require a thorough Beta, temperature values testing to give some final analysi\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7155573,
     "sourceId": 11425261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7296411,
     "sourceId": 11629547,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 184692496,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
