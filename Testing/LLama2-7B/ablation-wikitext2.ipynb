{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/Compression-Framework-for-EdgeAI\n!git clone https://github.com/ha405/Compression-Framework-for-EdgeAI","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:48:06.139644Z","iopub.execute_input":"2025-06-25T18:48:06.140301Z","iopub.status.idle":"2025-06-25T18:48:07.423404Z","shell.execute_reply.started":"2025-06-25T18:48:06.140277Z","shell.execute_reply":"2025-06-25T18:48:07.422478Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Compression-Framework-for-EdgeAI'...\nremote: Enumerating objects: 998, done.\u001b[K\nremote: Counting objects: 100% (154/154), done.\u001b[K\nremote: Compressing objects: 100% (108/108), done.\u001b[K\nremote: Total 998 (delta 102), reused 87 (delta 46), pack-reused 844 (from 1)\u001b[K\nReceiving objects: 100% (998/998), 16.05 MiB | 36.77 MiB/s, done.\nResolving deltas: 100% (632/632), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Run the below command only once per session. If you reset session run again. ","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/working/Compression-Framework-for-EdgeAI/requirements.txt\n!pip install logbar\n!pip install tokenicer\n!pip install device_smi\n!pip install random_word\n!pip install datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\nlibrary_path = \"/kaggle/working/Compression-Framework-for-EdgeAI/KLAWQ\" \nif library_path not in sys.path:\n     sys.path.insert(0, library_path)\n     print(f\"Added '{library_path}' to sys.path\")\nfrom quant import GPTQModel, QuantizeConfig ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:50:12.764587Z","iopub.execute_input":"2025-06-25T18:50:12.765101Z","iopub.status.idle":"2025-06-25T18:50:35.175379Z","shell.execute_reply.started":"2025-06-25T18:50:12.765062Z","shell.execute_reply":"2025-06-25T18:50:35.174804Z"}},"outputs":[{"name":"stdout","text":"Added '/kaggle/working/Compression-Framework-for-EdgeAI/KLAWQ' to sys.path\n\n\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n","output_type":"stream"},{"name":"stderr","text":"2025-06-25 18:50:24.737506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750877424.898543      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750877424.944832      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport shutil\nimport math\nimport pandas as pd\nfrom transformers import AutoTokenizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datasets import load_dataset, DatasetDict\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:50:40.314475Z","iopub.execute_input":"2025-06-25T18:50:40.315217Z","iopub.status.idle":"2025-06-25T18:50:40.318961Z","shell.execute_reply.started":"2025-06-25T18:50:40.315194Z","shell.execute_reply":"2025-06-25T18:50:40.318171Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## WikiText-2","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\ntrain_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]\n\ncalibration_dataset = train_dataset.select(range(1000))\n\ndataset_splits = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset,\n    \"calibration\": calibration_dataset,\n})\n\nprint({k: len(v) for k, v in dataset_splits.items()})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:50:42.635369Z","iopub.execute_input":"2025-06-25T18:50:42.635637Z","iopub.status.idle":"2025-06-25T18:50:45.639734Z","shell.execute_reply.started":"2025-06-25T18:50:42.635618Z","shell.execute_reply":"2025-06-25T18:50:45.639171Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c59078a741648e5a2241f499da6a285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a6212274df4463b8ae272c5b4425c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20170374784045dba1ab81e603fd92e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de41e30dbf27474caf8e9ffe226f372a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b405fa80206747dbb4fbe99f8ebd40dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dade1eeb1942401487d34394f4c5011c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c703d1e9c304fb4bd29ebda53a92554"}},"metadata":{}},{"name":"stdout","text":"{'train': 36718, 'validation': 3760, 'test': 4358, 'calibration': 1000}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"To Clear GPU Cache","metadata":{}},{"cell_type":"code","source":"def clear_gpu_cache():\n    gc.collect()  \n    torch.cuda.empty_cache()  \n    torch.cuda.ipc_collect()  \n    print(\"✅ GPU VRAM and cache cleared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:50:50.471003Z","iopub.execute_input":"2025-06-25T18:50:50.471325Z","iopub.status.idle":"2025-06-25T18:50:50.475462Z","shell.execute_reply.started":"2025-06-25T18:50:50.471304Z","shell.execute_reply":"2025-06-25T18:50:50.474805Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Quantization Functions","metadata":{}},{"cell_type":"code","source":"def clear_quant_path(path=None):\n    if path and os.path.exists(path):\n        shutil.rmtree(path)\n    torch.cuda.empty_cache()\n    gc.collect()\n\ndef quantize_and_eval(\n    model_id: str,\n    calib_tokenized: dict,\n    eval_texts: list[str],\n    beta: float,\n    tau: float,\n    quant_path: str,\n    tokenizer: AutoTokenizer,\n    max_len: int,\n    batch_size: int = 8\n):\n    print(f\"  -> [Quantize] beta={beta}, tau={tau}\")\n    clear_quant_path(quant_path)\n\n    # 1) Quantize\n    quant_cfg = QuantizeConfig(bits=4, group_size=-1, beta=beta, tau=tau)\n    model = GPTQModel.load(\n        model_id,\n        quant_cfg,\n        trust_remote_code=True,\n        torch_dtype=\"auto\",\n        device_map=\"auto\"\n    )\n\n    # Format calibration data\n    calibration_data = []\n    input_ids_tensor     = calib_tokenized['input_ids']\n    attention_mask_tensor= calib_tokenized['attention_mask']\n    for i in range(input_ids_tensor.size(0)):\n        calibration_data.append({\n            \"input_ids\":     input_ids_tensor[i].tolist(),\n            \"attention_mask\":attention_mask_tensor[i].tolist()\n        })\n\n    model.quantize(calibration_data, batch_size=batch_size)\n    os.makedirs(os.path.dirname(quant_path), exist_ok=True)\n    model.save(quant_path)\n    print(f\"     Quantization complete and saved to {quant_path}\")\n\n    # Free GPU RAM\n    del model\n    clear_quant_path()\n\n    # 2) Load the quantized model\n    model = GPTQModel.from_pretrained(\n        quant_path,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        quantize_config=quant_cfg\n    )\n    model.eval()\n\n    # 3) Tokenize evaluation texts\n    encodings = tokenizer(\n        eval_texts,\n        return_tensors=\"pt\",\n        padding=\"longest\",\n        truncation=True,\n        max_length=max_len\n    )\n    input_ids     = encodings.input_ids\n    attention_mask= encodings.attention_mask\n\n    # 4) Compute loss & perplexity with proper pad‐masking\n    total_nll    = 0.0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for i in tqdm(\n            range(0, input_ids.size(0), batch_size),\n            desc=\"     Evaluating PPL\",\n            leave=False\n        ):\n            b_ids  = input_ids[i:i+batch_size].to(model.device)\n            b_mask = attention_mask[i:i+batch_size].to(model.device)\n\n            # --- mask out pads in the labels ---\n            labels = b_ids.clone()\n            labels[b_mask == 0] = -100  # ignore padding\n\n            out = model(\n                input_ids=b_ids,\n                attention_mask=b_mask,\n                labels=labels\n            )\n\n            # out.loss is avg NLL over non-ignored tokens\n            num_real = (labels != -100).sum().item()\n            if num_real > 0:\n                batch_nll = out.loss.item() * num_real\n                total_nll    += batch_nll\n                total_tokens += num_real\n\n    avg_loss  = total_nll / total_tokens\n    perplexity = math.exp(avg_loss)\n    print(f\"     Eval complete: loss={avg_loss:.4f}, ppl={perplexity:.2f}\")\n\n    # Cleanup\n    del model\n    clear_quant_path(quant_path)\n\n    return avg_loss, perplexity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:50:53.076831Z","iopub.execute_input":"2025-06-25T18:50:53.077159Z","iopub.status.idle":"2025-06-25T18:50:53.088433Z","shell.execute_reply.started":"2025-06-25T18:50:53.077100Z","shell.execute_reply":"2025-06-25T18:50:53.087634Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from huggingface_hub import login\nfrom getpass import getpass\n\nHF_TOKEN = getpass(\"Enter your Hugging Face token: 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\")\nlogin(token=HF_TOKEN)\n\nprint(\"Hugging Face login successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:50:57.757084Z","iopub.execute_input":"2025-06-25T18:50:57.757633Z","iopub.status.idle":"2025-06-25T18:51:05.737962Z","shell.execute_reply.started":"2025-06-25T18:50:57.757611Z","shell.execute_reply":"2025-06-25T18:51:05.737225Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your Hugging Face token: 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 ········\n"},{"name":"stdout","text":"Hugging Face login successful!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"calib_texts = dataset_splits[\"calibration\"][\"text\"]\neval_texts  = [t for t in dataset_splits[\"validation\"][\"text\"] if t.strip()][:3000]\n\n# --- init tokenizer & pre-tokenize calibration set ---\nmodel_id = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\nmax_len = tokenizer.model_max_length\nmax_len = 4096 \ncalib_tokenized = tokenizer(\n    calib_texts,\n    truncation=True,\n    padding=\"longest\",\n    max_length=max_len,\n    return_tensors=\"pt\"\n)\n\nbase_quant_path = \"/kaggle/working/llama2-7b-quant\"\nbeta_values     = [0.2, 0.4, 0.6, 0.8, 1.0]\ntau_values      = [0.5, 1.0, 1.5, 2.0]\nresults = []\ntotal_iters = len(beta_values) + len(tau_values)\niter_count = 0\n\n\nfor beta in beta_values:\n    iter_count += 1\n    print(f\"[Iter {iter_count}/{total_iters}] β={beta}, τ=0.5\")\n    qp = f\"{base_quant_path}-b{beta}-t0.5\"\n    loss, ppl = quantize_and_eval(\n        model_id=model_id,\n        calib_tokenized=calib_tokenized,\n        eval_texts=eval_texts,\n        beta=beta,\n        tau=0.5,\n        quant_path=qp,\n        tokenizer=tokenizer,      # ← pass it here\n        max_len=max_len,          # ← and here\n        batch_size=8\n    )\n    results.append({\"beta\": beta, \"tau\": 0.5, \"loss\": loss, \"ppl\": ppl})\n\n# … select best_beta …\n\nfor tau in tau_values:\n    iter_count += 1\n    print(f\"[Iter {iter_count}/{total_iters}] β={best_beta}, τ={tau}\")\n    qp = f\"{base_quant_path}-b{best_beta}-t{tau}\"\n    loss, ppl = quantize_and_eval(\n        model_id=model_id,\n        calib_tokenized=calib_tokenized,\n        eval_texts=eval_texts,\n        beta=best_beta,\n        tau=tau,\n        quant_path=qp,\n        tokenizer=tokenizer,      # ← and here too\n        max_len=max_len,\n        batch_size=8\n    )\n    results.append({\"beta\": best_beta, \"tau\": tau, \"loss\": loss, \"ppl\": ppl})\n\ndf = pd.DataFrame(results)\nprint(df.to_markdown(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:51:19.714447Z","iopub.execute_input":"2025-06-25T18:51:19.715014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc07ad1d5ddd4446a4b66c20a4b1d75d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3118488cb504c1db13f1cfbd38a7db6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9ee63dd9d045d881d35a55c3c7def3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5423c0e659f24c459877db8986e1350b"}},"metadata":{}},{"name":"stdout","text":"[Iter 1/9] β=0.2, τ=0.5\n  -> [Quantize] beta=0.2, tau=0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e641529538486e8aa9d17171fdd7f3"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4 bpw, based on [bits: 4, group_size: -1]      \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5008dcf5919490a8ae9596de1c1cf02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE.txt:   0%|          | 0.00/7.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a139788e734510a4f3749997efb6fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/22.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aae7d7231ff43b191222e72828e6b10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706912ca8c494bb496c87252a993001b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"USE_POLICY.md:   0%|          | 0.00/4.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d383bb7b114ec6971a2e5abc1c0b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Responsible-Use-Guide.pdf:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e963ebb5516f4ec6a64e668eb5654816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1f36abeda540fda9aa7625ede3ab16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf25dbffda5487c926aa767e03eea81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccecbd523658450983a7ed332735bd31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbdc1b1db9224f0481ec7420454d6331"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                                         \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309718d009d342c28d5911608b98fdbb"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n\u001b[32mINFO\u001b[0m  Packing Kernel: selected: `TorchQuantLinear`                                                 \n\u001b[32mINFO\u001b[0m  Hooked Modules: Using tree based config for accurate targeting of modules                    \n\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_scholarch_time_06_25_2025_18h_54m_22s.log`\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram               |\n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[92m0.0000000002\u001b[0m | 571000      | 0.01000     | 2.105     | 42.287       | 434.81MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.499     | 42.287       | 434.81MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[92m0.0000000002\u001b[0m | 571000      | 0.01000     | 1.516     | 42.287       | 434.81MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.454     | 16.370       | 434.81MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.679     | 50.159       | 434.81MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.601     | 50.159       | 434.81MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:27 / 1:50:24 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.01000. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 24 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.01250                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.01250. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 30 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.01500                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.01500. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 38 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.01750                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.01750. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 47 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.02000                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.02000. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 57 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.02250                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.02250. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 72 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.02500                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.02500. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 73 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.02750                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:28 / 1:50:56 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.02750. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 91 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.03000                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:29 / 1:51:28 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.03000. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 119 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.03250                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:29 / 1:51:28 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.03250. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 151 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.03500                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:29 / 1:51:28 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.03500. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 201 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.03750                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:29 / 1:51:28 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.03750. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 269 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.04000                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:29 / 1:51:28 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.04000. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 399 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.04250                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:29 / 1:51:28 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.04250. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 762 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.04500                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:30 / 1:52:00 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.04500. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1791 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.04750                                                                   \nQuantizing mlp.down_proj in layer     [0 of 31] --------------------| 0:03:30 / 1:52:00 [1/32] 3.1%","output_type":"stream"},{"name":"stderr","text":"Hessian inverse failed for damp=0.04750. Reason: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 4986 is not positive-definite).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Increasing damp to 0.05000                                                                   \n\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.05000     | 8.133     | 86.096       | 434.81MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[92m0.0000000015\u001b[0m | 571000      | 0.01000     | 1.483     | 45.569       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 1.476     | 45.569       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[92m0.0000000014\u001b[0m | 571000      | 0.01000     | 1.491     | 45.569       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.481     | 16.960       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 1.664     | 52.709       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 1.664     | 52.709       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[92m0.0000569153\u001b[0m | 571000      | 0.01000     | 5.520     | 89.980       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[92m0.0000000025\u001b[0m | 571000      | 0.01000     | 1.468     | 46.419       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.0000000004\u001b[0m | 571000      | 0.01000     | 1.721     | 46.419       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[92m0.0000000039\u001b[0m | 571000      | 0.01000     | 1.547     | 46.419       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.541     | 16.931       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[92m0.0000000006\u001b[0m | 571000      | 0.01000     | 1.613     | 52.579       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[92m0.0000000005\u001b[0m | 571000      | 0.01000     | 1.603     | 52.579       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 5.539     | 92.482       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[92m0.0000000181\u001b[0m | 571000      | 0.01000     | 1.506     | 46.850       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.0000000040\u001b[0m | 571000      | 0.01000     | 1.557     | 46.850       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[92m0.0000000293\u001b[0m | 571000      | 0.01000     | 1.548     | 46.850       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.511     | 17.101       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[92m0.0000000019\u001b[0m | 571000      | 0.01000     | 1.696     | 53.392       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[92m0.0000000015\u001b[0m | 571000      | 0.01000     | 1.612     | 53.392       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 5.606     | 92.565       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[92m0.0000000226\u001b[0m | 571000      | 0.01000     | 1.473     | 46.719       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.0000000048\u001b[0m | 571000      | 0.01000     | 1.497     | 46.719       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[92m0.0000000443\u001b[0m | 571000      | 0.01000     | 1.508     | 46.719       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.482     | 16.958       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[92m0.0000000047\u001b[0m | 571000      | 0.01000     | 1.636     | 53.271       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[92m0.0000000037\u001b[0m | 571000      | 0.01000     | 1.599     | 53.271       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 5.537     | 92.582       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[92m0.0000000222\u001b[0m | 571000      | 0.01000     | 1.498     | 46.681       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.0000000059\u001b[0m | 571000      | 0.01000     | 1.492     | 46.681       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[92m0.0000000320\u001b[0m | 571000      | 0.01000     | 1.471     | 46.681       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.498     | 16.976       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[92m0.0000000062\u001b[0m | 571000      | 0.01000     | 1.625     | 52.886       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[92m0.0000000049\u001b[0m | 571000      | 0.01000     | 1.591     | 52.886       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 5.489     | 92.630       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[92m0.0000000500\u001b[0m | 571000      | 0.01000     | 1.472     | 46.542       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.0000000142\u001b[0m | 571000      | 0.01000     | 1.541     | 46.542       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[92m0.0000000790\u001b[0m | 571000      | 0.01000     | 1.505     | 46.542       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.484     | 17.017       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[92m0.0000000094\u001b[0m | 571000      | 0.01000     | 1.604     | 52.764       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[92m0.0000000071\u001b[0m | 571000      | 0.01000     | 1.696     | 52.764       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 5.546     | 92.577       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[92m0.0000000636\u001b[0m | 571000      | 0.01000     | 1.442     | 46.486       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.0000000190\u001b[0m | 571000      | 0.01000     | 1.496     | 46.486       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[92m0.0000000872\u001b[0m | 571000      | 0.01000     | 1.541     | 46.486       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.474     | 17.071       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[92m0.0000000121\u001b[0m | 571000      | 0.01000     | 1.604     | 52.924       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[92m0.0000000093\u001b[0m | 571000      | 0.01000     | 1.606     | 52.924       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 5.552     | 91.958       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[92m0.0000000693\u001b[0m | 571000      | 0.01000     | 1.494     | 46.570       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.0000000195\u001b[0m | 571000      | 0.01000     | 1.501     | 46.570       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[92m0.0000000968\u001b[0m | 571000      | 0.01000     | 1.481     | 46.570       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.477     | 17.017       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[92m0.0000000134\u001b[0m | 571000      | 0.01000     | 1.626     | 53.436       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[92m0.0000000109\u001b[0m | 571000      | 0.01000     | 1.597     | 53.436       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.0000000006\u001b[0m | 571000      | 0.01000     | 5.522     | 92.610       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[92m0.0000000854\u001b[0m | 571000      | 0.01000     | 1.448     | 46.635       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.0000000259\u001b[0m | 571000      | 0.01000     | 1.476     | 46.635       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[92m0.0000001106\u001b[0m | 571000      | 0.01000     | 1.446     | 46.635       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.0000000000\u001b[0m | 571000      | 0.01000     | 1.454     | 17.011       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[92m0.0000000153\u001b[0m | 571000      | 0.01000     | 1.642     | 53.639       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[92m0.0000000128\u001b[0m | 571000      | 0.01000     | 1.597     | 53.639       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 5.646     | 92.697       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[92m0.0000001221\u001b[0m | 571000      | 0.01000     | 1.497     | 46.929       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.0000000400\u001b[0m | 571000      | 0.01000     | 1.496     | 46.929       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[92m0.0000001558\u001b[0m | 571000      | 0.01000     | 1.511     | 46.929       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 1.475     | 16.983       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[92m0.0000000154\u001b[0m | 571000      | 0.01000     | 1.627     | 53.599       | 4895.75MB, 394.29MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.gate_proj                                               \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[92m0.0000000132\u001b[0m | 571000      | 0.01000     | 1.606     | 53.599       | 4895.75MB, 94.29MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.up_proj                                                 \n\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 5.602     | 92.554       | 4895.75MB, 94.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer mlp.down_proj                                               \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram                |\n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[92m0.0000001299\u001b[0m | 571000      | 0.01000     | 1.456     | 46.377       | 4895.75MB, 424.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.k_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.0000000504\u001b[0m | 571000      | 0.01000     | 1.518     | 46.377       | 4895.75MB, 232.19MB     | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.v_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[92m0.0000001682\u001b[0m | 571000      | 0.01000     | 1.488     | 46.377       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.q_proj                                            \n\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.0000000001\u001b[0m | 571000      | 0.01000     | 1.480     | 16.972       | 4895.75MB, 40.19MB      | \n\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------------------------------\n\u001b[36mDEBUG\u001b[0m Freeing resources for GPTQ layer self_attn.o_proj                                            \nQuantizing mlp.gate_proj in layer     [11 of 31] ██████-----------| 0:45:58 / 2:02:34 [12/32] 37.5%","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Plotting","metadata":{}},{"cell_type":"code","source":"df2 = pd.DataFrame(results2)\n\n# Constants\nconst_tau = 0.5\ndf_beta = df[df['tau'] == const_tau].reset_index(drop=True)\n\nbest_beta = df_beta.loc[df_beta['ppl'].idxmin(), 'beta']\ndf_tau = df[df['beta'] == best_beta].reset_index(drop=True)\n\ndef plot_zoomed_bar(x, y, xlabel, ylabel, title, cmap):\n    colors = cmap(np.linspace(0, 1, len(x)))\n    fig, ax = plt.subplots(figsize=(8, 4))\n    bars = ax.bar(x, y, color=colors, edgecolor='black', linewidth=0.8)\n\n    ax.set_title(title, fontsize=14)\n    ax.set_xlabel(xlabel, fontsize=12)\n    ax.set_ylabel(ylabel, fontsize=12)\n    ax.grid(axis='y', linestyle='--', alpha=0.6)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    y_min, y_max = y.min(), y.max()\n    margin = (y_max - y_min) * 0.15\n    ax.set_ylim(y_min - margin, y_max + margin)\n    \n    plt.tight_layout()\n\nplot_zoomed_bar(\n    x=df_beta['beta'].astype(str),\n    y=df_beta['ppl'],\n    xlabel='β (Beta values)',\n    ylabel='Perplexity',\n    title='Perplexity vs Beta @ τ = 0.5',\n    cmap=plt.cm.Set2\n)\n\nplot_zoomed_bar(\n    x=df_beta['beta'].astype(str),\n    y=df_beta['loss'],\n    xlabel='β (Beta values)',\n    ylabel='Avg NLL Loss',\n    title='Loss vs Beta @ τ = 0.5',\n    cmap=plt.cm.Pastel1\n)\n\nplot_zoomed_bar(\n    x=df_tau['tau'].astype(str),\n    y=df_tau['ppl'],\n    xlabel='τ (Tau values)',\n    ylabel='Perplexity',\n    title=f'Perplexity vs Tau @ β = {best_beta}',\n    cmap=plt.cm.Pastel2\n)\n\nplot_zoomed_bar(\n    x=df_tau['tau'].astype(str),\n    y=df_tau['loss'],\n    xlabel='τ (Tau values)',\n    ylabel='Avg NLL Loss',\n    title=f'Loss vs Tau @ β = {best_beta}',\n    cmap=plt.cm.Dark2\n)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}