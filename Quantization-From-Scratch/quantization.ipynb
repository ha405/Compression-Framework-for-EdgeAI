{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoProcessor, AutoModelForVision2Seq, AutoConfig\nfrom datasets import load_dataset\nfrom PIL import Image\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport gc\nimport math\nimport time\nimport os\nimport pandas as pd\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# quantized_model_dir = '/kaggle/working/quantized_model_8bit'\n# if os.path.exists(quantized_model_dir):\n#     shutil.rmtree(quantized_model_dir)\n#     print(f\"Removed directory: {quantized_model_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = '/kaggle/input/flickr30k'\nimage_folder_name = 'flickr30k_images'\ncaptions_file_name = 'captions.txt'\ncalibration_output_dir = '/kaggle/working/flickr30k_calibration'\n\nimage_folder = os.path.join(base_path, image_folder_name)\ncaptions_file = os.path.join(base_path, captions_file_name)\n\ndf_captions = pd.read_csv(\n    captions_file,\n    delimiter=',',\n    header=None,\n    names=['image_name', 'caption_index', 'caption']\n)\n\ndf_captions['caption'] = df_captions['caption'].astype(str).str.strip()\ncaptions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\nimage_paths = {}\nfor img_name in captions_dict.keys():\n    full_path = os.path.join(image_folder, img_name)\n    if os.path.exists(full_path):\n        image_paths[img_name] = full_path\n\navailable_image_names = list(image_paths.keys())\nsample_size = min(500, len(available_image_names))\nsample_image_names = random.sample(available_image_names, sample_size)\n\nos.makedirs(calibration_output_dir, exist_ok=True)\nfor img_name in sample_image_names:\n    src_path = image_paths[img_name]\n    dst_path = os.path.join(calibration_output_dir, img_name)\n    shutil.copy(src_path, dst_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\n\nmodule_dir = '/kaggle/input/gptypys/'\n\nif module_dir not in sys.path:\n    sys.path.insert(0, module_dir)\n\nimport gptq\nimport quant","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\nimport os\nimport random\n\n# Disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set the device and use 16-bit precision\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nMODEL_ID = \"microsoft/Phi-3.5-vision-instruct\"\nbase_path = '/kaggle/input/flickr30k'\ncaptions_file_name = 'captions.txt'\ncalibration_output_dir = '/kaggle/working/flickr30k_calibration'\ncaptions_file = os.path.join(base_path, captions_file_name)\n\n# Prepare single input function\ndef prepare_single_input_phi3(img_path, caption_text, processor):\n    if not os.path.exists(img_path):\n        print(f\"Warning: Image path not found: {img_path}\")\n        return None\n    image = Image.open(img_path).convert(\"RGB\")\n\n    placeholder = \"<|image_1|>\\n\"\n    user_content = placeholder + str(caption_text).strip()\n\n    messages = [{\"role\": \"user\", \"content\": user_content}]\n\n    prompt = processor.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    inputs = processor(prompt, [image], return_tensors=\"pt\")\n    return inputs\n\n# Load the model and processor\nprint(f\"Loading model: {MODEL_ID}\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch_dtype,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    _attn_implementation='eager'  \n)\n\nprocessor = AutoProcessor.from_pretrained(\n    MODEL_ID,\n    trust_remote_code=True \n)\n\nmodel.eval()\n\n# Load captions data\ndf_captions = pd.read_csv(captions_file, delimiter=',', header=None, names=['image_name', 'caption_index', 'caption'])\ndf_captions['caption'] = df_captions['caption'].astype(str).str.strip()\ncaptions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\n\n# Select random image with corresponding caption\nimage_dir = calibration_output_dir\ncalib_image_names = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\ntest_img_name = random.choice(calib_image_names)\ntest_img_path = os.path.join(image_dir, test_img_name)\ntest_captions = captions_dict.get(test_img_name, [\"Describe the image.\"])\ntest_caption = test_captions[0]\n\nprint(f\"Selected sample: Image='{test_img_name}', Caption='{test_caption}'\")\n\n# Prepare input\ninputs = prepare_single_input_phi3(test_img_path, test_caption, processor)\ninputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n\n# Run inference\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\nwith torch.no_grad():\n    generate_ids = model.generate(\n        **inputs,\n        eos_token_id=processor.tokenizer.eos_token_id,\n        **generation_args\n    )\n\n    input_token_len = inputs['input_ids'].shape[1]\n    generate_ids = generate_ids[:, input_token_len:]\n\n    response = processor.batch_decode(\n        generate_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False\n    )[0]\n\n    print(\"\\nInference successful!\")\n    print(\"\\nDecoded Output:\", response)\n\nprint(\"\\n--- Single Instance Check Complete ---\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nimport time\nimport math\nimport gc\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nimport quant\nimport gptq\n\n# === Assume model, processor, device, torch_dtype are loaded ===\nmodel.eval()\ndevice = next(model.parameters()).device\n\nQUANT_BITS = 8\nQUANT_PERCHANNEL = True\nQUANT_SYMMETRIC = True\n\ncalibration_output_dir = '/kaggle/working/flickr30k_calibration'\ncaptions_file = '/kaggle/input/flickr30k/captions.txt'\n\nif not os.path.isdir(calibration_output_dir):\n    raise FileNotFoundError(f\"Calibration directory not found: {calibration_output_dir}\")\nif not os.path.isfile(captions_file):\n    raise FileNotFoundError(f\"Captions file not found: {captions_file}\")\n\ndf_captions = pd.read_csv(\n    captions_file,\n    delimiter=',',\n    header=None,\n    names=['image_name', 'caption_index', 'caption']\n)\ndf_captions['caption'] = df_captions['caption'].astype(str).str.strip()\ncaptions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\n\ncalib_image_names = [f for f in os.listdir(calibration_output_dir)\n                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\nif not calib_image_names:\n    raise ValueError(f\"No images found in calibration directory: {calibration_output_dir}\")\n\nn_calib_samples = min(100, len(calib_image_names))\ncalib_image_names = random.sample(calib_image_names, n_calib_samples)\n\n\ndef prepare_single_input(img_path, caption_text, processor):\n    if not os.path.exists(img_path):\n        print(f\"Warning: {img_path} not found\")\n        return None\n    try:\n        image = Image.open(img_path).convert(\"RGB\")\n        placeholder = \"<|image_1|>\\n\"\n        user_content = placeholder + caption_text.strip()\n        messages = [{\"role\": \"user\", \"content\": user_content}]\n        prompt = processor.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        inputs = processor(prompt, [image], return_tensors=\"pt\")\n        return inputs\n    except Exception as e:\n        print(f\"Error preparing input for {img_path}: {e}\")\n        return None\n\n\ngptq_modules = {}\nmodule_names = {}\n\ndef is_target_module(module):\n    return isinstance(module, nn.Linear) or (hasattr(module, '__class__') and module.__class__.__name__ == \"Conv1D\")\n\nprint(\"Identifying target modules...\")\nfor name, module in model.named_modules():\n    if is_target_module(module):\n        gptq_inst = gptq.GPTQ(module)\n        gptq_inst.quantizer = quant.Quantizer()\n        gptq_inst.quantizer.configure(\n            bits=QUANT_BITS,\n            perchannel=QUANT_PERCHANNEL,\n            sym=QUANT_SYMMETRIC\n        )\n        gptq_modules[module] = gptq_inst\n        module_names[module] = name\n\nprint(f\"Found {len(gptq_modules)} target modules.\")\n\nhook_handles = []\ndef make_hook(module, gptq_inst):\n    def hook(module, inp, out):\n        in_tensor = inp[0].detach() if isinstance(inp, tuple) else inp.detach()\n        out_tensor = out[0].detach() if isinstance(out, tuple) else out.detach()\n        try:\n            gptq_inst.add_batch(in_tensor, out_tensor)\n        except Exception as e:\n            print(f\"Error in hook for {module_names.get(module, type(module).__name__)} during add_batch: {e}\")\n    return hook\n\nprint(\"Registering forward hooks...\")\nfor module, gptq_inst in gptq_modules.items():\n    handle = module.register_forward_hook(make_hook(module, gptq_inst))\n    hook_handles.append(handle)\n\nprint(f\"Starting calibration with {n_calib_samples} samples...\")\nmodel.eval()\nprocessed_count = 0\nwith torch.no_grad():\n    for i, img_name in enumerate(calib_image_names):\n        cap_list = captions_dict.get(img_name, [\"Describe the image.\"])\n        caption = random.choice(cap_list)\n        img_path = os.path.join(calibration_output_dir, img_name)\n        inputs = prepare_single_input(img_path, caption, processor)\n        if inputs is None:\n            continue\n\n        try:\n            inputs_on_device = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n            _ = model(**inputs_on_device)\n            processed_count += 1\n            if (i + 1) % 20 == 0 or (i + 1) == n_calib_samples:\n                print(f\"  Processed calibration sample {i+1}/{n_calib_samples}: {img_name}\")\n        except Exception as e:\n            print(f\"Error during forward pass for calibration sample {img_name}: {e}\")\n\nprint(f\"Calibration finished. Processed {processed_count} samples.\")\n\nprint(\"Removing forward hooks...\")\nfor handle in hook_handles:\n    handle.remove()\nhook_handles.clear()\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Starting quantization and packing...\")\nquantized_layers_count = 0\nfailed_layers = []\n\nfor module, gptq_inst in gptq_modules.items():\n    module_name = module_names.get(module, type(module).__name__)\n    print(f\"Quantizing and packing: {module_name}\")\n    try:\n        if hasattr(gptq_inst, 'nsamples') and gptq_inst.nsamples == 0:\n            print(f\"  Skipping {module_name}: No calibration samples recorded.\")\n            failed_layers.append(module_name)\n            gptq_inst.free()\n            continue\n\n        gptq_inst.fasterquant(blocksize=128, percdamp=0.01, groupsize=-1, actorder=False)\n\n        if hasattr(gptq_inst, 'quantizer') and hasattr(gptq_inst.quantizer, 'scale') and hasattr(gptq_inst.quantizer, 'zero'):\n            scale = gptq_inst.quantizer.scale\n            zero = gptq_inst.quantizer.zero\n            maxq = gptq_inst.quantizer.maxq if hasattr(gptq_inst.quantizer, 'maxq') else (2 ** QUANT_BITS - 1)\n\n            # Retrieve the original weight parameter for deletion.\n            original_weight_param = None\n            param_name = None\n            for name, param in module.named_parameters():\n                if param is module.weight:\n                    original_weight_param = param\n                    param_name = name\n                    break\n\n            if original_weight_param is None and hasattr(module, 'weight'):\n                print(f\"  Warning: module.weight exists but is not registered parameter for {module_name}. Attempting packing.\")\n                current_weight_float = module.weight.data.clone()\n            elif original_weight_param is not None:\n                current_weight_float = original_weight_param.data.clone()\n            else:\n                print(f\"  Packing failed for {module_name}: Cannot find weight attribute/parameter after fasterquant.\")\n                failed_layers.append(module_name)\n                continue\n\n            weight_to_quantize = current_weight_float\n            if isinstance(module, transformers.Conv1D):\n                weight_to_quantize = current_weight_float.t()\n\n            # Call quantize to get the quantized indices, and force cast to int8.\n            qweight_int = quant.quantize(weight_to_quantize, scale, zero, maxq).to(torch.int8)\n\n            if isinstance(module, transformers.Conv1D):\n                qweight_int = qweight_int.t()\n\n            module.register_buffer('qweight', qweight_int.clone().to('cpu'), persistent=True)\n            module.register_buffer('scales', scale.clone().to('cpu'), persistent=True)\n            module.register_buffer('qzeros', zero.clone().to('cpu'), persistent=True)\n\n            module.quant_state = {\n                'bits': QUANT_BITS,\n                'groupsize': getattr(gptq_inst, 'groupsize', -1),\n                'sym': QUANT_SYMMETRIC,\n                'perchannel': QUANT_PERCHANNEL,\n            }\n\n            if param_name is not None:\n                del module._parameters[param_name]\n                print(f\"  Original weight parameter '{param_name}' deleted for {module_name}\")\n            else:\n                try:\n                    del module.weight\n                    print(f\"  Original weight attribute deleted for {module_name}\")\n                except AttributeError:\n                    print(f\"  Warning: Could not delete weight attribute/parameter for {module_name}\")\n\n            quantized_layers_count += 1\n\n        else:\n            print(f\"  Packing failed for {module_name}: Quantizer scale/zero not found after fasterquant.\")\n            failed_layers.append(module_name)\n\n    except Exception as e:\n        print(f\"  Error during fasterquant or packing for {module_name}: {e}\")\n        import traceback\n        traceback.print_exc()\n        failed_layers.append(module_name)\n    finally:\n        gptq_inst.free()\n\nprint(\"\\nQuantization and packing complete.\")\nprint(f\"Successfully quantized and packed {quantized_layers_count} layers.\")\nif failed_layers:\n    print(f\"Failed or skipped quantization/packing for {len(failed_layers)} layers: {failed_layers}\")\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"\\nModel layers modified. Quantized layers now have 'qweight', 'scales', 'qzeros' registered as buffers.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport gc\nimport transformers\n\nQUANT_BITS = 8\nQUANT_PERCHANNEL = True\nQUANT_SYMMETRIC = True\n\nsave_dir = '/kaggle/working/quantized_model_8bit'\nos.makedirs(save_dir, exist_ok=True)\nstate_dict = model.state_dict()\nstate_dict_path = os.path.join(save_dir, 'pytorch_model_quantized.bin')\ntorch.save(state_dict, state_dict_path)\nprint(f\"Quantized model state dictionary saved to: {state_dict_path}\")\ntry:\n    if hasattr(model, 'config'):\n        model.config.save_pretrained(save_dir)\n        print(f\"Model config saved to: {save_dir}\")\n    else:\n        print(\"Warning: 'model.config' not found. Configuration not saved automatically.\")\nexcept Exception as e:\n    print(f\"Warning: Error saving model config using save_pretrained: {e}\")\ntry:\n    if processor is not None:\n        default_chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}<|user|>\\n{{ message['content'] }}<|end|>\\n{% elif message['role'] == 'assistant' %}<|assistant|>\\n{{ message['content'] }}<|end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>\\n{% endif %}\"\n        has_template = False\n        if hasattr(processor, 'tokenizer') and processor.tokenizer is not None:\n             if getattr(processor.tokenizer, 'chat_template', None) is None:\n                  print(\"Processor tokenizer missing 'chat_template', attempting to set default.\")\n                  try:\n                      processor.tokenizer.chat_template = default_chat_template\n                      has_template = True\n                  except Exception as e_set:\n                      print(f\"  Warning: Failed to set default chat_template: {e_set}\")\n             else:\n                  has_template = True\n                  print(\"Processor tokenizer already has a 'chat_template'.\")\n\n        processor.save_pretrained(save_dir)\n        print(f\"Processor files saved to: {save_dir}\")\n        if not has_template:\n            print(\"  Warning: Processor was saved, but chat_template might still be missing/incorrect.\")\n    else:\n        print(\"Warning: 'processor' is None. Processor files not saved.\")\n\nexcept AttributeError as e_attr:\n     print(f\"Warning: AttributeError saving processor: {e_attr}\")\n     print(\"  Processor files might be incomplete in the save directory.\")\nexcept Exception as e:\n     print(f\"Warning: General error saving processor using save_pretrained: {e}\")\n     print(\"  Processor files might be incomplete in the save directory.\")\n\n\ndel state_dict\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\nprint(\"\\nSaving process finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom PIL import Image\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nimport warnings\nimport gc\nimport shutil\n\nwarnings.filterwarnings('ignore')\n\nMODEL_ID = \"microsoft/Phi-3.5-vision-instruct\"\nLOAD_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLOAD_DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\nload_dir = '/kaggle/working/quantized_model_8bit'\nquantized_ckpt_filename = 'pytorch_model.bin' # Use the actual saved name\nstate_dict_path = os.path.join(load_dir, quantized_ckpt_filename)\n\nif not os.path.exists(state_dict_path):\n    raise FileNotFoundError(f\"Quantized checkpoint not found: {state_dict_path}\")\n\nprint(f\"Loading base model architecture from: {MODEL_ID}\")\nconfig = transformers.AutoConfig.from_pretrained(load_dir, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(load_dir, trust_remote_code=True)\n\n# Load the base architecture\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    trust_remote_code=True,\n    torch_dtype=LOAD_DTYPE,\n    _attn_implementation='eager'\n)\nprint(\"Base model architecture loaded.\")\n\n# Load the state dict containing qweight etc. using strict=False\nprint(f\"Loading custom quantized state_dict from: {state_dict_path}\")\nquantized_state_dict = torch.load(state_dict_path, map_location='cpu')\nload_result = base_model.load_state_dict(quantized_state_dict, strict=False)\nprint(\"State dict loading results:\")\nprint(f\"  Missing keys: {load_result.missing_keys}\")\nprint(f\"  Unexpected keys: {load_result.unexpected_keys}\") # Should be empty if buffers were registered\ndel quantized_state_dict\ngc.collect()\n\nbase_model.to(LOAD_DEVICE)\nbase_model.eval()\nprint(f\"Model loaded onto device: {LOAD_DEVICE}\")\n\n# --- Necessary Dequantization and Patching Logic ---\ndef dequantize_weight(module, target_device):\n    if not hasattr(module, 'qweight') or not hasattr(module, 'scales') or not hasattr(module, 'qzeros'):\n         raise AttributeError(f\"Quantized module {module} is missing qweight, scales, or qzeros buffers.\")\n    qweight = module.qweight.to(target_device)\n    scales = module.scales.to(target_device)\n    qzeros = module.qzeros.to(target_device)\n    qzeros_float = qzeros.float()\n    if scales.dim() == qweight.dim():\n        dq_weight = (qweight.float() - qzeros_float) * scales\n    elif scales.dim() == 1 and qzeros.dim() == 1 and scales.shape[0] == qweight.shape[0]:\n        dq_weight = (qweight.float() - qzeros_float.unsqueeze(1)) * scales.unsqueeze(1)\n    elif scales.numel() == 1 and qzeros.numel() == 1:\n        dq_weight = (qweight.float() - qzeros_float) * scales\n    else:\n        raise ValueError(f\"Unhandled scale/zero shape for module {module}. Weight:{qweight.shape}, Scale:{scales.shape}, Zero:{qzeros.shape}.\")\n    return dq_weight.to(base_model.dtype)\n\ndef quantized_layer_forward(module_instance, input_tensor, *args, **kwargs):\n    target_device = input_tensor.device\n    dequantized_weight = dequantize_weight(module_instance, target_device)\n    bias = getattr(module_instance, 'bias', None)\n    if bias is not None:\n        bias = bias.to(target_device)\n    output = F.linear(input_tensor, dequantized_weight, bias)\n    return output\n\ndef patch_model_for_inference(model_to_patch):\n    patched_count = 0\n    for name, module in model_to_patch.named_modules():\n        if hasattr(module, 'qweight') and hasattr(module, 'scales') and hasattr(module, 'qzeros'):\n            is_weight_missing = any(mkey.startswith(name) and mkey.endswith('.weight') for mkey in load_result.missing_keys)\n            if is_weight_missing:\n                 print(f\"  Weight correctly missing for {name}. Patching forward method.\")\n            elif hasattr(module, 'weight'):\n                 print(f\"  WARNING: Module {name} has qweight AND weight. Patching anyway.\")\n            else:\n                 print(f\"  Patching forward method for: {name} (Weight status uncertain)\")\n            module.forward = lambda *args, module=module, **kwargs: quantized_layer_forward(module, *args, **kwargs)\n            patched_count += 1\n    print(f\"Patched {patched_count} quantized layers.\")\n\nprint(\"\\nPatching model for on-the-fly dequantization inference...\")\npatch_model_for_inference(base_model)\n# --- End of Necessary Logic ---\n\n\ndef prepare_single_input_phi3(img_path, caption_text, processor):\n    if not os.path.exists(img_path):\n        return None\n    image = Image.open(img_path).convert(\"RGB\")\n    placeholder = \"<|image_1|>\\n\"\n    user_content = placeholder + str(caption_text).strip()\n    messages = [{\"role\": \"user\", \"content\": user_content}]\n    prompt = processor.tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = processor(prompt, [image], return_tensors=\"pt\")\n    return inputs\n\nbase_path = '/kaggle/input/flickr30k'\ncaptions_file = os.path.join(base_path, 'captions.txt')\ncalibration_output_dir = '/kaggle/working/flickr30k_calibration'\n\ntry:\n    df_captions = pd.read_csv(captions_file, delimiter=',', header=None,\n                              names=['image_name', 'caption_index', 'caption'])\n    df_captions['caption'] = df_captions['caption'].astype(str).str.strip()\n    captions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\n\n    calib_image_names = [f for f in os.listdir(calibration_output_dir)\n                         if f.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.isfile(os.path.join(calibration_output_dir, f))]\n    if not calib_image_names:\n        raise ValueError(\"No images found in calibration directory.\")\n\n    test_img_name = random.choice(calib_image_names)\n    test_img_path = os.path.join(calibration_output_dir, test_img_name)\n    test_captions = captions_dict.get(test_img_name, [\"Describe the image.\"])\n    test_caption = test_captions[0]\n\n    print(f\"\\nRunning inference for sample: Image='{test_img_name}', Caption='{test_caption}'\")\n\n    inputs = prepare_single_input_phi3(test_img_path, test_caption, processor)\n    if inputs is None:\n        raise ValueError(\"Input preparation failed.\")\n\n    inputs = {k: v.to(LOAD_DEVICE, non_blocking=True) for k, v in inputs.items()}\n\n    generation_args = {\n        \"max_new_tokens\": 500,\n        \"temperature\": 0.0,\n        \"do_sample\": False,\n    }\n\n    print(\"Generating output...\")\n    with torch.no_grad():\n        eos_token_id = processor.tokenizer.eos_token_id\n        if isinstance(eos_token_id, list):\n            eos_token_id = eos_token_id[0]\n\n        generate_ids = base_model.generate( # Use the patched base_model\n            **inputs,\n            eos_token_id=eos_token_id,\n            **generation_args\n        )\n\n        input_token_len = inputs['input_ids'].shape[1]\n        generate_ids = generate_ids[:, input_token_len:]\n        response = processor.tokenizer.decode(\n            generate_ids[0],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=False\n        )\n\n    print(\"\\nInference successful!\")\n    print(\"\\nDecoded Output:\", response)\n\nexcept FileNotFoundError as e:\n    print(f\"Error: Required file or directory not found: {e}\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\nexcept Exception as e:\n    print(f\"\\nAn unexpected error occurred during example inference: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n--- Single Instance Check Complete ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import get_peft_model_state_dict\n\nfor name, param in base_model.named_parameters():\n    if hasattr(param, 'dtype'):\n        print(f\"{name}: {param.dtype}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}