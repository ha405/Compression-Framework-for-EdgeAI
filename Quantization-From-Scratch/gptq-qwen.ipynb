{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11425261,"sourceType":"datasetVersion","datasetId":7155573}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/gptqmain/GPTQModel-main')\n# !pip install -r /kaggle/input/gptqmain/GPTQModel-main/requirements.txt\n\nfrom datasets import load_dataset\nfrom gptqmodel import GPTQModel, QuantizeConfig\n\nimport torch\nimport torchvision\nimport torchvision.datasets as datasets\nfrom torchvision import transforms\n\n# Model configuration\nmodel_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\nquant_path = \"Qwen/Qwen2.5-VL-3B-Instruct-voc-text-calibrated\"\naccess_token = \"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\"\n\nquant_config = QuantizeConfig(bits=8, group_size=128)\n\n# VOC dataset configuration\nVOC_ROOT = './voc_data'\nVOC_YEAR = '2007'\nVOC_IMAGE_SET = 'test'\nDOWNLOAD_VOC = True\n\n# Load VOC dataset\ntry:\n    voc_dataset = datasets.VOCDetection(\n        root=VOC_ROOT,\n        year=VOC_YEAR,\n        image_set=VOC_IMAGE_SET,\n        download=DOWNLOAD_VOC,\n        transform=None\n    )\n    print(f\"VOC Detection dataset loaded successfully from {VOC_ROOT}, year {VOC_YEAR}, image set {VOC_IMAGE_SET}. Found {len(voc_dataset)} images.\")\nexcept Exception as e:\n    print(f\"Error loading VOC dataset: {e}\")\n    print(f\"Please ensure the VOC dataset can be downloaded to '{VOC_ROOT}' or is already present there. Set DOWNLOAD_VOC=True to attempt download.\")\n    raise\n\n# Prepare calibration datasets\ncalibration_dataset_complex = []\ncalibration_dataset_simple = []\nnum_calibration_samples = 300\nif len(voc_dataset) < num_calibration_samples:\n    num_calibration_samples = len(voc_dataset)\n    print(f\"Warning: VOC dataset has fewer images than requested calibration samples. Using {num_calibration_samples} images for calibration.\")\n\nfor i in range(num_calibration_samples):\n    _, target = voc_dataset[i]\n    boxes_info = target['annotation']['object']\n    if not isinstance(boxes_info, list):\n        boxes_info = [boxes_info]\n\n    image_class_names = []\n    for box_info in boxes_info:\n        class_name = box_info['name']\n        image_class_names.append(class_name)\n\n    calibration_text_complex_parts = [\n        \"Describe objects in this image. Objects are:\", \n        \", \".join(image_class_names), \n        \".\"\n    ]\n    calibration_text_complex = \" \".join(calibration_text_complex_parts)\n    calibration_dataset_complex.append(calibration_text_complex)\n\n    calibration_dataset_simple.extend(image_class_names)\n\n# Wrap calibration strings into dictionaries using key \"content\" (expected by the quantizer)\ncalibration_dataset_complex = [{\"content\": s} for s in calibration_dataset_complex]\ncalibration_dataset_simple = [{\"content\": s} for s in calibration_dataset_simple]\n\n# Load the GPTQ model\nmodel = GPTQModel.load(model_id, quant_config, trust_remote_code=True, token=access_token)\n\nprint(f\"Calibration dataset (complex) type: {type(calibration_dataset_complex)}\")\nif calibration_dataset_complex:\n    print(f\"First element (complex) type: {type(calibration_dataset_complex[0])} with keys {list(calibration_dataset_complex[0].keys())}\")\nelse:\n    print(\"Calibration dataset (complex) is empty!\")\n\nprint(f\"Calibration dataset (simple) type: {type(calibration_dataset_simple)}\")\nif calibration_dataset_simple:\n    print(f\"First element (simple) type: {type(calibration_dataset_simple[0])} with keys {list(calibration_dataset_simple[0].keys())}\")\nelse:\n    print(\"Calibration dataset (simple) is empty!\")\n\n# Quantize the model using the complex calibration dataset\ntry:\n    print(f\"Attempting to quantize model with {num_calibration_samples} VOC image descriptions (complex)...\")\n    model.quantize(calibration_dataset_complex, batch_size=2)\n    model.save(quant_path)\n    print(f\"GPTQ model saved to: {quant_path}\")\nexcept Exception as e:\n    print(f\"Error during quantization with VOC image descriptions (complex): {e}\")\n    print(f\"Error details: {e}\")\n\nprint(\"-\" * 50)\n\n# Quantize the model using the simple calibration dataset\ntry:\n    print(f\"Attempting to quantize model with SIMPLE VOC class names...\")\n    model.quantize(calibration_dataset_simple, batch_size=2)\n    model.save(quant_path + \"-simple\")\n    print(f\"GPTQ model saved to: {quant_path + '-simple'}\")\nexcept Exception as e:\n    print(f\"Error during quantization with SIMPLE VOC class names: {e}\")\n    print(f\"Error details: {e}\")\n\nprint(\"--- Script finished ---\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/gptqmain/GPTQModel-main')\n# !pip install -r /kaggle/input/gptqmain/GPTQModel-main/requirements.txt\n\nfrom gptqmodel import GPTQModel\n\n# --- Load the \"Quantized\" Model from Local Path ---\nquant_path = \"/kaggle/working/Qwen/Qwen2.5-VL-3B-Instruct-voc-text-calibrated\" # Path to your potentially quantized model\n\ntry:\n    model = GPTQModel.load(quant_path, device=\"cuda\") # Load from local path, let GPTQModel handle backend\n    print(f\"Quantized model loaded successfully from: {quant_path}\")\nexcept Exception as e:\n    print(f\"Error loading quantized model from: {quant_path}\")\n    print(f\"Error details: {e}\")\n    exit()\n\n# --- Run Text-Only Inference ---\ntry:\n    prompt = \"Describe a cat in detail:\"\n    result_tokens = model.generate(prompt)[0] # Generate tokens\n    output_text = model.tokenizer.decode(result_tokens) # Decode tokens to text\n\n    print(\"--- Generated Output Text ---\")\n    print(output_text)\n\nexcept Exception as e:\n    print(\"Error during text generation:\")\n    print(f\"Error details: {e}\")\n\nprint(\"--- Simplified Inference Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T23:32:33.873937Z","iopub.execute_input":"2025-04-15T23:32:33.874213Z","iopub.status.idle":"2025-04-15T23:33:00.075603Z","shell.execute_reply.started":"2025-04-15T23:32:33.874189Z","shell.execute_reply":"2025-04-15T23:33:00.074786Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"2025-04-15 23:32:39.329681: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744759959.352588     665 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744759959.359540     665 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \nfrom_quantized: adapter: None\n\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n\u001b[32mINFO\u001b[0m  The layer visual.blocks.0.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.0.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.0.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.0.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.0.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.1.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.1.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.1.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.1.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.1.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.2.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.2.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.2.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.2.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.2.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.3.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.3.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.3.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.3.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.3.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.4.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.4.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.4.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.4.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.4.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.5.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.5.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.5.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.5.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.5.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.6.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.6.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.6.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.6.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.6.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.7.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.7.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.7.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.7.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.7.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.8.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.8.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.8.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.8.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.8.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.9.attn.qkv is not quantized.                                         \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.9.attn.proj is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.9.mlp.gate_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.9.mlp.up_proj is not quantized.                                      \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.9.mlp.down_proj is not quantized.                                    \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.10.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.10.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.10.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.10.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.10.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.11.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.11.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.11.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.11.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.11.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.12.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.12.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.12.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.12.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.12.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.13.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.13.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.13.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.13.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.13.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.14.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.14.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.14.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.14.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.14.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.15.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.15.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.15.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.15.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.15.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.16.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.16.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.16.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.16.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.16.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.17.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.17.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.17.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.17.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.17.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.18.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.18.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.18.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.18.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.18.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.19.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.19.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.19.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.19.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.19.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.20.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.20.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.20.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.20.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.20.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.21.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.21.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.21.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.21.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.21.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.22.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.22.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.22.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.22.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.22.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.23.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.23.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.23.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.23.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.23.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.24.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.24.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.24.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.24.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.24.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.25.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.25.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.25.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.25.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.25.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.26.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.26.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.26.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.26.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.26.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.27.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.27.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.27.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.27.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.27.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.28.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.28.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.28.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.28.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.28.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.29.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.29.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.29.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.29.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.29.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.30.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.30.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.30.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.30.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.30.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.31.attn.qkv is not quantized.                                        \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.31.attn.proj is not quantized.                                       \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.31.mlp.gate_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.31.mlp.up_proj is not quantized.                                     \n\u001b[32mINFO\u001b[0m  The layer visual.blocks.31.mlp.down_proj is not quantized.                                   \n\u001b[32mINFO\u001b[0m  The layer visual.merger.mlp.0 is not quantized.                                              \n\u001b[32mINFO\u001b[0m  The layer visual.merger.mlp.2 is not quantized.                                              \n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.017447710037231445s                                           \n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\n\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.   \n\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"repetition_penalty\": 1.05,\n  \"temperature\": 1e-06\n}\n\n","output_type":"stream"},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \nQuantized model loaded successfully from: /kaggle/working/Qwen/Qwen2.5-VL-3B-Instruct-voc-text-calibrated\n--- Generated Output Text ---\nDescribe a cat in detail: size, color, fur texture, and any unique features.\nThe cat is a medium-sized feline\n--- Simplified Inference Script Finished ---\n","output_type":"stream"}],"execution_count":1}]}