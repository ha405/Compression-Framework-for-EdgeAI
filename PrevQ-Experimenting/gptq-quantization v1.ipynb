{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# quantized_model_dir = '/kaggle/working/phi3_vision_8bit_quantizers_wf_inspired.pt'\n",
    "# if os.path.exists(quantized_model_dir):\n",
    "#     shutil.rmtree(quantized_model_dir)\n",
    "#     print(f\"Removed directory: {quantized_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/flickr30k'\n",
    "image_folder_name = 'flickr30k_images'\n",
    "captions_file_name = 'captions.txt'\n",
    "calibration_output_dir = '/kaggle/working/flickr30k_calibration'\n",
    "\n",
    "image_folder = os.path.join(base_path, image_folder_name)\n",
    "captions_file = os.path.join(base_path, captions_file_name)\n",
    "\n",
    "df_captions = pd.read_csv(\n",
    "    captions_file,\n",
    "    delimiter=',',\n",
    "    header=None,\n",
    "    names=['image_name', 'caption_index', 'caption']\n",
    ")\n",
    "\n",
    "df_captions['caption'] = df_captions['caption'].astype(str).str.strip()\n",
    "captions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\n",
    "image_paths = {}\n",
    "for img_name in captions_dict.keys():\n",
    "    full_path = os.path.join(image_folder, img_name)\n",
    "    if os.path.exists(full_path):\n",
    "        image_paths[img_name] = full_path\n",
    "\n",
    "available_image_names = list(image_paths.keys())\n",
    "sample_size = min(500, len(available_image_names))\n",
    "sample_image_names = random.sample(available_image_names, sample_size)\n",
    "\n",
    "os.makedirs(calibration_output_dir, exist_ok=True)\n",
    "for img_name in sample_image_names:\n",
    "    src_path = image_paths[img_name]\n",
    "    dst_path = os.path.join(calibration_output_dir, img_name)\n",
    "    shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, Extension\n",
    "from torch.utils import cpp_extension\n",
    "\n",
    "setup(\n",
    "    name='quant_cuda', # The name of the module to import later\n",
    "    ext_modules=[cpp_extension.CUDAExtension(\n",
    "        'quant_cuda', # Must match the name argument above\n",
    "        ['quant_cuda.cpp', 'quant_cuda_kernel.cu'] # Source files\n",
    "    )],\n",
    "    cmdclass={'build_ext': cpp_extension.BuildExtension} # Command class for building\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile quant_cuda.cpp\n",
    "#include <torch/all.h>\n",
    "#include <torch/python.h>\n",
    "#include <c10/cuda/CUDAGuard.h>\n",
    "\n",
    "\n",
    "// Declaration for the 8-bit CUDA function (defined in .cu file)\n",
    "void vecquant8matmul_cuda(\n",
    "  torch::Tensor vec, torch::Tensor mat, torch::Tensor mul,\n",
    "  torch::Tensor scales, torch::Tensor zeros // expects uint8 zeros\n",
    ");\n",
    "\n",
    "\n",
    "// C++ Wrapper for the 8-bit CUDA function\n",
    "void vecquant8matmul(\n",
    "  torch::Tensor vec, torch::Tensor mat, torch::Tensor mul,\n",
    "  torch::Tensor scales, torch::Tensor zeros\n",
    ") {\n",
    "  // Ensure execution on the correct CUDA device\n",
    "  const at::cuda::OptionalCUDAGuard device_guard(device_of(vec));\n",
    "\n",
    "  // Input validation checks\n",
    "  TORCH_CHECK(vec.is_cuda(), \"Input vector 'vec' must be a CUDA tensor\");\n",
    "  TORCH_CHECK(mat.is_cuda(), \"Input matrix 'mat' must be a CUDA tensor\");\n",
    "  TORCH_CHECK(mul.is_cuda(), \"Output vector 'mul' must be a CUDA tensor\");\n",
    "  TORCH_CHECK(scales.is_cuda(), \"Input scales 'scales' must be a CUDA tensor\");\n",
    "  TORCH_CHECK(zeros.is_cuda(), \"Input zeros 'zeros' must be a CUDA tensor\");\n",
    "\n",
    "  TORCH_CHECK(vec.dim() >= 1, \"Input vector 'vec' must have at least 1 dimension\");\n",
    "  TORCH_CHECK(mat.dim() == 2, \"Input matrix 'mat' must have 2 dimensions\");\n",
    "  TORCH_CHECK(mul.dim() == 1, \"Output vector 'mul' must have 1 dimension\");\n",
    "  TORCH_CHECK(scales.dim() == 1 || scales.size(0) == mul.size(0), \"Scales must be 1D and match output size\");\n",
    "  TORCH_CHECK(zeros.dim() == 1 || zeros.size(0) == mul.size(0), \"Zeros must be 1D and match output size\");\n",
    "\n",
    "  TORCH_CHECK(mat.dtype() == torch::kInt32 || mat.dtype() == torch::kUInt32, \"Matrix 'mat' must be int32 or uint32\");\n",
    "  TORCH_CHECK(mat.size(1) == mul.size(0), \"Matrix columns must match output size\");\n",
    "  TORCH_CHECK(mat.size(1) == scales.size(0), \"Matrix columns must match scales size\");\n",
    "  TORCH_CHECK(mat.size(1) == zeros.size(0), \"Matrix columns must match zeros size\");\n",
    "  TORCH_CHECK(mat.size(0) * 4 == vec.size(-1), \"Packed matrix rows * 4 must match input vector size\");\n",
    "\n",
    "  // Ensure zeros tensor is uint8 as expected by the kernel\n",
    "  auto zeros_uint8 = zeros.to(torch::kUInt8);\n",
    "\n",
    "  // Call the CUDA kernel launcher\n",
    "  vecquant8matmul_cuda(vec, mat, mul, scales, zeros_uint8);\n",
    "}\n",
    "\n",
    "\n",
    "// Pybind11 Module Definition\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "  m.def(\"vecquant8matmul\", &vecquant8matmul, \"Vector 8-bit Quantized Matrix Multiplication (CUDA)\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile quant_cuda_kernel.cu\n",
    "// quant_cuda_kernel.cu\n",
    "\n",
    "#include <torch/all.h>\n",
    "#include <torch/python.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "\n",
    "template <typename scalar_t>\n",
    "__global__ void VecQuant3MatMulKernel(\n",
    "    const  scalar_t* __restrict__ vec,\n",
    "    const       int* __restrict__ mat,\n",
    "           scalar_t* __restrict__ mul,\n",
    "    const  scalar_t* __restrict__ scales,\n",
    "    const  scalar_t* __restrict__ zeros,\n",
    "    int height,\n",
    "    int width\n",
    ");\n",
    "\n",
    "__global__ void VecQuant3MatMulKernelFaster(\n",
    "    const  half2* __restrict__ vec,\n",
    "    const    int* __restrict__ mat,\n",
    "           float* __restrict__ mul,\n",
    "    const  float* __restrict__ scales,\n",
    "    const  float* __restrict__ zeros,\n",
    "    int height,\n",
    "    int width\n",
    ");\n",
    "\n",
    "template <typename scalar_t>\n",
    "__global__ void VecQuant8MatMulKernel(\n",
    "    const  scalar_t* __restrict__ vec,\n",
    "    const  uint32_t* __restrict__ mat,\n",
    "           scalar_t* __restrict__ mul,\n",
    "    const  scalar_t* __restrict__ scales,\n",
    "    const   uint8_t* __restrict__ zeros,\n",
    "    int height,\n",
    "    int width\n",
    ");\n",
    "\n",
    "\n",
    "const int BLOCKWIDTH  = 256;\n",
    "const int BLOCKHEIGHT =  24;\n",
    "const int BLOCKWIDTH_8BIT = 128; // Example block width for 8bit kernel\n",
    "\n",
    "\n",
    "void vecquant3matmul_cuda(\n",
    "  torch::Tensor vec,\n",
    "  torch::Tensor mat,\n",
    "  torch::Tensor mul,\n",
    "  torch::Tensor scales,\n",
    "  torch::Tensor zeros\n",
    ") {\n",
    "  int height = mat.size(0);\n",
    "  int width = mat.size(1);\n",
    "\n",
    "  dim3 blocks(\n",
    "    (height + BLOCKHEIGHT - 1) / BLOCKHEIGHT,\n",
    "    (width + BLOCKWIDTH - 1) / BLOCKWIDTH\n",
    "  );\n",
    "  dim3 threads(BLOCKWIDTH);\n",
    "\n",
    "  AT_DISPATCH_FLOATING_TYPES(\n",
    "    vec.type(), \"vecquant3matmul_cuda\", ([&] {\n",
    "      VecQuant3MatMulKernel<<<blocks, threads>>>(\n",
    "        vec.data_ptr<scalar_t>(), mat.data_ptr<int>(), mul.data_ptr<scalar_t>(),\n",
    "        scales.data_ptr<scalar_t>(), zeros.data_ptr<scalar_t>(),\n",
    "        height, width\n",
    "      );\n",
    "    })\n",
    "  );\n",
    "}\n",
    "\n",
    "void vecquant3matmul_faster_cuda(\n",
    "  torch::Tensor vec,\n",
    "  torch::Tensor mat,\n",
    "  torch::Tensor mul,\n",
    "  torch::Tensor scales,\n",
    "  torch::Tensor zeros\n",
    ") {\n",
    "  int height = mat.size(0);\n",
    "  int width = mat.size(1);\n",
    "\n",
    "  dim3 blocks(\n",
    "    (height + BLOCKHEIGHT - 1) / BLOCKHEIGHT,\n",
    "    (width + BLOCKWIDTH - 1) / BLOCKWIDTH\n",
    "  );\n",
    "  dim3 threads(BLOCKWIDTH);\n",
    "\n",
    "  VecQuant3MatMulKernelFaster<<<blocks, threads>>>(\n",
    "    (half2*) vec.data_ptr(),\n",
    "    mat.data_ptr<int>(),\n",
    "    mul.data_ptr<float>(),\n",
    "    scales.data_ptr<float>(),\n",
    "    zeros.data_ptr<float>(),\n",
    "    height, width\n",
    "  );\n",
    "}\n",
    "\n",
    "\n",
    "void vecquant8matmul_cuda(\n",
    "  torch::Tensor vec,\n",
    "  torch::Tensor mat,\n",
    "  torch::Tensor mul,\n",
    "  torch::Tensor scales,\n",
    "  torch::Tensor zeros\n",
    ") {\n",
    "  // mat shape: [infeatures / 4, outfeatures] -> height = infeatures / 4, width = outfeatures\n",
    "  // vec shape: [infeatures]\n",
    "  // mul shape: [outfeatures]\n",
    "  // scales shape: [outfeatures]\n",
    "  // zeros shape: [outfeatures] (uint8)\n",
    "  int packed_rows = mat.size(0);\n",
    "  int width = mat.size(1); // outfeatures\n",
    "  int height = vec.size(0); // infeatures\n",
    "\n",
    "  // Launch configuration maps threads to output columns (width)\n",
    "  dim3 blocks((width + BLOCKWIDTH_8BIT - 1) / BLOCKWIDTH_8BIT);\n",
    "  dim3 threads(BLOCKWIDTH_8BIT);\n",
    "\n",
    "  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
    "    vec.scalar_type(), \"vecquant8matmul_cuda\", ([&] {\n",
    "      VecQuant8MatMulKernel<<<blocks, threads>>>(\n",
    "        vec.data_ptr<scalar_t>(),\n",
    "        mat.data_ptr<uint32_t>(),\n",
    "        mul.data_ptr<scalar_t>(),\n",
    "        scales.data_ptr<scalar_t>(),\n",
    "        zeros.data_ptr<uint8_t>(),\n",
    "        height, // infeatures\n",
    "        width   // outfeatures\n",
    "      );\n",
    "    })\n",
    "  );\n",
    "}\n",
    "\n",
    "\n",
    "__device__ inline unsigned int as_unsigned(int i) {\n",
    "  return *reinterpret_cast<unsigned int*>(&i);\n",
    "}\n",
    "\n",
    "\n",
    "template <typename scalar_t>\n",
    "__global__ void VecQuant3MatMulKernel(\n",
    "    const  scalar_t* __restrict__ vec,\n",
    "    const       int* __restrict__ mat,\n",
    "           scalar_t* __restrict__ mul,\n",
    "    const  scalar_t* __restrict__ scales,\n",
    "    const  scalar_t* __restrict__ zeros,\n",
    "    int height,\n",
    "    int width\n",
    ") {\n",
    "  int row = BLOCKHEIGHT * blockIdx.x;\n",
    "  int col =  BLOCKWIDTH * blockIdx.y + threadIdx.x;\n",
    "\n",
    "  __shared__ scalar_t blockvec[BLOCKWIDTH];\n",
    "  if (threadIdx.x < BLOCKWIDTH) {\n",
    "      int vec_idx = (row / BLOCKHEIGHT) * BLOCKWIDTH + threadIdx.x;\n",
    "       if (vec_idx < height) { // Ensure reading within bounds of vec (height=infeatures)\n",
    "          blockvec[threadIdx.x] = vec[vec_idx];\n",
    "       } else {\n",
    "           blockvec[threadIdx.x] = 0; // Pad with zero if out of bounds\n",
    "       }\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  if (col >= width) return; // Check column bounds\n",
    "\n",
    "  scalar_t scale = scales[col];\n",
    "  scalar_t zero = zeros[col]; // Float zero point for 3bit\n",
    "\n",
    "  scalar_t res = 0;\n",
    "  int i = width * row + col; // Index into mat [height=rows(packed), width=cols(outfeatures)]\n",
    "  int k = 0;\n",
    "\n",
    "  unsigned int tmp1;\n",
    "  unsigned int tmp2;\n",
    "  unsigned int tmp;\n",
    "\n",
    "  // Assuming height is multiple of BLOCKHEIGHT for simplicity in loop structure\n",
    "  // A more robust kernel handles arbitrary height/infeatures\n",
    "  while (k < BLOCKWIDTH) {\n",
    "    tmp1 = as_unsigned(mat[i]);\n",
    "    res += (scale * scalar_t((tmp1 >>  0) & 0x7) - zero) * blockvec[k + 0];\n",
    "    res += (scale * scalar_t((tmp1 >>  3) & 0x7) - zero) * blockvec[k + 1];\n",
    "    res += (scale * scalar_t((tmp1 >>  6) & 0x7) - zero) * blockvec[k + 2];\n",
    "    res += (scale * scalar_t((tmp1 >>  9) & 0x7) - zero) * blockvec[k + 3];\n",
    "    res += (scale * scalar_t((tmp1 >> 12) & 0x7) - zero) * blockvec[k + 4];\n",
    "    res += (scale * scalar_t((tmp1 >> 15) & 0x7) - zero) * blockvec[k + 5];\n",
    "    res += (scale * scalar_t((tmp1 >> 18) & 0x7) - zero) * blockvec[k + 6];\n",
    "    res += (scale * scalar_t((tmp1 >> 21) & 0x7) - zero) * blockvec[k + 7];\n",
    "    res += (scale * scalar_t((tmp1 >> 24) & 0x7) - zero) * blockvec[k + 8];\n",
    "    res += (scale * scalar_t((tmp1 >> 27) & 0x7) - zero) * blockvec[k + 9];\n",
    "    i += width; // Move to the next packed row for this column\n",
    "    tmp2 = as_unsigned(mat[i]);\n",
    "    tmp = (tmp1 >> 30) | ((tmp2 << 2) & 0x4);\n",
    "    tmp2 >>= 1;\n",
    "    res += (scale * scalar_t(tmp) - zero) * blockvec[k + 10];\n",
    "    k += 11;\n",
    "    res += (scale * scalar_t((tmp2 >>  0) & 0x7) - zero) * blockvec[k + 0];\n",
    "    res += (scale * scalar_t((tmp2 >>  3) & 0x7) - zero) * blockvec[k + 1];\n",
    "    res += (scale * scalar_t((tmp2 >>  6) & 0x7) - zero) * blockvec[k + 2];\n",
    "    res += (scale * scalar_t((tmp2 >>  9) & 0x7) - zero) * blockvec[k + 3];\n",
    "    res += (scale * scalar_t((tmp2 >> 12) & 0x7) - zero) * blockvec[k + 4];\n",
    "    res += (scale * scalar_t((tmp2 >> 15) & 0x7) - zero) * blockvec[k + 5];\n",
    "    res += (scale * scalar_t((tmp2 >> 18) & 0x7) - zero) * blockvec[k + 6];\n",
    "    res += (scale * scalar_t((tmp2 >> 21) & 0x7) - zero) * blockvec[k + 7];\n",
    "    res += (scale * scalar_t((tmp2 >> 24) & 0x7) - zero) * blockvec[k + 8];\n",
    "    res += (scale * scalar_t((tmp2 >> 27) & 0x7) - zero) * blockvec[k + 9];\n",
    "    i += width;\n",
    "    tmp1 = as_unsigned(mat[i]);\n",
    "    tmp = (tmp2 >> 30) | ((tmp1 << 1) & 0x6);\n",
    "    tmp1 >>= 2;\n",
    "    res += (scale * scalar_t(tmp) - zero) * blockvec[k + 10];\n",
    "    k += 11;\n",
    "    res += (scale * scalar_t((tmp1 >>  0) & 0x7) - zero) * blockvec[k + 0];\n",
    "    res += (scale * scalar_t((tmp1 >>  3) & 0x7) - zero) * blockvec[k + 1];\n",
    "    res += (scale * scalar_t((tmp1 >>  6) & 0x7) - zero) * blockvec[k + 2];\n",
    "    res += (scale * scalar_t((tmp1 >>  9) & 0x7) - zero) * blockvec[k + 3];\n",
    "    res += (scale * scalar_t((tmp1 >> 12) & 0x7) - zero) * blockvec[k + 4];\n",
    "    res += (scale * scalar_t((tmp1 >> 15) & 0x7) - zero) * blockvec[k + 5];\n",
    "    res += (scale * scalar_t((tmp1 >> 18) & 0x7) - zero) * blockvec[k + 6];\n",
    "    res += (scale * scalar_t((tmp1 >> 21) & 0x7) - zero) * blockvec[k + 7];\n",
    "    res += (scale * scalar_t((tmp1 >> 24) & 0x7) - zero) * blockvec[k + 8];\n",
    "    res += (scale * scalar_t((tmp1 >> 27) & 0x7) - zero) * blockvec[k + 9];\n",
    "    i += width;\n",
    "    k += 10;\n",
    "  }\n",
    "\n",
    "  atomicAdd(&mul[col], res);\n",
    "}\n",
    "\n",
    "__global__ void VecQuant3MatMulKernelFaster(\n",
    "    const  half2* __restrict__ vec,\n",
    "    const    int* __restrict__ mat,\n",
    "           float* __restrict__ mul,\n",
    "    const  float* __restrict__ scales,\n",
    "    const  float* __restrict__ zeros,\n",
    "    int height,\n",
    "    int width\n",
    ") {\n",
    "  const int blockwidth2 = BLOCKWIDTH / 2;\n",
    "\n",
    "  int row = BLOCKHEIGHT * blockIdx.x;\n",
    "  int col = BLOCKWIDTH * blockIdx.y + threadIdx.x;\n",
    "\n",
    "   __shared__ half2 blockvec[blockwidth2];\n",
    "   if (threadIdx.x < blockwidth2) {\n",
    "       int vec_idx = (row / BLOCKHEIGHT) * blockwidth2 + threadIdx.x;\n",
    "        // Assuming vec has length height = infeatures\n",
    "       if (vec_idx * 2 + 1 < height) { // Check bounds for half2\n",
    "           blockvec[threadIdx.x] = vec[vec_idx];\n",
    "       } else if (vec_idx * 2 < height) {\n",
    "           // Handle case where only the first half is valid\n",
    "           // This requires careful handling or assuming vec length is even\n",
    "            blockvec[threadIdx.x] = __halves2half2(((half*)vec)[vec_idx * 2], __float2half(0.0f));\n",
    "       }\n",
    "       else {\n",
    "            blockvec[threadIdx.x] = __float2half2_rn(0.0f); // Pad with zero\n",
    "       }\n",
    "   }\n",
    "\n",
    "  __shared__ half2 deq2[64][32];\n",
    "  int val = threadIdx.x / 32;\n",
    "  int off = threadIdx.x % 32;\n",
    "  for (; val < 64; val += BLOCKWIDTH / 32) {\n",
    "    deq2[val][off] = __halves2half2(\n",
    "       __int2half_rn(val & 0x7), __int2half_rn(val >> 3)\n",
    "    );\n",
    "  }\n",
    "\n",
    "  __syncthreads();\n",
    "\n",
    "  if (col >= width) return; // Check column bounds\n",
    "\n",
    "  half2 scale = __float2half2_rn(scales[col]);\n",
    "  half2 zero = __float2half2_rn(-zeros[col]); // Note: using -zeros[col]\n",
    "\n",
    "  int i = width * row + col;\n",
    "  int k = 0;\n",
    "\n",
    "  float res = 0;\n",
    "  half2 res2;\n",
    "\n",
    "  unsigned int tmp1;\n",
    "  unsigned int tmp2;\n",
    "  unsigned int tmp;\n",
    "\n",
    "\n",
    "  while (k < blockwidth2) { // Iterate through input features (paired in half2)\n",
    "    res2 = {};\n",
    "    tmp1 = as_unsigned(mat[i]);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >>  0) & 0x3f][off], scale, zero), blockvec[k + 0], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >>  6) & 0x3f][off], scale, zero), blockvec[k + 1], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >> 12) & 0x3f][off], scale, zero), blockvec[k + 2], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >> 18) & 0x3f][off], scale, zero), blockvec[k + 3], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >> 24) & 0x3f][off], scale, zero), blockvec[k + 4], res2);\n",
    "    i += width;\n",
    "    tmp2 = as_unsigned(mat[i]);\n",
    "    tmp = (tmp1 >> 30) | ((tmp2 << 2) & 0x3c);\n",
    "    res2 = __hfma2(__hfma2(deq2[tmp][off], scale, zero), blockvec[k + 5], res2);\n",
    "    tmp2 >>= 4;\n",
    "    k += 6;\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp2 >>  0) & 0x3f][off], scale, zero), blockvec[k + 0], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp2 >>  6) & 0x3f][off], scale, zero), blockvec[k + 1], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp2 >> 12) & 0x3f][off], scale, zero), blockvec[k + 2], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp2 >> 18) & 0x3f][off], scale, zero), blockvec[k + 3], res2);\n",
    "    i += width;\n",
    "    tmp1 = as_unsigned(mat[i]);\n",
    "    tmp = (tmp2 >> 24) | ((tmp1 << 4) & 0x30);\n",
    "    res2 = __hfma2(__hfma2(deq2[tmp][off], scale, zero), blockvec[k + 4], res2);\n",
    "    tmp1 >>= 2;\n",
    "    k += 5;\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >>  0) & 0x3f][off], scale, zero), blockvec[k + 0], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >>  6) & 0x3f][off], scale, zero), blockvec[k + 1], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >> 12) & 0x3f][off], scale, zero), blockvec[k + 2], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >> 18) & 0x3f][off], scale, zero), blockvec[k + 3], res2);\n",
    "    res2 = __hfma2(__hfma2(deq2[(tmp1 >> 24) & 0x3f][off], scale, zero), blockvec[k + 4], res2);\n",
    "    i += width;\n",
    "    k += 5;\n",
    "    res += __half2float(res2.x) + __half2float(res2.y);\n",
    "  }\n",
    "\n",
    "  atomicAdd(&mul[col], res);\n",
    "}\n",
    "\n",
    "\n",
    "template <typename scalar_t>\n",
    "__global__ void VecQuant8MatMulKernel(\n",
    "    const  scalar_t* __restrict__ vec,     // Input vector [infeatures]\n",
    "    const  uint32_t* __restrict__ mat,     // Packed weights [infeatures / 4, outfeatures]\n",
    "           scalar_t* __restrict__ mul,     // Output vector [outfeatures]\n",
    "    const  scalar_t* __restrict__ scales,  // Scales [outfeatures]\n",
    "    const   uint8_t* __restrict__ zeros,   // Integer zero points [outfeatures]\n",
    "    int height,                           // Infeatures dimension\n",
    "    int width                             // Outfeatures dimension\n",
    ") {\n",
    "    int out_col = blockIdx.x * blockDim.x + threadIdx.x; // Maps threads to output columns\n",
    "\n",
    "    if (out_col >= width) return; // Ensure thread is within output bounds\n",
    "\n",
    "    scalar_t scale = scales[out_col];\n",
    "    uint8_t zero_point = zeros[out_col];\n",
    "    scalar_t zero_point_f = static_cast<scalar_t>(zero_point);\n",
    "\n",
    "    scalar_t accum = 0;\n",
    "\n",
    "    // Iterate over the input features (height = infeatures)\n",
    "    for (int k = 0; k < height; ++k) {\n",
    "        // Calculate index into packed matrix\n",
    "        int packed_row = k / 4;\n",
    "        int sub_idx = k % 4; // Which 8-bit weight within the uint32 (0, 1, 2, or 3)\n",
    "\n",
    "        // Read the packed uint32 value\n",
    "        // Index: packed_row * width + out_col (Column-major access)\n",
    "        uint32_t packed_val = mat[packed_row * width + out_col];\n",
    "\n",
    "        // Unpack the specific 8-bit weight\n",
    "        uint8_t q_val = (packed_val >> (sub_idx * 8)) & 0xFF;\n",
    "\n",
    "        // Dequantize: W = scale * (Q - zero_point)\n",
    "        scalar_t w_val = scale * (static_cast<scalar_t>(q_val) - zero_point_f);\n",
    "\n",
    "        // Multiply and accumulate\n",
    "        accum += w_val * vec[k];\n",
    "    }\n",
    "\n",
    "    // Atomically add the result for this output column\n",
    "    // Assumes mul is initialized to zero before kernel launch\n",
    "    atomicAdd(&mul[out_col], accum);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This command compiles the C++/CUDA code and installs the module\n",
    "# The \"! \" prefix runs this as a shell command\n",
    "# Make sure the GPU is enabled in your Kaggle notebook settings\n",
    "!pip install . --verbose\n",
    "# Using --verbose can help diagnose build issues if they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import quant_cuda\n",
    "    print(\"quant_cuda module imported successfully!\")\n",
    "    print(\"Available functions:\", dir(quant_cuda))\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import quant_cuda: {e}\")\n",
    "    print(\"Please check the build output in the cell above for errors.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during import: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_dir = '/kaggle/input/gptmodified'\n",
    "\n",
    "if module_dir not in sys.path:\n",
    "    sys.path.insert(0, module_dir)\n",
    "\n",
    "import gptq\n",
    "import quant\n",
    "import modelutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/demo_single_layer.py\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "module_dir_working = '/kaggle/working'\n",
    "module_dir_input = '/kaggle/input/gptqmodified' # Adjust if needed\n",
    "\n",
    "if module_dir_working not in sys.path:\n",
    "    sys.path.insert(0, module_dir_working)\n",
    "if module_dir_input not in sys.path:\n",
    "    sys.path.insert(0, module_dir_input)\n",
    "\n",
    "try:\n",
    "    from gptq import GPTQ\n",
    "    from modelutils import find_layers\n",
    "    from quant import Quantizer, Quant3Linear, make_quant3\n",
    "    import quant_cuda\n",
    "    print(\"Custom modules and CUDA extension imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during import: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "DEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if DEV != torch.device(\"cpu\") else torch.float32\n",
    "print(f\"Using device: {DEV}, dtype: {torch_dtype}\")\n",
    "\n",
    "in_features = 1024\n",
    "out_features = 512\n",
    "assert in_features % 4 == 0, \"in_features must be divisible by 4\"\n",
    "\n",
    "wbits = 8\n",
    "groupsize = -1\n",
    "sym = False\n",
    "percdamp = 0.01\n",
    "nsamples = 32\n",
    "seqlen = 16\n",
    "\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self, in_f, out_f):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_f, out_f, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "model = DemoModel(in_features, out_features).to(DEV, dtype=torch_dtype)\n",
    "original_layer = model.linear_layer\n",
    "print(f\"Created original model with layer: {original_layer}\")\n",
    "\n",
    "print(f\"\\nGenerating {nsamples} dummy calibration samples...\")\n",
    "calibration_data = [torch.randn(seqlen, in_features, device=DEV, dtype=torch_dtype) for _ in range(nsamples)]\n",
    "print(\"Dummy data generated.\")\n",
    "\n",
    "print(\"\\nSetting up GPTQ and Quantizer...\")\n",
    "layer_name = 'linear_layer'\n",
    "gptq_instance = GPTQ(original_layer)\n",
    "quantizer = Quantizer()\n",
    "quantizer.configure(wbits, perchannel=True, sym=sym, mse=False)\n",
    "gptq_instance.quantizer = quantizer\n",
    "print(\"GPTQ setup complete.\")\n",
    "\n",
    "print(\"\\nRunning calibration (computing Hessian)...\")\n",
    "with torch.no_grad():\n",
    "    for inp_batch in calibration_data:\n",
    "        out_batch = original_layer(inp_batch)\n",
    "        inp_reshaped = inp_batch.view(-1, in_features)\n",
    "        out_reshaped = out_batch.view(-1, out_features)\n",
    "        gptq_instance.add_batch(inp_reshaped, out_reshaped)\n",
    "print(\"Calibration complete.\")\n",
    "\n",
    "print(\"\\nPerforming quantization (fasterquant)...\")\n",
    "quant_tick = time.time()\n",
    "try:\n",
    "    gptq_instance.fasterquant(percdamp=percdamp, groupsize=groupsize)\n",
    "except Exception as e:\n",
    "    print(f\"Error during fasterquant: {e}\"); traceback.print_exc(); sys.exit(1)\n",
    "quant_time = time.time() - quant_tick\n",
    "print(f\"Quantization finished in {quant_time:.2f}s.\")\n",
    "\n",
    "# --- Get FP Quantized Output (Reference) ---\n",
    "print(\"\\nGetting reference output from FP-quantized layer (before packing)...\")\n",
    "test_input = torch.randn(1, in_features, device=DEV, dtype=torch_dtype)\n",
    "with torch.no_grad():\n",
    "    # original_layer weights are now the FP representation of quantized weights\n",
    "    fp_quantized_output = original_layer(test_input).detach().clone()\n",
    "print(f\"Reference FP Quantized Output (example): {fp_quantized_output.flatten()[:5]}\")\n",
    "\n",
    "# --- Packing ---\n",
    "print(\"\\nPacking the quantized layer...\")\n",
    "pack_tick = time.time()\n",
    "try:\n",
    "    final_quantizer = gptq_instance.quantizer.to('cpu')\n",
    "    scale = final_quantizer.scale\n",
    "    zero = final_quantizer.zero\n",
    "    layer_map = {layer_name: original_layer}\n",
    "    make_quant3(model, layer_map) # Modifies model in-place\n",
    "    packed_layer = model.linear_layer\n",
    "    if not isinstance(packed_layer, Quant3Linear):\n",
    "         raise TypeError(f\"make_quant3 failed. Found: {type(packed_layer)}\")\n",
    "    print(f\"Layer replaced with: {packed_layer}\")\n",
    "    packed_layer.pack(original_layer.to('cpu'), scale, zero)\n",
    "except Exception as e:\n",
    "    print(f\"Error during packing: {e}\"); traceback.print_exc(); sys.exit(1)\n",
    "pack_time = time.time() - pack_tick\n",
    "print(f\"Packing finished in {pack_time:.2f}s.\")\n",
    "\n",
    "# --- Verification Checks ---\n",
    "print(\"\\n--- Verification Checks ---\")\n",
    "verification_passed = True\n",
    "try:\n",
    "    packed_layer_cpu = packed_layer.to('cpu') # Move to CPU for checks\n",
    "\n",
    "    # 1. Check qweight\n",
    "    qweight = packed_layer_cpu.qweight\n",
    "    print(f\"qweight dtype: {qweight.dtype}\")\n",
    "    print(f\"qweight shape: {qweight.shape}\")\n",
    "    expected_qweight_shape = (in_features // 4, out_features)\n",
    "    if qweight.dtype != torch.uint32:\n",
    "        print(\"!!! Verification FAILED: qweight dtype is not torch.uint32 !!!\")\n",
    "        verification_passed = False\n",
    "    if qweight.shape != expected_qweight_shape:\n",
    "        print(f\"!!! Verification FAILED: qweight shape mismatch. Expected {expected_qweight_shape}, Got {qweight.shape} !!!\")\n",
    "        verification_passed = False\n",
    "    print(f\"qweight example values: {qweight.flatten()[:5].tolist()} ... {qweight.flatten()[-5:].tolist()}\")\n",
    "\n",
    "    # 2. Check scales\n",
    "    scales = packed_layer_cpu.scales\n",
    "    print(f\"\\nscales dtype: {scales.dtype}\")\n",
    "    print(f\"scales shape: {scales.shape}\")\n",
    "    expected_scales_shape = (out_features,)\n",
    "    if scales.shape != expected_scales_shape:\n",
    "         print(f\"!!! Verification FAILED: scales shape mismatch. Expected {expected_scales_shape}, Got {scales.shape} !!!\")\n",
    "         verification_passed = False\n",
    "    if not torch.all(scales > 0):\n",
    "         print(f\"!!! Verification WARNING: Some scales are not positive !!!\")\n",
    "    print(f\"scales stats: min={scales.min().item():.4f}, max={scales.max().item():.4f}, mean={scales.mean().item():.4f}\")\n",
    "\n",
    "    # 3. Check zeros\n",
    "    zeros = packed_layer_cpu.zeros\n",
    "    print(f\"\\nzeros dtype: {zeros.dtype}\")\n",
    "    print(f\"zeros shape: {zeros.shape}\")\n",
    "    expected_zeros_shape = (out_features,)\n",
    "    if zeros.dtype != torch.uint8:\n",
    "        print(\"!!! Verification FAILED: zeros dtype is not torch.uint8 !!!\")\n",
    "        verification_passed = False\n",
    "    if zeros.shape != expected_zeros_shape:\n",
    "         print(f\"!!! Verification FAILED: zeros shape mismatch. Expected {expected_zeros_shape}, Got {zeros.shape} !!!\")\n",
    "         verification_passed = False\n",
    "    if not (torch.all(zeros >= 0) and torch.all(zeros <= 255)):\n",
    "         print(f\"!!! Verification WARNING: zeros values out of range [0, 255] !!!\")\n",
    "    print(f\"zeros stats: min={zeros.min().item()}, max={zeros.max().item()}, mean={zeros.float().mean().item():.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! Verification FAILED during checks: {e} !!!\")\n",
    "    verification_passed = False\n",
    "    traceback.print_exc()\n",
    "\n",
    "if not verification_passed:\n",
    "    print(\"\\n--- Verification FAILED ---\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"\\n--- Basic Buffer Verification Passed ---\")\n",
    "\n",
    "\n",
    "# --- Inference ---\n",
    "print(\"\\nRunning inference on packed layer...\")\n",
    "model = model.to(DEV) # Ensure model is back on GPU\n",
    "packed_layer = model.linear_layer\n",
    "try:\n",
    "    inf_tick = time.time()\n",
    "    with torch.no_grad():\n",
    "        quantized_output = packed_layer(test_input) # Use the same test_input\n",
    "    inf_time = time.time() - inf_tick\n",
    "    print(f\"Inference finished in {inf_time:.4f}s.\")\n",
    "    print(f\"Output shape: {quantized_output.shape}\")\n",
    "    print(f\"Quantized Output (example): {quantized_output.flatten()[:5]}\")\n",
    "\n",
    "    # --- 4. Compare Outputs ---\n",
    "    if 'fp_quantized_output' in locals():\n",
    "        mae = torch.mean(torch.abs(quantized_output - fp_quantized_output)).item()\n",
    "        max_diff = torch.max(torch.abs(quantized_output - fp_quantized_output)).item()\n",
    "        print(f\"\\nOutput Comparison vs FP Quantized:\")\n",
    "        print(f\"  Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "        print(f\"  Maximum Absolute Difference: {max_diff:.6f}\")\n",
    "        if mae == 0:\n",
    "             print(\"  !!! WARNING: MAE is zero. Quantization might not have had an effect or comparison is flawed. !!!\")\n",
    "        elif mae > 0.1: # Arbitrary threshold, adjust based on expectation\n",
    "             print(f\"  !!! WARNING: MAE ({mae:.6f}) seems high. Check quantization/packing. !!!\")\n",
    "        else:\n",
    "             print(\"  Difference seems reasonable (non-zero, not excessively large).\")\n",
    "    else:\n",
    "        print(\"\\nSkipping output comparison (reference output not generated).\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference or comparison: {e}\")\n",
    "    traceback.print_exc(); sys.exit(1)\n",
    "\n",
    "print(\"\\n--- Demo Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/demo_single_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile quantize_phi3_vision.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_dir = '/kaggle/input/gptqmodified' # Assuming this is the correct path now\n",
    "if module_dir not in sys.path:\n",
    "    sys.path.insert(0, module_dir)\n",
    "\n",
    "# IMPORTANT: Ensure quant.py used here has the dtype=torch.uint32 fix\n",
    "from gptq import GPTQ\n",
    "from modelutils import find_layers\n",
    "from quant import Quantizer, Quant3Linear, make_quant3\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Argument Parsing ---\n",
    "# (parse_args function remains the same)\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Quantize Phi-3.5 Vision model using GPTQ\")\n",
    "    parser.add_argument('--model_id', type=str, default=\"microsoft/Phi-3.5-vision-instruct\", help='Model ID from Hugging Face Hub.')\n",
    "    parser.add_argument('--dataset_path', type=str, default='/kaggle/input/flickr30k', help='Path to the Flickr30k dataset.')\n",
    "    parser.add_argument('--captions_file', type=str, default='captions.txt', help='Name of the captions file within dataset_path.')\n",
    "    parser.add_argument('--image_subdir', type=str, default='flickr30k_images', help='Subdirectory containing images within dataset_path.')\n",
    "    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration data samples.')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='Random seed for sampling calibration data.')\n",
    "    parser.add_argument('--wbits', type=int, default=8, choices=[8], help='Number of bits for quantization.')\n",
    "    parser.add_argument('--groupsize', type=int, default=-1, help='Group size for quantization (-1 for per-channel).')\n",
    "    parser.add_argument('--sym', action='store_true', help='Whether to perform symmetric quantization.')\n",
    "    parser.add_argument('--percdamp', type=float, default=0.01, help='Damping factor for Hessian.')\n",
    "    parser.add_argument('--save_dir', type=str, default='/kaggle/working/phi3_vision_quantized_8bit_hf', help='Directory to save the quantized model using save_pretrained.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if args.wbits != 8:\n",
    "        raise ValueError(\"This script only supports 8-bit quantization (--wbits 8)\")\n",
    "    return args\n",
    "\n",
    "\n",
    "# --- Input Preparation ---\n",
    "# (prepare_calibration_input function remains the same)\n",
    "def prepare_calibration_input(img_path, caption_text, processor, device):\n",
    "    try:\n",
    "        if not os.path.exists(img_path):\n",
    "            return None\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        placeholder = \"<|image_1|>\\n\"\n",
    "        user_content = placeholder + str(caption_text).strip()\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = processor(prompt, [image], return_tensors=\"pt\", padding=False)\n",
    "        return {k: v.to(device) for k, v in inputs.items()}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Quantization Function ---\n",
    "# (phi3_vision_sequential function remains the same)\n",
    "@torch.no_grad()\n",
    "def phi3_vision_sequential(model, processor, calibration_data, dev, args):\n",
    "    print('Starting GPTQ quantization...')\n",
    "    try:\n",
    "        layers = model.model.layers\n",
    "        print(f\"Found {len(layers)} transformer layers.\")\n",
    "    except AttributeError:\n",
    "        print(\"Could not find layers at model.model.layers.\")\n",
    "        return None, None\n",
    "\n",
    "    model.model.vision_embed_tokens = model.model.vision_embed_tokens.to(dev)\n",
    "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
    "    model.model.norm = model.model.norm.to(dev)\n",
    "    model.lm_head = model.lm_head.to(dev)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i] = layers[i].to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    inputs_cache = []\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, hidden_states, attention_mask=None, position_ids=None, **kwargs):\n",
    "            if isinstance(hidden_states, torch.Tensor):\n",
    "                inputs_cache.append({\n",
    "                    'hidden_states': hidden_states.cpu(),\n",
    "                    'attention_mask': attention_mask.cpu() if attention_mask is not None else None,\n",
    "                    'position_ids': position_ids.cpu() if position_ids is not None else None,\n",
    "                })\n",
    "            raise ValueError\n",
    "\n",
    "    print(\"Capturing inputs to the first transformer layer...\")\n",
    "    original_layer_0 = layers[0]\n",
    "    layers[0] = Catcher(layers[0]).to(dev)\n",
    "\n",
    "    for i, (img_path, caption) in enumerate(calibration_data):\n",
    "        if i >= args.nsamples: break\n",
    "        calib_input = prepare_calibration_input(img_path, caption, processor, dev)\n",
    "        if calib_input is None: continue\n",
    "        try: model(**calib_input)\n",
    "        except ValueError: pass\n",
    "        except Exception as e: print(f\"Error during capture sample {i}: {e}\")\n",
    "        finally: del calib_input; torch.cuda.empty_cache()\n",
    "\n",
    "    layers[0] = original_layer_0.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if not inputs_cache:\n",
    "        print(\"Error: No calibration inputs captured.\")\n",
    "        return model, None\n",
    "    print(f\"Captured inputs for {len(inputs_cache)} samples.\")\n",
    "\n",
    "    quantizers = {}\n",
    "    current_layer_inputs = inputs_cache\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        print(f\"\\n--- Quantizing layer {i} ---\")\n",
    "        layer = layers[i].to(dev)\n",
    "        layer.train(False)\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        if not subset:\n",
    "            print(f\"  No linear layers found. Propagating inputs.\")\n",
    "            next_layer_inputs = []\n",
    "            for sample_idx in range(len(current_layer_inputs)):\n",
    "                layer_input_args = {k: v.to(dev) if isinstance(v, torch.Tensor) else v for k, v in current_layer_inputs[sample_idx].items() if v is not None}\n",
    "                if 'hidden_states' not in layer_input_args: continue\n",
    "                try:\n",
    "                    layer_outputs = layer(**layer_input_args, use_cache=False)\n",
    "                    next_layer_inputs.append({\n",
    "                        'hidden_states': layer_outputs[0].cpu(),\n",
    "                        'attention_mask': current_layer_inputs[sample_idx]['attention_mask'],\n",
    "                        'position_ids': current_layer_inputs[sample_idx].get('position_ids', None)\n",
    "                    })\n",
    "                except Exception as e: print(f\"Error during fwd (no quant) L{i} S{sample_idx}: {e}\"); traceback.print_exc(); next_layer_inputs.append(current_layer_inputs[sample_idx])\n",
    "                finally: del layer_input_args; torch.cuda.empty_cache()\n",
    "            layers[i] = layer.cpu()\n",
    "            current_layer_inputs = next_layer_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        gptq_handlers = {name: GPTQ(subset[name]) for name in subset}\n",
    "        for name in subset:\n",
    "            gptq_handlers[name].quantizer = Quantizer()\n",
    "            gptq_handlers[name].quantizer.configure(args.wbits, perchannel=True, sym=args.sym, mse=False)\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                try:\n",
    "                    inp_tensor = inp[0].data\n",
    "                    out_tensor = out.data if isinstance(out, torch.Tensor) else out[0].data\n",
    "                    gptq_handlers[name].add_batch(inp_tensor, out_tensor)\n",
    "                except Exception as e: print(f\"!!! Error in add_batch hook for {name}: {e} !!!\")\n",
    "            return tmp\n",
    "\n",
    "        handles = [subset[name].register_forward_hook(add_batch(name)) for name in subset]\n",
    "\n",
    "        for sample_idx in range(len(current_layer_inputs)):\n",
    "            layer_input_args = {k: v.to(dev) if isinstance(v, torch.Tensor) else v for k, v in current_layer_inputs[sample_idx].items() if v is not None}\n",
    "            if 'hidden_states' not in layer_input_args: continue\n",
    "            try: _ = layer(**layer_input_args, use_cache=False)\n",
    "            except Exception as e: print(f\"Error during Hessian L{i} S{sample_idx}: {e}\"); traceback.print_exc()\n",
    "            finally: del layer_input_args; torch.cuda.empty_cache()\n",
    "\n",
    "        for h in handles: h.remove()\n",
    "\n",
    "        for name in subset:\n",
    "            try:\n",
    "                print(f\"  Quantizing {name}...\")\n",
    "                gptq_handlers[name].fasterquant(percdamp=args.percdamp, groupsize=args.groupsize)\n",
    "                quantizers[f'model.layers.{i}.{name}'] = gptq_handlers[name].quantizer.cpu()\n",
    "                gptq_handlers[name].free()\n",
    "            except Exception as e: print(f\"Error during fasterquant for {name}: {e}\"); traceback.print_exc()\n",
    "\n",
    "        next_layer_inputs = []\n",
    "        for sample_idx in range(len(current_layer_inputs)):\n",
    "            layer_input_args = {k: v.to(dev) if isinstance(v, torch.Tensor) else v for k, v in current_layer_inputs[sample_idx].items() if v is not None}\n",
    "            if 'hidden_states' not in layer_input_args: continue\n",
    "            try:\n",
    "                layer_outputs = layer(**layer_input_args, use_cache=False)\n",
    "                next_layer_inputs.append({\n",
    "                    'hidden_states': layer_outputs[0].cpu(),\n",
    "                    'attention_mask': current_layer_inputs[sample_idx]['attention_mask'],\n",
    "                    'position_ids': current_layer_inputs[sample_idx].get('position_ids', None)\n",
    "                })\n",
    "            except Exception as e: print(f\"Error during propagation L{i} S{sample_idx}: {e}\"); traceback.print_exc(); next_layer_inputs.append(current_layer_inputs[sample_idx])\n",
    "            finally: del layer_input_args; torch.cuda.empty_cache()\n",
    "\n",
    "        layers[i] = layer.cpu()\n",
    "        del layer, gptq_handlers, subset\n",
    "        current_layer_inputs = next_layer_inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nGPTQ quantization finished.\")\n",
    "    model.model.vision_embed_tokens = model.model.vision_embed_tokens.to('cpu')\n",
    "    model.model.embed_tokens = model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm = model.model.norm.to('cpu')\n",
    "    model.lm_head = model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, quantizers\n",
    "\n",
    "\n",
    "# --- Packing Function ---\n",
    "# (phi3_vision_pack function remains the same)\n",
    "def phi3_vision_pack(model, quantizers):\n",
    "    print(\"\\nPacking model weights...\")\n",
    "    layer_names_to_replace = list(quantizers.keys())\n",
    "    modules_to_modify = list({name.rsplit('.', 1)[0] for name in layer_names_to_replace})\n",
    "    total_modules = len(modules_to_modify)\n",
    "    print(f\"  Replacing layers in modules: {modules_to_modify}\")\n",
    "    relative_names = {\n",
    "        mod_name: [name.split('.')[-1] for name in layer_names_to_replace if name.startswith(mod_name + '.')]\n",
    "        for mod_name in modules_to_modify\n",
    "    }\n",
    "\n",
    "    # Process each module and print progress after each module is processed.\n",
    "    for mod_index, mod_name in enumerate(modules_to_modify, start=1):\n",
    "        print(f\"\\nProcessing module {mod_index}/{total_modules}: '{mod_name}'\")\n",
    "        parent_module = model.get_submodule(mod_name)\n",
    "        names_in_module = relative_names[mod_name]\n",
    "        original_linears = {name: layer for name, layer in find_layers(parent_module).items() if name in names_in_module}\n",
    "        layers_to_replace_dict = {k: v for k, v in find_layers(parent_module).items() if k in names_in_module}\n",
    "\n",
    "        if not layers_to_replace_dict:\n",
    "            print(f\"  No layers found in module '{mod_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        make_quant3(parent_module, layers_to_replace_dict)\n",
    "        qlayers = find_layers(parent_module, [Quant3Linear])\n",
    "        total_layers = len(qlayers)\n",
    "        if total_layers == 0:\n",
    "            print(f\"  No Quant3Linear layers found in module '{mod_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Process each linear layer in the current module.\n",
    "        layers_interval = max(1, total_layers // 5)  # Change interval as needed.\n",
    "        for layer_index, (name, qlayer) in enumerate(qlayers.items(), start=1):\n",
    "            if name not in names_in_module:\n",
    "                continue\n",
    "            full_name = f\"{mod_name}.{name}\"\n",
    "            if full_name not in quantizers:\n",
    "                continue\n",
    "            if name not in original_linears:\n",
    "                continue\n",
    "\n",
    "            quantizer_data = quantizers[full_name].to('cpu')\n",
    "            original_layer_ref = original_linears[name].to('cpu')\n",
    "            try:\n",
    "                qlayer.pack(original_layer_ref, quantizer_data.scale, quantizer_data.zero)\n",
    "            except Exception as e:\n",
    "                print(f\"Error packing {full_name}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "            # Print progress every few layers.\n",
    "            if layer_index % layers_interval == 0 or layer_index == total_layers:\n",
    "                print(f\"  Processed {layer_index}/{total_layers} layers in module '{mod_name}'\")\n",
    "\n",
    "    print(\"\\nPacking complete.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    print(\"Parsed arguments:\", args)\n",
    "\n",
    "    DEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    print(f\"Using device: {DEV}, dtype: {torch_dtype}\")\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    print(f\"Loading model: {args.model_id}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='eager' # Keep eager as requested\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(args.model_id, trust_remote_code=True)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Loading calibration data...\")\n",
    "    captions_path = os.path.join(args.dataset_path, args.captions_file)\n",
    "    image_dir = os.path.join(args.dataset_path, args.image_subdir)\n",
    "    if not os.path.exists(captions_path): raise FileNotFoundError(f\"Captions file not found: {captions_path}\")\n",
    "    if not os.path.isdir(image_dir): raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "\n",
    "    try:\n",
    "        df_captions = pd.read_csv(captions_path)\n",
    "        if len(df_captions.columns) < 3: df_captions = pd.read_csv(captions_path, sep='\\t', header=None)\n",
    "        df_captions.columns = ['image_name', 'caption_index', 'caption']\n",
    "    except Exception as e: raise ValueError(f\"Error reading captions file {captions_path}: {e}\")\n",
    "\n",
    "    df_captions['caption'] = df_captions['caption'].astype(str).str.strip()\n",
    "    captions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\n",
    "    all_image_names = list(captions_dict.keys())\n",
    "    if not all_image_names: raise ValueError(f\"No image names found in captions file: {captions_path}\")\n",
    "\n",
    "    num_available = len(all_image_names)\n",
    "    num_to_sample = min(args.nsamples, num_available)\n",
    "    print(f\"Sampling {num_to_sample} calibration images.\")\n",
    "    sampled_image_names = random.sample(all_image_names, num_to_sample)\n",
    "\n",
    "    calibration_data = []\n",
    "    missing_files = 0\n",
    "    for img_name in sampled_image_names:\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        if os.path.exists(img_path): calibration_data.append((img_path, captions_dict[img_name][0]))\n",
    "        else: missing_files += 1\n",
    "    if missing_files > 0: print(f\"Warning: Skipped {missing_files} samples due to missing image files.\")\n",
    "    if not calibration_data: raise ValueError(\"No valid calibration data prepared.\")\n",
    "    print(f\"Prepared {len(calibration_data)} calibration data pairs.\")\n",
    "\n",
    "    tick = time.time()\n",
    "    model.to('cpu'); torch.cuda.empty_cache()\n",
    "    model, quantizers = phi3_vision_sequential(model, processor, calibration_data, DEV, args)\n",
    "    quant_time = time.time() - tick\n",
    "    print(f\"\\nQuantization completed in {quant_time:.2f} seconds.\")\n",
    "\n",
    "    if args.save_dir and quantizers:\n",
    "        print(\"\\nProceeding with packing and saving...\")\n",
    "        model.to('cpu'); torch.cuda.empty_cache()\n",
    "\n",
    "        pack_tick = time.time()\n",
    "        model = phi3_vision_pack(model, quantizers)\n",
    "        pack_time = time.time() - pack_tick\n",
    "        print(f\"Packing finished in {pack_time:.2f} seconds.\")\n",
    "\n",
    "        print(f\"\\nSaving packed model and processor to {args.save_dir}...\")\n",
    "        save_tick = time.time()\n",
    "        os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "        # --- Temporary Cast Before Saving ---\n",
    "        original_dtypes = {}\n",
    "        try:\n",
    "            print(\"Temporarily casting qweight buffers to int32 for saving...\")\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, Quant3Linear): # Use the actual class name used\n",
    "                    if hasattr(module, 'qweight') and module.qweight.dtype == torch.uint32:\n",
    "                        original_dtypes[name] = module.qweight.dtype\n",
    "                        module.qweight = module.qweight.view(torch.int32) # Reinterpret bits as int32\n",
    "\n",
    "            # Use safe_serialization=False to handle tied weights AND the lack of uint32 support\n",
    "            model.save_pretrained(args.save_dir, safe_serialization=False)\n",
    "            processor.save_pretrained(args.save_dir)\n",
    "            save_time = time.time() - save_tick\n",
    "            print(f\"Model and processor saved successfully in {save_time:.2f} seconds.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during saving: {e}\")\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            # --- Cast Back After Saving (Optional but good practice) ---\n",
    "            print(\"Casting qweight buffers back to original dtype...\")\n",
    "            for name, module in model.named_modules():\n",
    "                 if name in original_dtypes:\n",
    "                     if hasattr(module, 'qweight') and module.qweight.dtype == torch.int32:\n",
    "                         module.qweight = module.qweight.view(torch.uint32) # Reinterpret bits back to uint32\n",
    "            print(\"Casting back complete.\")\n",
    "\n",
    "\n",
    "        if os.path.exists(os.path.join(args.save_dir, \"pytorch_model.bin\")):\n",
    "             print(f\"Saved artifacts found in {args.save_dir}\")\n",
    "        else:\n",
    "             print(f\"Warning: Expected files not found in save directory {args.save_dir}\")\n",
    "\n",
    "    elif not quantizers: print(\"\\nQuantization failed. Model not packed or saved.\")\n",
    "    else: print(\"\\nNo save directory specified (--save_dir). Skipping saving.\")\n",
    "\n",
    "    print(\"\\n--- Quantization Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python quantize_phi3_vision.py \\\n",
    "    --dataset_path /kaggle/input/flickr30k \\\n",
    "    --captions_file captions.txt \\\n",
    "    --image_subdir flickr30k_images \\\n",
    "    --nsamples 16 \\\n",
    "    --save_dir /kaggle/working/phi3_vision_quantized_8bit_hf_sym \\\n",
    "    --sym  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === INFERENCE CELL (Corrected Path + Memory Footprint) ===\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# --- Setup Paths ---\n",
    "# Add path to custom modules NEEDED FOR LOADING the custom layer\n",
    "module_dir_working = '/kaggle/working'\n",
    "# Assuming gptq.py and modelutils.py are here now too based on previous error fixes\n",
    "# module_dir_input = '/kaggle/input/gptqmain4' # Input for others (If needed)\n",
    "\n",
    "if module_dir_working not in sys.path:\n",
    "    sys.path.insert(0, module_dir_working)\n",
    "# if module_dir_input not in sys.path: # Uncomment if gptq/modelutils are still in input\n",
    "#     sys.path.insert(0, module_dir_input)\n",
    "\n",
    "# --- Imports (Needed for loading custom code) ---\n",
    "try:\n",
    "    from gptq import GPTQ\n",
    "    from modelutils import find_layers\n",
    "    from quant import Quantizer, Quant3Linear, make_quant3\n",
    "    import quant_cuda\n",
    "    print(\"Custom modules for loading found.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing custom modules needed for loading: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during custom module import: {e}\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "# *** CORRECTED: POINT TO THE DIRECTORY SAVED BY save_pretrained ***\n",
    "quantized_model_dir = '/kaggle/working/phi3_vision_quantized_8bit_hf_sym' # Default save_dir\n",
    "\n",
    "original_model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "dataset_path = '/kaggle/input/flickr30k'\n",
    "captions_file_name = 'captions.txt'\n",
    "image_subdir = 'flickr30k_images'\n",
    "\n",
    "# --- Check if model saved ---\n",
    "saved_weight_file = os.path.join(quantized_model_dir, \"pytorch_model.bin\") # Check for the correct file\n",
    "if not os.path.exists(saved_weight_file):\n",
    "    print(f\"Error: Quantized model weights file ({saved_weight_file}) not found.\")\n",
    "    print(f\"Please ensure the quantization script ran successfully and saved to: {quantized_model_dir}\")\n",
    "else:\n",
    "    print(f\"Found quantized model artifacts in {quantized_model_dir}. Proceeding with inference.\")\n",
    "\n",
    "    # --- Setup ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    print(f\"Using device: {device}, dtype: {torch_dtype}\")\n",
    "\n",
    "    print(\"Loading quantized model and original processor...\")\n",
    "    load_tick = time.time()\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            quantized_model_dir,      # Load from the directory\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=\"auto\",        # Handles device placement\n",
    "            trust_remote_code=True    # *** CRITICAL ***\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(original_model_id, trust_remote_code=True)\n",
    "        load_time = time.time() - load_tick\n",
    "        print(f\"Model and processor loaded in {load_time:.2f} seconds.\")\n",
    "        model.eval() # Ensure model is in eval mode\n",
    "\n",
    "        # --- Calculate and Print Memory Footprint ---\n",
    "        try:\n",
    "            mem_footprint_bytes = model.get_memory_footprint()\n",
    "            mem_footprint_gb = mem_footprint_bytes / (1024**3)\n",
    "            print(f\"Model memory footprint (calculated by HF): {mem_footprint_gb:.2f} GB\")\n",
    "            if device == \"cuda\":\n",
    "                 # Wait for model loading to finish if using device_map='auto'\n",
    "                 torch.cuda.synchronize()\n",
    "                 allocated_mem_gb = torch.cuda.memory_allocated() / (1024**3)\n",
    "                 reserved_mem_gb = torch.cuda.memory_reserved() / (1024**3)\n",
    "                 print(f\"GPU Memory Allocated: {allocated_mem_gb:.2f} GB\")\n",
    "                 print(f\"GPU Memory Reserved: {reserved_mem_gb:.2f} GB\")\n",
    "        except Exception as mem_e:\n",
    "            print(f\"Could not calculate memory footprint: {mem_e}\")\n",
    "        # --------------------------------------------\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading the model or processor: {e}\")\n",
    "        traceback.print_exc()\n",
    "        model = None\n",
    "\n",
    "    # --- Prepare Sample Input ---\n",
    "    if model:\n",
    "        print(\"\\nPreparing a sample input...\")\n",
    "        captions_path = os.path.join(dataset_path, captions_file_name)\n",
    "        image_dir = os.path.join(dataset_path, image_subdir)\n",
    "        test_img_path = None\n",
    "        test_caption = \"Describe the image.\"\n",
    "        try:\n",
    "            df_captions = pd.read_csv(captions_path)\n",
    "            if len(df_captions.columns) < 3: df_captions = pd.read_csv(captions_path, sep='\\t', header=None)\n",
    "            df_captions.columns = ['image_name', 'caption_index', 'caption']\n",
    "            df_captions['caption'] = df_captions['caption'].astype(str).str.strip()\n",
    "            captions_dict = df_captions.groupby('image_name')['caption'].apply(list).to_dict()\n",
    "            all_image_names = list(captions_dict.keys())\n",
    "            if all_image_names:\n",
    "                test_img_name = random.choice(all_image_names)\n",
    "                test_img_path = os.path.join(image_dir, test_img_name)\n",
    "                test_caption = captions_dict.get(test_img_name, [test_caption])[0]\n",
    "                print(f\"Selected sample: Image='{test_img_name}', Caption='{test_caption}'\")\n",
    "                if not os.path.exists(test_img_path): print(f\"Error: Image not found: {test_img_path}\"); test_img_path = None\n",
    "            else: print(\"No images found in caption data.\")\n",
    "        except Exception as e: print(f\"Error loading caption data: {e}\")\n",
    "\n",
    "        # --- Run Inference ---\n",
    "        if test_img_path:\n",
    "            try:\n",
    "                image = Image.open(test_img_path).convert(\"RGB\")\n",
    "                placeholder = \"<|image_1|>\\n\"; user_content = placeholder + str(test_caption).strip()\n",
    "                messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "                prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = processor(prompt, [image], return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()} # Move to model's device\n",
    "                generation_args = {\"max_new_tokens\": 500, \"temperature\": 0.0, \"do_sample\": False}\n",
    "\n",
    "                print(\"\\nRunning model.generate (using quantized layers)...\")\n",
    "                inf_tick = time.time()\n",
    "                with torch.no_grad():\n",
    "                    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
    "                inf_time = time.time() - inf_tick\n",
    "\n",
    "                input_token_len = inputs['input_ids'].shape[1]\n",
    "                generate_ids = generate_ids[:, input_token_len:]\n",
    "                response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "                print(f\"\\nInference completed in {inf_time:.2f} seconds.\")\n",
    "                print(\"\\n--- Inference Result ---\")\n",
    "                print(response)\n",
    "                print(\"------------------------\")\n",
    "            except Exception as e: print(f\"\\nError during inference: {e}\"); traceback.print_exc()\n",
    "        else: print(\"\\nSkipping inference due to issues preparing test sample.\")\n",
    "\n",
    "    print(\"\\n--- Inference Cell Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2808179,
     "sourceId": 4845244,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7099181,
     "sourceId": 11345956,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
