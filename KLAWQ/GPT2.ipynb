{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":11425261,"datasetId":7155573,"databundleVersionId":11863045},{"sourceType":"datasetVersion","sourceId":11629547,"datasetId":7296411,"databundleVersionId":12095246},{"sourceType":"kernelVersion","sourceId":184692496}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -r '/kaggle/input/gptqmain/GPTQModel-main/requirements.txt'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/klawq-gptq /kaggle/working/gptqvanilla2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\n\nlibrary_path = \"/kaggle/working/gptqvanilla2\" \nif library_path not in sys.path:\n     sys.path.insert(0, library_path)\n     print(f\"Added '{library_path}' to sys.path\")\n\ngptq_file_path = os.path.join(library_path, 'gptqmodel', 'quantization', 'gptq.py') \nif os.path.exists(gptq_file_path):\n    print(f\"Found gptq.py at: {gptq_file_path}\")\nfrom gptqmodel import GPTQModel, QuantizeConfig \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T18:38:48.349093Z","iopub.execute_input":"2025-04-30T18:38:48.349833Z","iopub.status.idle":"2025-04-30T18:38:48.439110Z","shell.execute_reply.started":"2025-04-30T18:38:48.349805Z","shell.execute_reply":"2025-04-30T18:38:48.438562Z"}},"outputs":[{"name":"stdout","text":"Added '/kaggle/working/gptqvanilla2' to sys.path\nFound gptq.py at: /kaggle/working/gptqvanilla2/gptqmodel/quantization/gptq.py\n\n\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\nmodel_id   = \"gpt2\"                          \nquant_path = \"gpt2-text-calibrated\" \naccess_token = \"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\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom itertools import islice\n\n# 1) Open C4 in streaming mode (no full download)\nc4_stream = load_dataset(\n    \"allenai/c4\", \n    \"en\", \n    split=\"train\", \n    streaming=True\n)\nnum_calibration_samples = 1024\ncalibration_dataset_c4 = []\nfor sample in islice(c4_stream, num_calibration_samples):\n    text = sample.get(\"text\", \"\").strip()\n    if not text:\n        continue\n    calibration_dataset_c4.append({\"content\": text})\n\n\nprint(f\"Loaded {len(calibration_dataset_c4)} C4 examples for calibration.\")\nwt2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation[:2000]\")\neval_texts = [ex[\"text\"] for ex in wt2 if ex[\"text\"].strip()]\n\nprint(f\"Using {len(eval_texts)} non-empty lines from Wikitext-2 for evaluation.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel, QuantizeConfig\nimport os\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_id    = \"gpt2\"\nquant_path  = \"/kaggle/working/gpt2-calibrated-8bit-g128\"\nquant_cfg   = QuantizeConfig(bits=8, group_size=128, beta=2.0, tau=0.7)\nmodel       = GPTQModel.load(model_id, quant_cfg, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\nmax_len = tokenizer.model_max_length\n\ncalib_tokenized = []\nfor txt in calibration_texts:\n    enc = tokenizer(\n        txt,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_len,\n    )\n    calib_tokenized.append({\n        \"input_ids\":      enc[\"input_ids\"],\n        \"attention_mask\": enc[\"attention_mask\"],\n    })\n\nmodel.quantize(calib_tokenized, batch_size=8)\nos.makedirs(os.path.dirname(quant_path), exist_ok=True)\nmodel.save(quant_path)\nprint(f\"Quantized GPT-2 saved to {quant_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\ndef clear_gpu_cache():\n    gc.collect()  \n    torch.cuda.empty_cache()  \n    torch.cuda.ipc_collect()  \n    print(\"✅ GPU VRAM and cache cleared.\")\nclear_gpu_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Quantizd 8  bit, beta=2,temp - 0.7. AlienC4","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel, QuantizeConfig\nfrom itertools import islice\n\n# 1) Stream a small “Alien-C4” subset\nc4_stream = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\neval_texts = [s[\"text\"].strip() for s in islice(c4_stream, 1024) if s[\"text\"].strip()]\nprint(f\"Alien-C4 evaluation samples: {len(eval_texts)}\")\n\n# 2) Load the quantized GPT-2 model\nquant_path = \"/kaggle/working/gpt2-calibrated-8bit-g128\"\nquant_cfg  = QuantizeConfig(bits=8, group_size=128, beta=2.0, tau=0.7)\nmodel = GPTQModel.from_pretrained(\n    quant_path,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    quantize_config=quant_cfg\n)\nmodel.eval()\n\n# 3) Tokenize for evaluation\ntokenizer           = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\nencodings = tokenizer(\n    eval_texts,\n    return_tensors=\"pt\",\n    padding=\"longest\",\n    truncation=True,\n    max_length=tokenizer.model_max_length,\n)\ninput_ids      = encodings.input_ids.to(model.device)\nattention_mask = encodings.attention_mask.to(model.device)\n\n# 4) Compute average loss & perplexity\nbatch_size = 8\nlosses = []\nwith torch.no_grad():\n    for i in range(0, len(eval_texts), batch_size):\n        b_ids  = input_ids[i : i+batch_size]\n        b_mask = attention_mask[i : i+batch_size]\n        out    = model(input_ids=b_ids, attention_mask=b_mask, labels=b_ids)\n        losses.append(out.loss.item())\n\navg_loss   = sum(losses) / len(losses)\nperplexity = math.exp(avg_loss)\nprint(f\"\\nAverage NLL loss: {avg_loss:.4f}\")\nprint(f\"Perplexity:       {perplexity:.2f}\")\n\nprint(\"\\n--- Sample Generations ---\")\nfor prompt in eval_texts[:3]:\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    gen = model.generate(**enc, max_new_tokens=50, do_sample=False)\n    continuation = tokenizer.decode(\n        gen[0, enc[\"input_ids\"].size(1):],\n        skip_special_tokens=True\n    )\n    print(\"Prompt:      \", prompt[:100] + \"…\")\n    print(\"Continuation:\", continuation.strip())\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T18:39:20.936916Z","iopub.execute_input":"2025-04-30T18:39:20.937520Z","iopub.status.idle":"2025-04-30T18:42:18.607681Z","shell.execute_reply.started":"2025-04-30T18:39:20.937495Z","shell.execute_reply":"2025-04-30T18:42:18.607004Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb3c794bc494d26a8817cbbc92b8d5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d76abffd634dbabfef26876d0880c3"}},"metadata":{}},{"name":"stdout","text":"Alien-C4 evaluation samples: 1024\n\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\nIf you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\nfrom_quantized: adapter: None\n\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0025763511657714844s                                          \n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256\n}\n\n\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \n\nAverage NLL loss: 8.4302\nPerplexity:       4583.32\n\n--- Sample Generations ---\nPrompt:       Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You…\nContinuation: The class will be held on the first Thursday of each month from 9:00am to 5:00pm.\nThe class will be held on the first Thursday of each month from 9:00am to 5:00pm.\nThe\n----------------------------------------\nPrompt:       Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\nI've got a 500gb internal dri…\nContinuation: I've also had this happen to me on a Mac OS X 10.6.1.1 machine.\nI've had this happen to me on a Mac OS X 10.6.1.1 machine.\nI've had this\n----------------------------------------\nPrompt:       Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt wi…\nContinuation: $1,000.00\n\n$1,000.00\n\n$1,000.00\n\n$1,000.00\n\n$1,000.00\n\n$1,000.00\n----------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Quantized 8 bit, beta = 2, temp = 0.7, WikiText","metadata":{}},{"cell_type":"code","source":"import os, math, torch, pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel, QuantizeConfig\nfrom itertools import islice\n\n# 1) Load your quantized GPT-2\nquant_path = \"/kaggle/working/gpt2-calibrated-8bit-g128\"\nquant_cfg  = QuantizeConfig(bits=8, group_size=128, beta=2.0, tau=0.7)\nmodel = GPTQModel.from_pretrained(\n    quant_path,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    quantize_config=quant_cfg\n)\nmodel.eval()\n\n# 2) Pull a small Wikitext-2 slice (128 non-empty lines)\nwt2      = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\ntexts    = [ex[\"text\"].strip() for ex in wt2 if ex[\"text\"].strip()][:128]\nprint(f\"Using {len(texts)} Wikitext-2 lines for evaluation\")\n\n# 3) Tokenize with padding\ntokenizer           = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\nencodings = tokenizer(\n    texts,\n    return_tensors=\"pt\",\n    padding=\"longest\",\n    truncation=True,\n    max_length=tokenizer.model_max_length,\n)\ninput_ids      = encodings.input_ids.to(model.device)\nattention_mask = encodings.attention_mask.to(model.device)\n\n# 4) Compute 4 metrics: NLL loss, perplexity, top-1 & top-5 token accuracy\nbatch_size = 8\nlosses, top1_accs, top5_accs = [], [], []\n\nwith torch.no_grad():\n    for i in range(0, len(texts), batch_size):\n        b_ids  = input_ids[i:i+batch_size]\n        b_mask = attention_mask[i:i+batch_size]\n        out    = model(input_ids=b_ids, attention_mask=b_mask, labels=b_ids)\n        losses.append(out.loss.item())\n\n        logits      = out.logits            # [B, L, V]\n        sl_logits   = logits[:, :-1, :]     # shift for next-token pred\n        sl_labels   = b_ids[:, 1:]\n        sl_mask     = b_mask[:, 1:].bool()\n\n        # top-1\n        preds1      = sl_logits.argmax(dim=-1)\n        correct1    = (preds1 == sl_labels) & sl_mask\n        top1_accs.append(correct1.sum().item() / sl_mask.sum().item())\n\n        # top-5\n        topk        = 5\n        topk_inds   = sl_logits.topk(topk, dim=-1).indices\n        match5      = (topk_inds == sl_labels.unsqueeze(-1)) & sl_mask.unsqueeze(-1)\n        top5_accs.append(match5.any(dim=-1).sum().item() / sl_mask.sum().item())\n\navg_loss   = sum(losses) / len(losses)\nperplexity = math.exp(avg_loss)\nacc1       = sum(top1_accs) / len(top1_accs)\nacc5       = sum(top5_accs) / len(top5_accs)\n\n# 5) Show metrics in a table\ndf = pd.DataFrame([\n    {\"Metric\": \"Avg NLL Loss\",      \"Value\": avg_loss},\n    {\"Metric\": \"Perplexity\",        \"Value\": perplexity},\n    {\"Metric\": \"Top-1 Token Acc\",   \"Value\": acc1},\n    {\"Metric\": \"Top-5 Token Acc\",   \"Value\": acc5},\n])\nprint(df.to_markdown(index=False))\n\n# 6) Print 3 sample continuations\nprint(\"\\n--- Generations on Wikitext-2 ---\")\nfor txt in texts[:3]:\n    enc = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n    gen = model.generate(**enc, max_new_tokens=50, do_sample=False)\n    cont = tokenizer.decode(gen[0, enc[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n    print(\"Prompt:      \", txt[:80] + \"…\")\n    print(\"Continuation:\", cont.strip())\n    print(\"-\" * 40)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T18:38:54.702663Z","iopub.execute_input":"2025-04-30T18:38:54.703354Z","iopub.status.idle":"2025-04-30T18:39:07.818174Z","shell.execute_reply.started":"2025-04-30T18:38:54.703330Z","shell.execute_reply":"2025-04-30T18:39:07.817494Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARN\u001b[0m  Model is already quantized, will use `from_quantized` to load quantized model.\nIf you want to quantize the model, please pass un_quantized model path or id, and use `from_pretrained` with `quantize_config`.\nfrom_quantized: adapter: None\n\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                                                   \n\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                                      \n\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.004240274429321289s                                           \n\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256\n}\n\n\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                                       \nUsing 128 Wikitext-2 lines for evaluation\n| Metric          |       Value |\n|:----------------|------------:|\n| Avg NLL Loss    |    8.41602  |\n| Perplexity      | 4518.89     |\n| Top-1 Token Acc |    0.351819 |\n| Top-5 Token Acc |    0.561647 |\n\n--- Generations on Wikitext-2 ---\nPrompt:       = Homarus gammarus =…\nContinuation: Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus\n----------------------------------------\nPrompt:       Homarus gammarus , known as the European lobster or common lobster , is a specie…\nContinuation: The species is found in the Atlantic Ocean, the Black Sea, the Mediterranean Sea, the Black Sea, the Black Sea, the Black Sea, the Black Sea, the Black Sea, the Black Sea, the Black Sea, the Black Sea\n----------------------------------------\nPrompt:       = = Description = =…\nContinuation: = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n----------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch, math\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# 1) Load GPT-2 with 8-bit quantization using bitsandbytes\nquant_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n    llm_int8_skip_modules=None\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    quantization_config=quant_config,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# 2) Load 128 Wikitext-2 non-empty lines\nwt2 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\ntexts = [ex[\"text\"].strip() for ex in wt2 if ex[\"text\"].strip()][:128]\nprint(f\"Using {len(texts)} Wikitext-2 lines for evaluation\")\n\n# 3) Tokenize\nencodings = tokenizer(\n    texts,\n    return_tensors=\"pt\",\n    padding=\"longest\",\n    truncation=True,\n    max_length=tokenizer.model_max_length,\n)\ninput_ids = encodings.input_ids.to(model.device)\nattention_mask = encodings.attention_mask.to(model.device)\n\n# 4) Evaluate metrics\nbatch_size = 8\nlosses, top1_accs, top5_accs = [], [], []\n\nmodel.eval()\nwith torch.no_grad():\n    for i in range(0, len(texts), batch_size):\n        b_ids = input_ids[i:i+batch_size]\n        b_mask = attention_mask[i:i+batch_size]\n        out = model(input_ids=b_ids, attention_mask=b_mask, labels=b_ids)\n        losses.append(out.loss.item())\n\n        logits = out.logits[:, :-1, :]\n        sl_labels = b_ids[:, 1:]\n        sl_mask = b_mask[:, 1:].bool()\n\n        preds1 = logits.argmax(dim=-1)\n        correct1 = (preds1 == sl_labels) & sl_mask\n        top1_accs.append(correct1.sum().item() / sl_mask.sum().item())\n\n        top5 = logits.topk(5, dim=-1).indices\n        correct5 = (top5 == sl_labels.unsqueeze(-1)) & sl_mask.unsqueeze(-1)\n        top5_accs.append(correct5.any(dim=-1).sum().item() / sl_mask.sum().item())\n\n# 5) Report metrics\navg_loss = sum(losses) / len(losses)\nperplexity = math.exp(avg_loss)\nacc1 = sum(top1_accs) / len(top1_accs)\nacc5 = sum(top5_accs) / len(top5_accs)\n\ndf = pd.DataFrame([\n    {\"Metric\": \"Avg NLL Loss\", \"Value\": avg_loss},\n    {\"Metric\": \"Perplexity\", \"Value\": perplexity},\n    {\"Metric\": \"Top-1 Token Acc\", \"Value\": acc1},\n    {\"Metric\": \"Top-5 Token Acc\", \"Value\": acc5},\n])\nprint(df.to_markdown(index=False))\n\n# 6) Generate sample continuations\nprint(\"\\n--- Generations on Wikitext-2 ---\")\nfor txt in texts[:3]:\n    enc = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n    gen = model.generate(**enc, max_new_tokens=50, do_sample=False)\n    cont = tokenizer.decode(gen[0, enc[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n    print(\"Prompt:      \", txt[:80] + \"…\")\n    print(\"Continuation:\", cont.strip())\n    print(\"-\" * 40)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T18:37:35.686029Z","iopub.execute_input":"2025-04-30T18:37:35.686774Z","iopub.status.idle":"2025-04-30T18:37:44.032085Z","shell.execute_reply.started":"2025-04-30T18:37:35.686748Z","shell.execute_reply":"2025-04-30T18:37:44.031249Z"}},"outputs":[{"name":"stdout","text":"Using 128 Wikitext-2 lines for evaluation\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"| Metric          |       Value |\n|:----------------|------------:|\n| Avg NLL Loss    |    8.50262  |\n| Perplexity      | 4927.66     |\n| Top-1 Token Acc |    0.355017 |\n| Top-5 Token Acc |    0.563651 |\n\n--- Generations on Wikitext-2 ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt:       = Homarus gammarus =…\nContinuation: Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus gammarus = Homarus\n----------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt:       Homarus gammarus , known as the European lobster or common lobster , is a specie…\nContinuation: The species is found in the Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms\n----------------------------------------\nPrompt:       = = Description = =…\nContinuation: The name of the file to be used.\n\nThe name of the file to be used. The name of the file to be used. The name of the file to be used. The name of the file to be used. The\n----------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Analysis ###","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Comparison: Custom GPTQ vs. HF 8-bit GPT-2\n\nBelow is a comparison between our custom GPTQ-quantized GPT-2 model (β = 2.0, τ = 0.7) and the default 8-bit GPT-2 model from the Hugging Face library, evaluated on 128 lines from the Wikitext-2 validation set.\n\n| Metric             | GPTQ (β=2.0, τ=0.7) | HF 8-bit GPT-2 |\n|--------------------|--------------------:|----------------:|\n| Avg NLL Loss       |              8.4160 |          8.5026 |\n| Perplexity         |           4518.89   |        4927.66  |\n| Top-1 Token Acc.   |             35.18 % |         35.50 % |\n| Top-5 Token Acc.   |             56.16 % |         56.36 % |\n\nOur KL-Aware GPTQ model demonstrates better loss and perplexity, showing stronger probabilistic modeling of the text. While the Hugging Face model performs slightly better in top-1 and top-5 token accuracy, the difference is minimal. This suggests that our quantization settings preserve model confidence better, with only marginal trade-offs in exact token prediction. I believe this will require a thorough Beta, temperature values testing to give some final analysi\n","metadata":{}}]}