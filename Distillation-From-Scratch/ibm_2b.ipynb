{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22727191b91b4b84be250cf8448a5d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb4edaa9a5ce4fdf9a0fc98b2e781936",
              "IPY_MODEL_1df7bf53135f42678d3f615c32d5d29a",
              "IPY_MODEL_9385d359e7ee47b48650d4c2696d9f5e"
            ],
            "layout": "IPY_MODEL_31913315e5ce45c092605a0559d3e0be"
          }
        },
        "cb4edaa9a5ce4fdf9a0fc98b2e781936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db0faaed594b40c39d155ebbda807c62",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_04672925bc1948d4bcb237c964825ffe",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "1df7bf53135f42678d3f615c32d5d29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6391caf88944f6be9a705cbeaafd2d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c1b32769ea41eb9c119752b867254d",
            "value": 2
          }
        },
        "9385d359e7ee47b48650d4c2696d9f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16e1278888d341d7a9a5aae602d88bed",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ceb69eb969344b4ab8910ec0287f3b2",
            "value": "â€‡2/2â€‡[00:30&lt;00:00,â€‡13.58s/it]"
          }
        },
        "31913315e5ce45c092605a0559d3e0be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db0faaed594b40c39d155ebbda807c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04672925bc1948d4bcb237c964825ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed6391caf88944f6be9a705cbeaafd2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c1b32769ea41eb9c119752b867254d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16e1278888d341d7a9a5aae602d88bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ceb69eb969344b4ab8910ec0287f3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import bitsandbytes"
      ],
      "metadata": {
        "id": "ZY9-98-lpupD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch transformers psutil\n",
        "# !pip install bitsandbytes"
      ],
      "metadata": {
        "id": "ezb6wQOPC9fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import torch\n",
        "import psutil\n",
        "\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model path\n",
        "model_path = \"ibm-granite/granite-vision-3.2-2b\"\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "\n",
        "# Load model in 8-bit\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_path,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Print GPU memory footprint (if available)\n",
        "def get_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"ðŸŸ¢ GPU Memory Allocated: {allocated:.2f} MB\")\n",
        "        print(f\"ðŸŸ¡ GPU Memory Reserved : {reserved:.2f} MB\")\n",
        "    else:\n",
        "        print(\"ðŸ”´ CUDA not available.\")\n",
        "\n",
        "# Print CPU memory usage\n",
        "def get_cpu_memory():\n",
        "    mem = psutil.virtual_memory()\n",
        "    print(f\"ðŸ§  CPU Memory Used : {mem.used / 1024**2:.2f} MB\")\n",
        "    print(f\"ðŸ§  CPU Total Memory: {mem.total / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display memory usage\n",
        "print(\"\\n=== Memory Footprint ===\\n\")\n",
        "get_gpu_memory()\n",
        "get_cpu_memory()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "22727191b91b4b84be250cf8448a5d1e",
            "cb4edaa9a5ce4fdf9a0fc98b2e781936",
            "1df7bf53135f42678d3f615c32d5d29a",
            "9385d359e7ee47b48650d4c2696d9f5e",
            "31913315e5ce45c092605a0559d3e0be",
            "db0faaed594b40c39d155ebbda807c62",
            "04672925bc1948d4bcb237c964825ffe",
            "ed6391caf88944f6be9a705cbeaafd2d",
            "83c1b32769ea41eb9c119752b867254d",
            "16e1278888d341d7a9a5aae602d88bed",
            "2ceb69eb969344b4ab8910ec0287f3b2"
          ]
        },
        "id": "7ADk8pXIpyKw",
        "outputId": "e312e1ed-ffd7-44ab-fb31-7b82f60f782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22727191b91b4b84be250cf8448a5d1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Memory Footprint ===\n",
            "\n",
            "ðŸŸ¢ GPU Memory Allocated: 2966.77 MB\n",
            "ðŸŸ¡ GPU Memory Reserved : 3008.00 MB\n",
            "ðŸ§  CPU Memory Used : 3081.58 MB\n",
            "ðŸ§  CPU Total Memory: 12978.96 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model memory footprint if available\n",
        "if hasattr(model, \"get_memory_footprint\"):\n",
        "    footprint_bytes = model.get_memory_footprint()\n",
        "    print(f\"Model memory footprint: {footprint_bytes / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Model does not support get_memory_footprint().\")\n",
        "\n",
        "# Print GPU memory stats (if running on GPU)\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated(device)\n",
        "    reserved = torch.cuda.memory_reserved(device)\n",
        "    print(\"GPU Memory Stats:\")\n",
        "    print(f\"  Allocated: {allocated / 1e9:.2f} GB\")\n",
        "    print(f\"  Reserved:  {reserved / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RykPumBgChu9",
        "outputId": "e1b5a490-e7d4-4b05-edbc-e32c0be49a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model memory footprint: 2.37 GB\n",
            "GPU Memory Stats:\n",
            "  Allocated: 2.44 GB\n",
            "  Reserved:  2.56 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"FLASH_ATTENTION_2_ENABLED\"] = \"0\"\n",
        "\n",
        "# Force eager mode globally\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_math_sdp(True)\n"
      ],
      "metadata": {
        "id": "dhp_e-oTGsou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers==4.47.0\n",
        "# #"
      ],
      "metadata": {
        "id": "_BkDUE8RG7rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVPhr0PGpsYA",
        "outputId": "4b5cad5d-2c9f-4c8c-a127-d0576717b155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-vision-instruct.4a0d683eba9f1d0cbfb6151705d1ee73c25a80ca.modeling_phi3_v:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Output ===\n",
            "\n",
            "The presentation is about Microsoft Azure, focusing on its three types: Hyper-scale, Enterprise, and Hybrid. The presenter is Dinesh Kumar Wickramasinghe, a Senior Software Engineer from CMS Private Limited in Sri Lanka. The design is clean and professional, with a blue color scheme and hexagonal shapes. the word 'Hyper-scale' in the image. the word 'Hybrid' in the image. the word 'Hybrid' in the image.\n"
          ]
        }
      ],
      "source": [
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    num_crops=4\n",
        ")\n",
        "\n",
        "image_url = \"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-1-2048.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "# Prepare the input\n",
        "placeholder = \"<|image_1|>\\n\"  # Placeholder for the image tag\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": placeholder + \"Summarize the deck of slides.\"},\n",
        "]\n",
        "\n",
        "# Generate prompt\n",
        "prompt = processor.tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Process inputs\n",
        "inputs = processor(prompt, images=[image], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "# Generate response\n",
        "generate_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "# Trim input tokens\n",
        "generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
        "\n",
        "# Decode response\n",
        "response = processor.batch_decode(\n",
        "    generate_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")[0]\n",
        "\n",
        "print(\"\\n=== Model Output ===\\n\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCleaning up...\")\n",
        "del model\n",
        "del processor\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"Model and processor removed from memory. GPU cache cleared (if applicable).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_SbRFaTrIOE",
        "outputId": "63cae196-9eed-414b-c8a0-be4988985934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaning up...\n",
            "Model and processor removed from memory. GPU cache cleared (if applicable).\n"
          ]
        }
      ]
    }
  ]
}